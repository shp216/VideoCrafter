wandb_version: 1

gpu_num:
  desc: null
  value: 1
gpu_no:
  desc: null
  value: 0
kwargs:
  desc: null
  value: {}
config:
  desc: null
  value: '{}'
model_config:
  desc: null
  value: '{''target'': ''lvdm.models.ddpm3d.LatentDiffusion'', ''params'': {''linear_start'':
    0.00085, ''linear_end'': 0.012, ''num_timesteps_cond'': 1, ''timesteps'': 1000,
    ''first_stage_key'': ''video'', ''cond_stage_key'': ''caption'', ''cond_stage_trainable'':
    False, ''conditioning_key'': ''crossattn'', ''image_size'': [40, 64], ''channels'':
    4, ''scale_by_std'': False, ''scale_factor'': 0.18215, ''use_ema'': False, ''uncond_type'':
    ''empty_seq'', ''use_scale'': True, ''scale_b'': 0.7, ''unet_config'': {''target'':
    ''lvdm.modules.networks.openaimodel3d.UNetModel'', ''params'': {''in_channels'':
    4, ''out_channels'': 4, ''model_channels'': 320, ''attention_resolutions'': [4,
    2, 1], ''num_res_blocks'': 2, ''channel_mult'': [1, 2, 4, 4], ''num_head_channels'':
    64, ''transformer_depth'': 1, ''context_dim'': 1024, ''use_linear'': True, ''use_checkpoint'':
    True, ''temporal_conv'': True, ''temporal_attention'': True, ''temporal_selfatt_only'':
    True, ''use_relative_position'': False, ''use_causal_attention'': False, ''temporal_length'':
    16, ''addition_attention'': True, ''fps_cond'': True}}, ''first_stage_config'':
    {''target'': ''lvdm.models.autoencoder.AutoencoderKL'', ''params'': {''embed_dim'':
    4, ''monitor'': ''val/rec_loss'', ''ddconfig'': {''double_z'': True, ''z_channels'':
    4, ''resolution'': 512, ''in_channels'': 3, ''out_ch'': 3, ''ch'': 128, ''ch_mult'':
    [1, 2, 4, 4], ''num_res_blocks'': 2, ''attn_resolutions'': [], ''dropout'': 0.0},
    ''lossconfig'': {''target'': ''torch.nn.Identity''}}}, ''cond_stage_config'':
    {''target'': ''lvdm.modules.encoders.condition.FrozenOpenCLIPEmbedder'', ''params'':
    {''freeze'': True, ''layer'': ''penultimate''}}}}'
mmg_model:
  desc: null
  value: "LatentDiffusion(\n  (model): DiffusionWrapper(\n    (diffusion_model): UNetModel(\n\
    \      (time_embed): Sequential(\n        (0): Linear(in_features=320, out_features=1280,\
    \ bias=True)\n        (1): SiLU()\n        (2): Linear(in_features=1280, out_features=1280,\
    \ bias=True)\n      )\n      (fps_embedding): Sequential(\n        (0): Linear(in_features=320,\
    \ out_features=1280, bias=True)\n        (1): SiLU()\n        (2): Linear(in_features=1280,\
    \ out_features=1280, bias=True)\n      )\n      (input_blocks): ModuleList(\n\
    \        (0): TimestepEmbedSequential(\n          (0): Conv2d(4, 320, kernel_size=(3,\
    \ 3), stride=(1, 1), padding=(1, 1))\n        )\n        (1-2): 2 x TimestepEmbedSequential(\n\
    \          (0): ResBlock(\n            (in_layers): Sequential(\n            \
    \  (0): GroupNormSpecific(32, 320, eps=1e-05, affine=True)\n              (1):\
    \ SiLU()\n              (2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1),\
    \ padding=(1, 1))\n            )\n            (h_upd): Identity()\n          \
    \  (x_upd): Identity()\n            (emb_layers): Sequential(\n              (0):\
    \ SiLU()\n              (1): Linear(in_features=1280, out_features=320, bias=True)\n\
    \            )\n            (out_layers): Sequential(\n              (0): GroupNormSpecific(32,\
    \ 320, eps=1e-05, affine=True)\n              (1): SiLU()\n              (2):\
    \ Dropout(p=0.0, inplace=False)\n              (3): Conv2d(320, 320, kernel_size=(3,\
    \ 3), stride=(1, 1), padding=(1, 1))\n            )\n            (skip_connection):\
    \ Identity()\n            (temopral_conv): TemporalConvBlock(\n              (conv1):\
    \ Sequential(\n                (0): GroupNorm(32, 320, eps=1e-05, affine=True)\n\
    \                (1): SiLU()\n                (2): Conv3d(320, 320, kernel_size=(3,\
    \ 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n              )\n             \
    \ (conv2): Sequential(\n                (0): GroupNorm(32, 320, eps=1e-05, affine=True)\n\
    \                (1): SiLU()\n                (2): Dropout(p=0.1, inplace=False)\n\
    \                (3): Conv3d(320, 320, kernel_size=(3, 1, 1), stride=(1, 1, 1),\
    \ padding=(1, 0, 0))\n              )\n              (conv3): Sequential(\n  \
    \              (0): GroupNorm(32, 320, eps=1e-05, affine=True)\n             \
    \   (1): SiLU()\n                (2): Dropout(p=0.1, inplace=False)\n        \
    \        (3): Conv3d(320, 320, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1,\
    \ 0, 0))\n              )\n              (conv4): Sequential(\n              \
    \  (0): GroupNorm(32, 320, eps=1e-05, affine=True)\n                (1): SiLU()\n\
    \                (2): Dropout(p=0.1, inplace=False)\n                (3): Conv3d(320,\
    \ 320, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n         \
    \     )\n            )\n          )\n          (1): SpatialTransformer(\n    \
    \        (norm): GroupNorm(32, 320, eps=1e-06, affine=True)\n            (proj_in):\
    \ Linear(in_features=320, out_features=320, bias=True)\n            (transformer_blocks):\
    \ ModuleList(\n              (0): BasicTransformerBlock(\n                (attn1):\
    \ CrossAttention(\n                  (to_q): Linear(in_features=320, out_features=320,\
    \ bias=False)\n                  (to_k): Linear(in_features=320, out_features=320,\
    \ bias=False)\n                  (to_v): Linear(in_features=320, out_features=320,\
    \ bias=False)\n                  (to_out): Sequential(\n                    (0):\
    \ Linear(in_features=320, out_features=320, bias=True)\n                    (1):\
    \ Dropout(p=0.0, inplace=False)\n                  )\n                )\n    \
    \            (ff): FeedForward(\n                  (net): Sequential(\n      \
    \              (0): GEGLU(\n                      (proj): Linear(in_features=320,\
    \ out_features=2560, bias=True)\n                    )\n                    (1):\
    \ Dropout(p=0.0, inplace=False)\n                    (2): Linear(in_features=1280,\
    \ out_features=320, bias=True)\n                  )\n                )\n     \
    \           (attn2): CrossAttention(\n                  (to_q): Linear(in_features=320,\
    \ out_features=320, bias=False)\n                  (to_k): Linear(in_features=1024,\
    \ out_features=320, bias=False)\n                  (to_v): Linear(in_features=1024,\
    \ out_features=320, bias=False)\n                  (to_out): Sequential(\n   \
    \                 (0): Linear(in_features=320, out_features=320, bias=True)\n\
    \                    (1): Dropout(p=0.0, inplace=False)\n                  )\n\
    \                )\n                (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n\
    \                (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n\
    \                (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n\
    \              )\n            )\n            (proj_out): Linear(in_features=320,\
    \ out_features=320, bias=True)\n          )\n          (2): TemporalTransformer(\n\
    \            (norm): GroupNorm(32, 320, eps=1e-06, affine=True)\n            (proj_in):\
    \ Linear(in_features=320, out_features=320, bias=True)\n            (transformer_blocks):\
    \ ModuleList(\n              (0): BasicTransformerBlock(\n                (attn1):\
    \ CrossAttention(\n                  (to_q): Linear(in_features=320, out_features=320,\
    \ bias=False)\n                  (to_k): Linear(in_features=320, out_features=320,\
    \ bias=False)\n                  (to_v): Linear(in_features=320, out_features=320,\
    \ bias=False)\n                  (to_out): Sequential(\n                    (0):\
    \ Linear(in_features=320, out_features=320, bias=True)\n                    (1):\
    \ Dropout(p=0.0, inplace=False)\n                  )\n                )\n    \
    \            (ff): FeedForward(\n                  (net): Sequential(\n      \
    \              (0): GEGLU(\n                      (proj): Linear(in_features=320,\
    \ out_features=2560, bias=True)\n                    )\n                    (1):\
    \ Dropout(p=0.0, inplace=False)\n                    (2): Linear(in_features=1280,\
    \ out_features=320, bias=True)\n                  )\n                )\n     \
    \           (attn2): CrossAttention(\n                  (to_q): Linear(in_features=320,\
    \ out_features=320, bias=False)\n                  (to_k): Linear(in_features=320,\
    \ out_features=320, bias=False)\n                  (to_v): Linear(in_features=320,\
    \ out_features=320, bias=False)\n                  (to_out): Sequential(\n   \
    \                 (0): Linear(in_features=320, out_features=320, bias=True)\n\
    \                    (1): Dropout(p=0.0, inplace=False)\n                  )\n\
    \                )\n                (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n\
    \                (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n\
    \                (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n\
    \              )\n            )\n            (proj_out): Linear(in_features=320,\
    \ out_features=320, bias=True)\n          )\n        )\n        (3): TimestepEmbedSequential(\n\
    \          (0): Downsample(\n            (op): Conv2d(320, 320, kernel_size=(3,\
    \ 3), stride=(2, 2), padding=(1, 1))\n          )\n        )\n        (4): TimestepEmbedSequential(\n\
    \          (0): ResBlock(\n            (in_layers): Sequential(\n            \
    \  (0): GroupNormSpecific(32, 320, eps=1e-05, affine=True)\n              (1):\
    \ SiLU()\n              (2): Conv2d(320, 640, kernel_size=(3, 3), stride=(1, 1),\
    \ padding=(1, 1))\n            )\n            (h_upd): Identity()\n          \
    \  (x_upd): Identity()\n            (emb_layers): Sequential(\n              (0):\
    \ SiLU()\n              (1): Linear(in_features=1280, out_features=640, bias=True)\n\
    \            )\n            (out_layers): Sequential(\n              (0): GroupNormSpecific(32,\
    \ 640, eps=1e-05, affine=True)\n              (1): SiLU()\n              (2):\
    \ Dropout(p=0.0, inplace=False)\n              (3): Conv2d(640, 640, kernel_size=(3,\
    \ 3), stride=(1, 1), padding=(1, 1))\n            )\n            (skip_connection):\
    \ Conv2d(320, 640, kernel_size=(1, 1), stride=(1, 1))\n            (temopral_conv):\
    \ TemporalConvBlock(\n              (conv1): Sequential(\n                (0):\
    \ GroupNorm(32, 640, eps=1e-05, affine=True)\n                (1): SiLU()\n  \
    \              (2): Conv3d(640, 640, kernel_size=(3, 1, 1), stride=(1, 1, 1),\
    \ padding=(1, 0, 0))\n              )\n              (conv2): Sequential(\n  \
    \              (0): GroupNorm(32, 640, eps=1e-05, affine=True)\n             \
    \   (1): SiLU()\n                (2): Dropout(p=0.1, inplace=False)\n        \
    \        (3): Conv3d(640, 640, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1,\
    \ 0, 0))\n              )\n              (conv3): Sequential(\n              \
    \  (0): GroupNorm(32, 640, eps=1e-05, affine=True)\n                (1): SiLU()\n\
    \                (2): Dropout(p=0.1, inplace=False)\n                (3): Conv3d(640,\
    \ 640, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n         \
    \     )\n              (conv4): Sequential(\n                (0): GroupNorm(32,\
    \ 640, eps=1e-05, affine=True)\n                (1): SiLU()\n                (2):\
    \ Dropout(p=0.1, inplace=False)\n                (3): Conv3d(640, 640, kernel_size=(3,\
    \ 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n              )\n            )\n\
    \          )\n          (1): SpatialTransformer(\n            (norm): GroupNorm(32,\
    \ 640, eps=1e-06, affine=True)\n            (proj_in): Linear(in_features=640,\
    \ out_features=640, bias=True)\n            (transformer_blocks): ModuleList(\n\
    \              (0): BasicTransformerBlock(\n                (attn1): CrossAttention(\n\
    \                  (to_q): Linear(in_features=640, out_features=640, bias=False)\n\
    \                  (to_k): Linear(in_features=640, out_features=640, bias=False)\n\
    \                  (to_v): Linear(in_features=640, out_features=640, bias=False)\n\
    \                  (to_out): Sequential(\n                    (0): Linear(in_features=640,\
    \ out_features=640, bias=True)\n                    (1): Dropout(p=0.0, inplace=False)\n\
    \                  )\n                )\n                (ff): FeedForward(\n\
    \                  (net): Sequential(\n                    (0): GEGLU(\n     \
    \                 (proj): Linear(in_features=640, out_features=5120, bias=True)\n\
    \                    )\n                    (1): Dropout(p=0.0, inplace=False)\n\
    \                    (2): Linear(in_features=2560, out_features=640, bias=True)\n\
    \                  )\n                )\n                (attn2): CrossAttention(\n\
    \                  (to_q): Linear(in_features=640, out_features=640, bias=False)\n\
    \                  (to_k): Linear(in_features=1024, out_features=640, bias=False)\n\
    \                  (to_v): Linear(in_features=1024, out_features=640, bias=False)\n\
    \                  (to_out): Sequential(\n                    (0): Linear(in_features=640,\
    \ out_features=640, bias=True)\n                    (1): Dropout(p=0.0, inplace=False)\n\
    \                  )\n                )\n                (norm1): LayerNorm((640,),\
    \ eps=1e-05, elementwise_affine=True)\n                (norm2): LayerNorm((640,),\
    \ eps=1e-05, elementwise_affine=True)\n                (norm3): LayerNorm((640,),\
    \ eps=1e-05, elementwise_affine=True)\n              )\n            )\n      \
    \      (proj_out): Linear(in_features=640, out_features=640, bias=True)\n    \
    \      )\n          (2): TemporalTransformer(\n            (norm): GroupNorm(32,\
    \ 640, eps=1e-06, affine=True)\n            (proj_in): Linear(in_features=640,\
    \ out_features=640, bias=True)\n            (transformer_blocks): ModuleList(\n\
    \              (0): BasicTransformerBlock(\n                (attn1): CrossAttention(\n\
    \                  (to_q): Linear(in_features=640, out_features=640, bias=False)\n\
    \                  (to_k): Linear(in_features=640, out_features=640, bias=False)\n\
    \                  (to_v): Linear(in_features=640, out_features=640, bias=False)\n\
    \                  (to_out): Sequential(\n                    (0): Linear(in_features=640,\
    \ out_features=640, bias=True)\n                    (1): Dropout(p=0.0, inplace=False)\n\
    \                  )\n                )\n                (ff): FeedForward(\n\
    \                  (net): Sequential(\n                    (0): GEGLU(\n     \
    \                 (proj): Linear(in_features=640, out_features=5120, bias=True)\n\
    \                    )\n                    (1): Dropout(p=0.0, inplace=False)\n\
    \                    (2): Linear(in_features=2560, out_features=640, bias=True)\n\
    \                  )\n                )\n                (attn2): CrossAttention(\n\
    \                  (to_q): Linear(in_features=640, out_features=640, bias=False)\n\
    \                  (to_k): Linear(in_features=640, out_features=640, bias=False)\n\
    \                  (to_v): Linear(in_features=640, out_features=640, bias=False)\n\
    \                  (to_out): Sequential(\n                    (0): Linear(in_features=640,\
    \ out_features=640, bias=True)\n                    (1): Dropout(p=0.0, inplace=False)\n\
    \                  )\n                )\n                (norm1): LayerNorm((640,),\
    \ eps=1e-05, elementwise_affine=True)\n                (norm2): LayerNorm((640,),\
    \ eps=1e-05, elementwise_affine=True)\n                (norm3): LayerNorm((640,),\
    \ eps=1e-05, elementwise_affine=True)\n              )\n            )\n      \
    \      (proj_out): Linear(in_features=640, out_features=640, bias=True)\n    \
    \      )\n        )\n        (5): TimestepEmbedSequential(\n          (0): ResBlock(\n\
    \            (in_layers): Sequential(\n              (0): GroupNormSpecific(32,\
    \ 640, eps=1e-05, affine=True)\n              (1): SiLU()\n              (2):\
    \ Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      \
    \      )\n            (h_upd): Identity()\n            (x_upd): Identity()\n \
    \           (emb_layers): Sequential(\n              (0): SiLU()\n           \
    \   (1): Linear(in_features=1280, out_features=640, bias=True)\n            )\n\
    \            (out_layers): Sequential(\n              (0): GroupNormSpecific(32,\
    \ 640, eps=1e-05, affine=True)\n              (1): SiLU()\n              (2):\
    \ Dropout(p=0.0, inplace=False)\n              (3): Conv2d(640, 640, kernel_size=(3,\
    \ 3), stride=(1, 1), padding=(1, 1))\n            )\n            (skip_connection):\
    \ Identity()\n            (temopral_conv): TemporalConvBlock(\n              (conv1):\
    \ Sequential(\n                (0): GroupNorm(32, 640, eps=1e-05, affine=True)\n\
    \                (1): SiLU()\n                (2): Conv3d(640, 640, kernel_size=(3,\
    \ 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n              )\n             \
    \ (conv2): Sequential(\n                (0): GroupNorm(32, 640, eps=1e-05, affine=True)\n\
    \                (1): SiLU()\n                (2): Dropout(p=0.1, inplace=False)\n\
    \                (3): Conv3d(640, 640, kernel_size=(3, 1, 1), stride=(1, 1, 1),\
    \ padding=(1, 0, 0))\n              )\n              (conv3): Sequential(\n  \
    \              (0): GroupNorm(32, 640, eps=1e-05, affine=True)\n             \
    \   (1): SiLU()\n                (2): Dropout(p=0.1, inplace=False)\n        \
    \        (3): Conv3d(640, 640, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1,\
    \ 0, 0))\n              )\n              (conv4): Sequential(\n              \
    \  (0): GroupNorm(32, 640, eps=1e-05, affine=True)\n                (1): SiLU()\n\
    \                (2): Dropout(p=0.1, inplace=False)\n                (3): Conv3d(640,\
    \ 640, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n         \
    \     )\n            )\n          )\n          (1): SpatialTransformer(\n    \
    \        (norm): GroupNorm(32, 640, eps=1e-06, affine=True)\n            (proj_in):\
    \ Linear(in_features=640, out_features=640, bias=True)\n            (transformer_blocks):\
    \ ModuleList(\n              (0): BasicTransformerBlock(\n                (attn1):\
    \ CrossAttention(\n                  (to_q): Linear(in_features=640, out_features=640,\
    \ bias=False)\n                  (to_k): Linear(in_features=640, out_features=640,\
    \ bias=False)\n                  (to_v): Linear(in_features=640, out_features=640,\
    \ bias=False)\n                  (to_out): Sequential(\n                    (0):\
    \ Linear(in_features=640, out_features=640, bias=True)\n                    (1):\
    \ Dropout(p=0.0, inplace=False)\n                  )\n                )\n    \
    \            (ff): FeedForward(\n                  (net): Sequential(\n      \
    \              (0): GEGLU(\n                      (proj): Linear(in_features=640,\
    \ out_features=5120, bias=True)\n                    )\n                    (1):\
    \ Dropout(p=0.0, inplace=False)\n                    (2): Linear(in_features=2560,\
    \ out_features=640, bias=True)\n                  )\n                )\n     \
    \           (attn2): CrossAttention(\n                  (to_q): Linear(in_features=640,\
    \ out_features=640, bias=False)\n                  (to_k): Linear(in_features=1024,\
    \ out_features=640, bias=False)\n                  (to_v): Linear(in_features=1024,\
    \ out_features=640, bias=False)\n                  (to_out): Sequential(\n   \
    \                 (0): Linear(in_features=640, out_features=640, bias=True)\n\
    \                    (1): Dropout(p=0.0, inplace=False)\n                  )\n\
    \                )\n                (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n\
    \                (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n\
    \                (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n\
    \              )\n            )\n            (proj_out): Linear(in_features=640,\
    \ out_features=640, bias=True)\n          )\n          (2): TemporalTransformer(\n\
    \            (norm): GroupNorm(32, 640, eps=1e-06, affine=True)\n            (proj_in):\
    \ Linear(in_features=640, out_features=640, bias=True)\n            (transformer_blocks):\
    \ ModuleList(\n              (0): BasicTransformerBlock(\n                (attn1):\
    \ CrossAttention(\n                  (to_q): Linear(in_features=640, out_features=640,\
    \ bias=False)\n                  (to_k): Linear(in_features=640, out_features=640,\
    \ bias=False)\n                  (to_v): Linear(in_features=640, out_features=640,\
    \ bias=False)\n                  (to_out): Sequential(\n                    (0):\
    \ Linear(in_features=640, out_features=640, bias=True)\n                    (1):\
    \ Dropout(p=0.0, inplace=False)\n                  )\n                )\n    \
    \            (ff): FeedForward(\n                  (net): Sequential(\n      \
    \              (0): GEGLU(\n                      (proj): Linear(in_features=640,\
    \ out_features=5120, bias=True)\n                    )\n                    (1):\
    \ Dropout(p=0.0, inplace=False)\n                    (2): Linear(in_features=2560,\
    \ out_features=640, bias=True)\n                  )\n                )\n     \
    \           (attn2): CrossAttention(\n                  (to_q): Linear(in_features=640,\
    \ out_features=640, bias=False)\n                  (to_k): Linear(in_features=640,\
    \ out_features=640, bias=False)\n                  (to_v): Linear(in_features=640,\
    \ out_features=640, bias=False)\n                  (to_out): Sequential(\n   \
    \                 (0): Linear(in_features=640, out_features=640, bias=True)\n\
    \                    (1): Dropout(p=0.0, inplace=False)\n                  )\n\
    \                )\n                (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n\
    \                (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n\
    \                (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n\
    \              )\n            )\n            (proj_out): Linear(in_features=640,\
    \ out_features=640, bias=True)\n          )\n        )\n        (6): TimestepEmbedSequential(\n\
    \          (0): Downsample(\n            (op): Conv2d(640, 640, kernel_size=(3,\
    \ 3), stride=(2, 2), padding=(1, 1))\n          )\n        )\n        (7): TimestepEmbedSequential(\n\
    \          (0): ResBlock(\n            (in_layers): Sequential(\n            \
    \  (0): GroupNormSpecific(32, 640, eps=1e-05, affine=True)\n              (1):\
    \ SiLU()\n              (2): Conv2d(640, 1280, kernel_size=(3, 3), stride=(1,\
    \ 1), padding=(1, 1))\n            )\n            (h_upd): Identity()\n      \
    \      (x_upd): Identity()\n            (emb_layers): Sequential(\n          \
    \    (0): SiLU()\n              (1): Linear(in_features=1280, out_features=1280,\
    \ bias=True)\n            )\n            (out_layers): Sequential(\n         \
    \     (0): GroupNormSpecific(32, 1280, eps=1e-05, affine=True)\n             \
    \ (1): SiLU()\n              (2): Dropout(p=0.0, inplace=False)\n            \
    \  (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n\
    \            )\n            (skip_connection): Conv2d(640, 1280, kernel_size=(1,\
    \ 1), stride=(1, 1))\n            (temopral_conv): TemporalConvBlock(\n      \
    \        (conv1): Sequential(\n                (0): GroupNorm(32, 1280, eps=1e-05,\
    \ affine=True)\n                (1): SiLU()\n                (2): Conv3d(1280,\
    \ 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n        \
    \      )\n              (conv2): Sequential(\n                (0): GroupNorm(32,\
    \ 1280, eps=1e-05, affine=True)\n                (1): SiLU()\n               \
    \ (2): Dropout(p=0.1, inplace=False)\n                (3): Conv3d(1280, 1280,\
    \ kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n              )\n\
    \              (conv3): Sequential(\n                (0): GroupNorm(32, 1280,\
    \ eps=1e-05, affine=True)\n                (1): SiLU()\n                (2): Dropout(p=0.1,\
    \ inplace=False)\n                (3): Conv3d(1280, 1280, kernel_size=(3, 1, 1),\
    \ stride=(1, 1, 1), padding=(1, 0, 0))\n              )\n              (conv4):\
    \ Sequential(\n                (0): GroupNorm(32, 1280, eps=1e-05, affine=True)\n\
    \                (1): SiLU()\n                (2): Dropout(p=0.1, inplace=False)\n\
    \                (3): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1,\
    \ 1), padding=(1, 0, 0))\n              )\n            )\n          )\n      \
    \    (1): SpatialTransformer(\n            (norm): GroupNorm(32, 1280, eps=1e-06,\
    \ affine=True)\n            (proj_in): Linear(in_features=1280, out_features=1280,\
    \ bias=True)\n            (transformer_blocks): ModuleList(\n              (0):\
    \ BasicTransformerBlock(\n                (attn1): CrossAttention(\n         \
    \         (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n  \
    \                (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n\
    \                  (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n\
    \                  (to_out): Sequential(\n                    (0): Linear(in_features=1280,\
    \ out_features=1280, bias=True)\n                    (1): Dropout(p=0.0, inplace=False)\n\
    \                  )\n                )\n                (ff): FeedForward(\n\
    \                  (net): Sequential(\n                    (0): GEGLU(\n     \
    \                 (proj): Linear(in_features=1280, out_features=10240, bias=True)\n\
    \                    )\n                    (1): Dropout(p=0.0, inplace=False)\n\
    \                    (2): Linear(in_features=5120, out_features=1280, bias=True)\n\
    \                  )\n                )\n                (attn2): CrossAttention(\n\
    \                  (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n\
    \                  (to_k): Linear(in_features=1024, out_features=1280, bias=False)\n\
    \                  (to_v): Linear(in_features=1024, out_features=1280, bias=False)\n\
    \                  (to_out): Sequential(\n                    (0): Linear(in_features=1280,\
    \ out_features=1280, bias=True)\n                    (1): Dropout(p=0.0, inplace=False)\n\
    \                  )\n                )\n                (norm1): LayerNorm((1280,),\
    \ eps=1e-05, elementwise_affine=True)\n                (norm2): LayerNorm((1280,),\
    \ eps=1e-05, elementwise_affine=True)\n                (norm3): LayerNorm((1280,),\
    \ eps=1e-05, elementwise_affine=True)\n              )\n            )\n      \
    \      (proj_out): Linear(in_features=1280, out_features=1280, bias=True)\n  \
    \        )\n          (2): TemporalTransformer(\n            (norm): GroupNorm(32,\
    \ 1280, eps=1e-06, affine=True)\n            (proj_in): Linear(in_features=1280,\
    \ out_features=1280, bias=True)\n            (transformer_blocks): ModuleList(\n\
    \              (0): BasicTransformerBlock(\n                (attn1): CrossAttention(\n\
    \                  (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n\
    \                  (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n\
    \                  (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n\
    \                  (to_out): Sequential(\n                    (0): Linear(in_features=1280,\
    \ out_features=1280, bias=True)\n                    (1): Dropout(p=0.0, inplace=False)\n\
    \                  )\n                )\n                (ff): FeedForward(\n\
    \                  (net): Sequential(\n                    (0): GEGLU(\n     \
    \                 (proj): Linear(in_features=1280, out_features=10240, bias=True)\n\
    \                    )\n                    (1): Dropout(p=0.0, inplace=False)\n\
    \                    (2): Linear(in_features=5120, out_features=1280, bias=True)\n\
    \                  )\n                )\n                (attn2): CrossAttention(\n\
    \                  (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n\
    \                  (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n\
    \                  (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n\
    \                  (to_out): Sequential(\n                    (0): Linear(in_features=1280,\
    \ out_features=1280, bias=True)\n                    (1): Dropout(p=0.0, inplace=False)\n\
    \                  )\n                )\n                (norm1): LayerNorm((1280,),\
    \ eps=1e-05, elementwise_affine=True)\n                (norm2): LayerNorm((1280,),\
    \ eps=1e-05, elementwise_affine=True)\n                (norm3): LayerNorm((1280,),\
    \ eps=1e-05, elementwise_affine=True)\n              )\n            )\n      \
    \      (proj_out): Linear(in_features=1280, out_features=1280, bias=True)\n  \
    \        )\n        )\n        (8): TimestepEmbedSequential(\n          (0): ResBlock(\n\
    \            (in_layers): Sequential(\n              (0): GroupNormSpecific(32,\
    \ 1280, eps=1e-05, affine=True)\n              (1): SiLU()\n              (2):\
    \ Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    \
    \        )\n            (h_upd): Identity()\n            (x_upd): Identity()\n\
    \            (emb_layers): Sequential(\n              (0): SiLU()\n          \
    \    (1): Linear(in_features=1280, out_features=1280, bias=True)\n           \
    \ )\n            (out_layers): Sequential(\n              (0): GroupNormSpecific(32,\
    \ 1280, eps=1e-05, affine=True)\n              (1): SiLU()\n              (2):\
    \ Dropout(p=0.0, inplace=False)\n              (3): Conv2d(1280, 1280, kernel_size=(3,\
    \ 3), stride=(1, 1), padding=(1, 1))\n            )\n            (skip_connection):\
    \ Identity()\n            (temopral_conv): TemporalConvBlock(\n              (conv1):\
    \ Sequential(\n                (0): GroupNorm(32, 1280, eps=1e-05, affine=True)\n\
    \                (1): SiLU()\n                (2): Conv3d(1280, 1280, kernel_size=(3,\
    \ 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n              )\n             \
    \ (conv2): Sequential(\n                (0): GroupNorm(32, 1280, eps=1e-05, affine=True)\n\
    \                (1): SiLU()\n                (2): Dropout(p=0.1, inplace=False)\n\
    \                (3): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1,\
    \ 1), padding=(1, 0, 0))\n              )\n              (conv3): Sequential(\n\
    \                (0): GroupNorm(32, 1280, eps=1e-05, affine=True)\n          \
    \      (1): SiLU()\n                (2): Dropout(p=0.1, inplace=False)\n     \
    \           (3): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1,\
    \ 0, 0))\n              )\n              (conv4): Sequential(\n              \
    \  (0): GroupNorm(32, 1280, eps=1e-05, affine=True)\n                (1): SiLU()\n\
    \                (2): Dropout(p=0.1, inplace=False)\n                (3): Conv3d(1280,\
    \ 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n        \
    \      )\n            )\n          )\n          (1): SpatialTransformer(\n   \
    \         (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n            (proj_in):\
    \ Linear(in_features=1280, out_features=1280, bias=True)\n            (transformer_blocks):\
    \ ModuleList(\n              (0): BasicTransformerBlock(\n                (attn1):\
    \ CrossAttention(\n                  (to_q): Linear(in_features=1280, out_features=1280,\
    \ bias=False)\n                  (to_k): Linear(in_features=1280, out_features=1280,\
    \ bias=False)\n                  (to_v): Linear(in_features=1280, out_features=1280,\
    \ bias=False)\n                  (to_out): Sequential(\n                    (0):\
    \ Linear(in_features=1280, out_features=1280, bias=True)\n                   \
    \ (1): Dropout(p=0.0, inplace=False)\n                  )\n                )\n\
    \                (ff): FeedForward(\n                  (net): Sequential(\n  \
    \                  (0): GEGLU(\n                      (proj): Linear(in_features=1280,\
    \ out_features=10240, bias=True)\n                    )\n                    (1):\
    \ Dropout(p=0.0, inplace=False)\n                    (2): Linear(in_features=5120,\
    \ out_features=1280, bias=True)\n                  )\n                )\n    \
    \            (attn2): CrossAttention(\n                  (to_q): Linear(in_features=1280,\
    \ out_features=1280, bias=False)\n                  (to_k): Linear(in_features=1024,\
    \ out_features=1280, bias=False)\n                  (to_v): Linear(in_features=1024,\
    \ out_features=1280, bias=False)\n                  (to_out): Sequential(\n  \
    \                  (0): Linear(in_features=1280, out_features=1280, bias=True)\n\
    \                    (1): Dropout(p=0.0, inplace=False)\n                  )\n\
    \                )\n                (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n\
    \                (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n\
    \                (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n\
    \              )\n            )\n            (proj_out): Linear(in_features=1280,\
    \ out_features=1280, bias=True)\n          )\n          (2): TemporalTransformer(\n\
    \            (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n           \
    \ (proj_in): Linear(in_features=1280, out_features=1280, bias=True)\n        \
    \    (transformer_blocks): ModuleList(\n              (0): BasicTransformerBlock(\n\
    \                (attn1): CrossAttention(\n                  (to_q): Linear(in_features=1280,\
    \ out_features=1280, bias=False)\n                  (to_k): Linear(in_features=1280,\
    \ out_features=1280, bias=False)\n                  (to_v): Linear(in_features=1280,\
    \ out_features=1280, bias=False)\n                  (to_out): Sequential(\n  \
    \                  (0): Linear(in_features=1280, out_features=1280, bias=True)\n\
    \                    (1): Dropout(p=0.0, inplace=False)\n                  )\n\
    \                )\n                (ff): FeedForward(\n                  (net):\
    \ Sequential(\n                    (0): GEGLU(\n                      (proj):\
    \ Linear(in_features=1280, out_features=10240, bias=True)\n                  \
    \  )\n                    (1): Dropout(p=0.0, inplace=False)\n               \
    \     (2): Linear(in_features=5120, out_features=1280, bias=True)\n          \
    \        )\n                )\n                (attn2): CrossAttention(\n    \
    \              (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n\
    \                  (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n\
    \                  (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n\
    \                  (to_out): Sequential(\n                    (0): Linear(in_features=1280,\
    \ out_features=1280, bias=True)\n                    (1): Dropout(p=0.0, inplace=False)\n\
    \                  )\n                )\n                (norm1): LayerNorm((1280,),\
    \ eps=1e-05, elementwise_affine=True)\n                (norm2): LayerNorm((1280,),\
    \ eps=1e-05, elementwise_affine=True)\n                (norm3): LayerNorm((1280,),\
    \ eps=1e-05, elementwise_affine=True)\n              )\n            )\n      \
    \      (proj_out): Linear(in_features=1280, out_features=1280, bias=True)\n  \
    \        )\n        )\n        (9): TimestepEmbedSequential(\n          (0): Downsample(\n\
    \            (op): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(2, 2), padding=(1,\
    \ 1))\n          )\n        )\n        (10-11): 2 x TimestepEmbedSequential(\n\
    \          (0): ResBlock(\n            (in_layers): Sequential(\n            \
    \  (0): GroupNormSpecific(32, 1280, eps=1e-05, affine=True)\n              (1):\
    \ SiLU()\n              (2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1,\
    \ 1), padding=(1, 1))\n            )\n            (h_upd): Identity()\n      \
    \      (x_upd): Identity()\n            (emb_layers): Sequential(\n          \
    \    (0): SiLU()\n              (1): Linear(in_features=1280, out_features=1280,\
    \ bias=True)\n            )\n            (out_layers): Sequential(\n         \
    \     (0): GroupNormSpecific(32, 1280, eps=1e-05, affine=True)\n             \
    \ (1): SiLU()\n              (2): Dropout(p=0.0, inplace=False)\n            \
    \  (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n\
    \            )\n            (skip_connection): Identity()\n            (temopral_conv):\
    \ TemporalConvBlock(\n              (conv1): Sequential(\n                (0):\
    \ GroupNorm(32, 1280, eps=1e-05, affine=True)\n                (1): SiLU()\n \
    \               (2): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1),\
    \ padding=(1, 0, 0))\n              )\n              (conv2): Sequential(\n  \
    \              (0): GroupNorm(32, 1280, eps=1e-05, affine=True)\n            \
    \    (1): SiLU()\n                (2): Dropout(p=0.1, inplace=False)\n       \
    \         (3): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1,\
    \ 0, 0))\n              )\n              (conv3): Sequential(\n              \
    \  (0): GroupNorm(32, 1280, eps=1e-05, affine=True)\n                (1): SiLU()\n\
    \                (2): Dropout(p=0.1, inplace=False)\n                (3): Conv3d(1280,\
    \ 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n        \
    \      )\n              (conv4): Sequential(\n                (0): GroupNorm(32,\
    \ 1280, eps=1e-05, affine=True)\n                (1): SiLU()\n               \
    \ (2): Dropout(p=0.1, inplace=False)\n                (3): Conv3d(1280, 1280,\
    \ kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n              )\n\
    \            )\n          )\n        )\n      )\n      (init_attn): TimestepEmbedSequential(\n\
    \        (0): TemporalTransformer(\n          (norm): GroupNorm(32, 320, eps=1e-06,\
    \ affine=True)\n          (proj_in): Conv1d(320, 512, kernel_size=(1,), stride=(1,))\n\
    \          (transformer_blocks): ModuleList(\n            (0): BasicTransformerBlock(\n\
    \              (attn1): CrossAttention(\n                (to_q): Linear(in_features=512,\
    \ out_features=512, bias=False)\n                (to_k): Linear(in_features=512,\
    \ out_features=512, bias=False)\n                (to_v): Linear(in_features=512,\
    \ out_features=512, bias=False)\n                (to_out): Sequential(\n     \
    \             (0): Linear(in_features=512, out_features=512, bias=True)\n    \
    \              (1): Dropout(p=0.0, inplace=False)\n                )\n       \
    \       )\n              (ff): FeedForward(\n                (net): Sequential(\n\
    \                  (0): GEGLU(\n                    (proj): Linear(in_features=512,\
    \ out_features=4096, bias=True)\n                  )\n                  (1): Dropout(p=0.0,\
    \ inplace=False)\n                  (2): Linear(in_features=2048, out_features=512,\
    \ bias=True)\n                )\n              )\n              (attn2): CrossAttention(\n\
    \                (to_q): Linear(in_features=512, out_features=512, bias=False)\n\
    \                (to_k): Linear(in_features=512, out_features=512, bias=False)\n\
    \                (to_v): Linear(in_features=512, out_features=512, bias=False)\n\
    \                (to_out): Sequential(\n                  (0): Linear(in_features=512,\
    \ out_features=512, bias=True)\n                  (1): Dropout(p=0.0, inplace=False)\n\
    \                )\n              )\n              (norm1): LayerNorm((512,),\
    \ eps=1e-05, elementwise_affine=True)\n              (norm2): LayerNorm((512,),\
    \ eps=1e-05, elementwise_affine=True)\n              (norm3): LayerNorm((512,),\
    \ eps=1e-05, elementwise_affine=True)\n            )\n          )\n          (proj_out):\
    \ Conv1d(512, 320, kernel_size=(1,), stride=(1,))\n        )\n      )\n      (middle_block):\
    \ TimestepEmbedSequential(\n        (0): ResBlock(\n          (in_layers): Sequential(\n\
    \            (0): GroupNormSpecific(32, 1280, eps=1e-05, affine=True)\n      \
    \      (1): SiLU()\n            (2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1,\
    \ 1), padding=(1, 1))\n          )\n          (h_upd): Identity()\n          (x_upd):\
    \ Identity()\n          (emb_layers): Sequential(\n            (0): SiLU()\n \
    \           (1): Linear(in_features=1280, out_features=1280, bias=True)\n    \
    \      )\n          (out_layers): Sequential(\n            (0): GroupNormSpecific(32,\
    \ 1280, eps=1e-05, affine=True)\n            (1): SiLU()\n            (2): Dropout(p=0.0,\
    \ inplace=False)\n            (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1,\
    \ 1), padding=(1, 1))\n          )\n          (skip_connection): Identity()\n\
    \          (temopral_conv): TemporalConvBlock(\n            (conv1): Sequential(\n\
    \              (0): GroupNorm(32, 1280, eps=1e-05, affine=True)\n            \
    \  (1): SiLU()\n              (2): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1,\
    \ 1, 1), padding=(1, 0, 0))\n            )\n            (conv2): Sequential(\n\
    \              (0): GroupNorm(32, 1280, eps=1e-05, affine=True)\n            \
    \  (1): SiLU()\n              (2): Dropout(p=0.1, inplace=False)\n           \
    \   (3): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1,\
    \ 0, 0))\n            )\n            (conv3): Sequential(\n              (0):\
    \ GroupNorm(32, 1280, eps=1e-05, affine=True)\n              (1): SiLU()\n   \
    \           (2): Dropout(p=0.1, inplace=False)\n              (3): Conv3d(1280,\
    \ 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n        \
    \    )\n            (conv4): Sequential(\n              (0): GroupNorm(32, 1280,\
    \ eps=1e-05, affine=True)\n              (1): SiLU()\n              (2): Dropout(p=0.1,\
    \ inplace=False)\n              (3): Conv3d(1280, 1280, kernel_size=(3, 1, 1),\
    \ stride=(1, 1, 1), padding=(1, 0, 0))\n            )\n          )\n        )\n\
    \        (1): SpatialTransformer(\n          (norm): GroupNorm(32, 1280, eps=1e-06,\
    \ affine=True)\n          (proj_in): Linear(in_features=1280, out_features=1280,\
    \ bias=True)\n          (transformer_blocks): ModuleList(\n            (0): BasicTransformerBlock(\n\
    \              (attn1): CrossAttention(\n                (to_q): Linear(in_features=1280,\
    \ out_features=1280, bias=False)\n                (to_k): Linear(in_features=1280,\
    \ out_features=1280, bias=False)\n                (to_v): Linear(in_features=1280,\
    \ out_features=1280, bias=False)\n                (to_out): Sequential(\n    \
    \              (0): Linear(in_features=1280, out_features=1280, bias=True)\n \
    \                 (1): Dropout(p=0.0, inplace=False)\n                )\n    \
    \          )\n              (ff): FeedForward(\n                (net): Sequential(\n\
    \                  (0): GEGLU(\n                    (proj): Linear(in_features=1280,\
    \ out_features=10240, bias=True)\n                  )\n                  (1):\
    \ Dropout(p=0.0, inplace=False)\n                  (2): Linear(in_features=5120,\
    \ out_features=1280, bias=True)\n                )\n              )\n        \
    \      (attn2): CrossAttention(\n                (to_q): Linear(in_features=1280,\
    \ out_features=1280, bias=False)\n                (to_k): Linear(in_features=1024,\
    \ out_features=1280, bias=False)\n                (to_v): Linear(in_features=1024,\
    \ out_features=1280, bias=False)\n                (to_out): Sequential(\n    \
    \              (0): Linear(in_features=1280, out_features=1280, bias=True)\n \
    \                 (1): Dropout(p=0.0, inplace=False)\n                )\n    \
    \          )\n              (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n\
    \              (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n\
    \              (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n\
    \            )\n          )\n          (proj_out): Linear(in_features=1280, out_features=1280,\
    \ bias=True)\n        )\n        (2): TemporalTransformer(\n          (norm):\
    \ GroupNorm(32, 1280, eps=1e-06, affine=True)\n          (proj_in): Linear(in_features=1280,\
    \ out_features=1280, bias=True)\n          (transformer_blocks): ModuleList(\n\
    \            (0): BasicTransformerBlock(\n              (attn1): CrossAttention(\n\
    \                (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n\
    \                (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n\
    \                (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n\
    \                (to_out): Sequential(\n                  (0): Linear(in_features=1280,\
    \ out_features=1280, bias=True)\n                  (1): Dropout(p=0.0, inplace=False)\n\
    \                )\n              )\n              (ff): FeedForward(\n      \
    \          (net): Sequential(\n                  (0): GEGLU(\n               \
    \     (proj): Linear(in_features=1280, out_features=10240, bias=True)\n      \
    \            )\n                  (1): Dropout(p=0.0, inplace=False)\n       \
    \           (2): Linear(in_features=5120, out_features=1280, bias=True)\n    \
    \            )\n              )\n              (attn2): CrossAttention(\n    \
    \            (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n\
    \                (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n\
    \                (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n\
    \                (to_out): Sequential(\n                  (0): Linear(in_features=1280,\
    \ out_features=1280, bias=True)\n                  (1): Dropout(p=0.0, inplace=False)\n\
    \                )\n              )\n              (norm1): LayerNorm((1280,),\
    \ eps=1e-05, elementwise_affine=True)\n              (norm2): LayerNorm((1280,),\
    \ eps=1e-05, elementwise_affine=True)\n              (norm3): LayerNorm((1280,),\
    \ eps=1e-05, elementwise_affine=True)\n            )\n          )\n          (proj_out):\
    \ Linear(in_features=1280, out_features=1280, bias=True)\n        )\n        (3):\
    \ ResBlock(\n          (in_layers): Sequential(\n            (0): GroupNormSpecific(32,\
    \ 1280, eps=1e-05, affine=True)\n            (1): SiLU()\n            (2): Conv2d(1280,\
    \ 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          )\n    \
    \      (h_upd): Identity()\n          (x_upd): Identity()\n          (emb_layers):\
    \ Sequential(\n            (0): SiLU()\n            (1): Linear(in_features=1280,\
    \ out_features=1280, bias=True)\n          )\n          (out_layers): Sequential(\n\
    \            (0): GroupNormSpecific(32, 1280, eps=1e-05, affine=True)\n      \
    \      (1): SiLU()\n            (2): Dropout(p=0.0, inplace=False)\n         \
    \   (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n\
    \          )\n          (skip_connection): Identity()\n          (temopral_conv):\
    \ TemporalConvBlock(\n            (conv1): Sequential(\n              (0): GroupNorm(32,\
    \ 1280, eps=1e-05, affine=True)\n              (1): SiLU()\n              (2):\
    \ Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n\
    \            )\n            (conv2): Sequential(\n              (0): GroupNorm(32,\
    \ 1280, eps=1e-05, affine=True)\n              (1): SiLU()\n              (2):\
    \ Dropout(p=0.1, inplace=False)\n              (3): Conv3d(1280, 1280, kernel_size=(3,\
    \ 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n            )\n            (conv3):\
    \ Sequential(\n              (0): GroupNorm(32, 1280, eps=1e-05, affine=True)\n\
    \              (1): SiLU()\n              (2): Dropout(p=0.1, inplace=False)\n\
    \              (3): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1),\
    \ padding=(1, 0, 0))\n            )\n            (conv4): Sequential(\n      \
    \        (0): GroupNorm(32, 1280, eps=1e-05, affine=True)\n              (1):\
    \ SiLU()\n              (2): Dropout(p=0.1, inplace=False)\n              (3):\
    \ Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n\
    \            )\n          )\n        )\n      )\n      (output_blocks): ModuleList(\n\
    \        (0-1): 2 x TimestepEmbedSequential(\n          (0): ResBlock(\n     \
    \       (in_layers): Sequential(\n              (0): GroupNormSpecific(32, 2560,\
    \ eps=1e-05, affine=True)\n              (1): SiLU()\n              (2): Conv2d(2560,\
    \ 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            )\n  \
    \          (h_upd): Identity()\n            (x_upd): Identity()\n            (emb_layers):\
    \ Sequential(\n              (0): SiLU()\n              (1): Linear(in_features=1280,\
    \ out_features=1280, bias=True)\n            )\n            (out_layers): Sequential(\n\
    \              (0): GroupNormSpecific(32, 1280, eps=1e-05, affine=True)\n    \
    \          (1): SiLU()\n              (2): Dropout(p=0.0, inplace=False)\n   \
    \           (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1,\
    \ 1))\n            )\n            (skip_connection): Conv2d(2560, 1280, kernel_size=(1,\
    \ 1), stride=(1, 1))\n            (temopral_conv): TemporalConvBlock(\n      \
    \        (conv1): Sequential(\n                (0): GroupNorm(32, 1280, eps=1e-05,\
    \ affine=True)\n                (1): SiLU()\n                (2): Conv3d(1280,\
    \ 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n        \
    \      )\n              (conv2): Sequential(\n                (0): GroupNorm(32,\
    \ 1280, eps=1e-05, affine=True)\n                (1): SiLU()\n               \
    \ (2): Dropout(p=0.1, inplace=False)\n                (3): Conv3d(1280, 1280,\
    \ kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n              )\n\
    \              (conv3): Sequential(\n                (0): GroupNorm(32, 1280,\
    \ eps=1e-05, affine=True)\n                (1): SiLU()\n                (2): Dropout(p=0.1,\
    \ inplace=False)\n                (3): Conv3d(1280, 1280, kernel_size=(3, 1, 1),\
    \ stride=(1, 1, 1), padding=(1, 0, 0))\n              )\n              (conv4):\
    \ Sequential(\n                (0): GroupNorm(32, 1280, eps=1e-05, affine=True)\n\
    \                (1): SiLU()\n                (2): Dropout(p=0.1, inplace=False)\n\
    \                (3): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1,\
    \ 1), padding=(1, 0, 0))\n              )\n            )\n          )\n      \
    \  )\n        (2): TimestepEmbedSequential(\n          (0): ResBlock(\n      \
    \      (in_layers): Sequential(\n              (0): GroupNormSpecific(32, 2560,\
    \ eps=1e-05, affine=True)\n              (1): SiLU()\n              (2): Conv2d(2560,\
    \ 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            )\n  \
    \          (h_upd): Identity()\n            (x_upd): Identity()\n            (emb_layers):\
    \ Sequential(\n              (0): SiLU()\n              (1): Linear(in_features=1280,\
    \ out_features=1280, bias=True)\n            )\n            (out_layers): Sequential(\n\
    \              (0): GroupNormSpecific(32, 1280, eps=1e-05, affine=True)\n    \
    \          (1): SiLU()\n              (2): Dropout(p=0.0, inplace=False)\n   \
    \           (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1,\
    \ 1))\n            )\n            (skip_connection): Conv2d(2560, 1280, kernel_size=(1,\
    \ 1), stride=(1, 1))\n            (temopral_conv): TemporalConvBlock(\n      \
    \        (conv1): Sequential(\n                (0): GroupNorm(32, 1280, eps=1e-05,\
    \ affine=True)\n                (1): SiLU()\n                (2): Conv3d(1280,\
    \ 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n        \
    \      )\n              (conv2): Sequential(\n                (0): GroupNorm(32,\
    \ 1280, eps=1e-05, affine=True)\n                (1): SiLU()\n               \
    \ (2): Dropout(p=0.1, inplace=False)\n                (3): Conv3d(1280, 1280,\
    \ kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n              )\n\
    \              (conv3): Sequential(\n                (0): GroupNorm(32, 1280,\
    \ eps=1e-05, affine=True)\n                (1): SiLU()\n                (2): Dropout(p=0.1,\
    \ inplace=False)\n                (3): Conv3d(1280, 1280, kernel_size=(3, 1, 1),\
    \ stride=(1, 1, 1), padding=(1, 0, 0))\n              )\n              (conv4):\
    \ Sequential(\n                (0): GroupNorm(32, 1280, eps=1e-05, affine=True)\n\
    \                (1): SiLU()\n                (2): Dropout(p=0.1, inplace=False)\n\
    \                (3): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1,\
    \ 1), padding=(1, 0, 0))\n              )\n            )\n          )\n      \
    \    (1): Upsample(\n            (conv): Conv2d(1280, 1280, kernel_size=(3, 3),\
    \ stride=(1, 1), padding=(1, 1))\n          )\n        )\n        (3-4): 2 x TimestepEmbedSequential(\n\
    \          (0): ResBlock(\n            (in_layers): Sequential(\n            \
    \  (0): GroupNormSpecific(32, 2560, eps=1e-05, affine=True)\n              (1):\
    \ SiLU()\n              (2): Conv2d(2560, 1280, kernel_size=(3, 3), stride=(1,\
    \ 1), padding=(1, 1))\n            )\n            (h_upd): Identity()\n      \
    \      (x_upd): Identity()\n            (emb_layers): Sequential(\n          \
    \    (0): SiLU()\n              (1): Linear(in_features=1280, out_features=1280,\
    \ bias=True)\n            )\n            (out_layers): Sequential(\n         \
    \     (0): GroupNormSpecific(32, 1280, eps=1e-05, affine=True)\n             \
    \ (1): SiLU()\n              (2): Dropout(p=0.0, inplace=False)\n            \
    \  (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n\
    \            )\n            (skip_connection): Conv2d(2560, 1280, kernel_size=(1,\
    \ 1), stride=(1, 1))\n            (temopral_conv): TemporalConvBlock(\n      \
    \        (conv1): Sequential(\n                (0): GroupNorm(32, 1280, eps=1e-05,\
    \ affine=True)\n                (1): SiLU()\n                (2): Conv3d(1280,\
    \ 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n        \
    \      )\n              (conv2): Sequential(\n                (0): GroupNorm(32,\
    \ 1280, eps=1e-05, affine=True)\n                (1): SiLU()\n               \
    \ (2): Dropout(p=0.1, inplace=False)\n                (3): Conv3d(1280, 1280,\
    \ kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n              )\n\
    \              (conv3): Sequential(\n                (0): GroupNorm(32, 1280,\
    \ eps=1e-05, affine=True)\n                (1): SiLU()\n                (2): Dropout(p=0.1,\
    \ inplace=False)\n                (3): Conv3d(1280, 1280, kernel_size=(3, 1, 1),\
    \ stride=(1, 1, 1), padding=(1, 0, 0))\n              )\n              (conv4):\
    \ Sequential(\n                (0): GroupNorm(32, 1280, eps=1e-05, affine=True)\n\
    \                (1): SiLU()\n                (2): Dropout(p=0.1, inplace=False)\n\
    \                (3): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1,\
    \ 1), padding=(1, 0, 0))\n              )\n            )\n          )\n      \
    \    (1): SpatialTransformer(\n            (norm): GroupNorm(32, 1280, eps=1e-06,\
    \ affine=True)\n            (proj_in): Linear(in_features=1280, out_features=1280,\
    \ bias=True)\n            (transformer_blocks): ModuleList(\n              (0):\
    \ BasicTransformerBlock(\n                (attn1): CrossAttention(\n         \
    \         (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n  \
    \                (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n\
    \                  (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n\
    \                  (to_out): Sequential(\n                    (0): Linear(in_features=1280,\
    \ out_features=1280, bias=True)\n                    (1): Dropout(p=0.0, inplace=False)\n\
    \                  )\n                )\n                (ff): FeedForward(\n\
    \                  (net): Sequential(\n                    (0): GEGLU(\n     \
    \                 (proj): Linear(in_features=1280, out_features=10240, bias=True)\n\
    \                    )\n                    (1): Dropout(p=0.0, inplace=False)\n\
    \                    (2): Linear(in_features=5120, out_features=1280, bias=True)\n\
    \                  )\n                )\n                (attn2): CrossAttention(\n\
    \                  (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n\
    \                  (to_k): Linear(in_features=1024, out_features=1280, bias=False)\n\
    \                  (to_v): Linear(in_features=1024, out_features=1280, bias=False)\n\
    \                  (to_out): Sequential(\n                    (0): Linear(in_features=1280,\
    \ out_features=1280, bias=True)\n                    (1): Dropout(p=0.0, inplace=False)\n\
    \                  )\n                )\n                (norm1): LayerNorm((1280,),\
    \ eps=1e-05, elementwise_affine=True)\n                (norm2): LayerNorm((1280,),\
    \ eps=1e-05, elementwise_affine=True)\n                (norm3): LayerNorm((1280,),\
    \ eps=1e-05, elementwise_affine=True)\n              )\n            )\n      \
    \      (proj_out): Linear(in_features=1280, out_features=1280, bias=True)\n  \
    \        )\n          (2): TemporalTransformer(\n            (norm): GroupNorm(32,\
    \ 1280, eps=1e-06, affine=True)\n            (proj_in): Linear(in_features=1280,\
    \ out_features=1280, bias=True)\n            (transformer_blocks): ModuleList(\n\
    \              (0): BasicTransformerBlock(\n                (attn1): CrossAttention(\n\
    \                  (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n\
    \                  (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n\
    \                  (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n\
    \                  (to_out): Sequential(\n                    (0): Linear(in_features=1280,\
    \ out_features=1280, bias=True)\n                    (1): Dropout(p=0.0, inplace=False)\n\
    \                  )\n                )\n                (ff): FeedForward(\n\
    \                  (net): Sequential(\n                    (0): GEGLU(\n     \
    \                 (proj): Linear(in_features=1280, out_features=10240, bias=True)\n\
    \                    )\n                    (1): Dropout(p=0.0, inplace=False)\n\
    \                    (2): Linear(in_features=5120, out_features=1280, bias=True)\n\
    \                  )\n                )\n                (attn2): CrossAttention(\n\
    \                  (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n\
    \                  (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n\
    \                  (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n\
    \                  (to_out): Sequential(\n                    (0): Linear(in_features=1280,\
    \ out_features=1280, bias=True)\n                    (1): Dropout(p=0.0, inplace=False)\n\
    \                  )\n                )\n                (norm1): LayerNorm((1280,),\
    \ eps=1e-05, elementwise_affine=True)\n                (norm2): LayerNorm((1280,),\
    \ eps=1e-05, elementwise_affine=True)\n                (norm3): LayerNorm((1280,),\
    \ eps=1e-05, elementwise_affine=True)\n              )\n            )\n      \
    \      (proj_out): Linear(in_features=1280, out_features=1280, bias=True)\n  \
    \        )\n        )\n        (5): TimestepEmbedSequential(\n          (0): ResBlock(\n\
    \            (in_layers): Sequential(\n              (0): GroupNormSpecific(32,\
    \ 1920, eps=1e-05, affine=True)\n              (1): SiLU()\n              (2):\
    \ Conv2d(1920, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    \
    \        )\n            (h_upd): Identity()\n            (x_upd): Identity()\n\
    \            (emb_layers): Sequential(\n              (0): SiLU()\n          \
    \    (1): Linear(in_features=1280, out_features=1280, bias=True)\n           \
    \ )\n            (out_layers): Sequential(\n              (0): GroupNormSpecific(32,\
    \ 1280, eps=1e-05, affine=True)\n              (1): SiLU()\n              (2):\
    \ Dropout(p=0.0, inplace=False)\n              (3): Conv2d(1280, 1280, kernel_size=(3,\
    \ 3), stride=(1, 1), padding=(1, 1))\n            )\n            (skip_connection):\
    \ Conv2d(1920, 1280, kernel_size=(1, 1), stride=(1, 1))\n            (temopral_conv):\
    \ TemporalConvBlock(\n              (conv1): Sequential(\n                (0):\
    \ GroupNorm(32, 1280, eps=1e-05, affine=True)\n                (1): SiLU()\n \
    \               (2): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1),\
    \ padding=(1, 0, 0))\n              )\n              (conv2): Sequential(\n  \
    \              (0): GroupNorm(32, 1280, eps=1e-05, affine=True)\n            \
    \    (1): SiLU()\n                (2): Dropout(p=0.1, inplace=False)\n       \
    \         (3): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1,\
    \ 0, 0))\n              )\n              (conv3): Sequential(\n              \
    \  (0): GroupNorm(32, 1280, eps=1e-05, affine=True)\n                (1): SiLU()\n\
    \                (2): Dropout(p=0.1, inplace=False)\n                (3): Conv3d(1280,\
    \ 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n        \
    \      )\n              (conv4): Sequential(\n                (0): GroupNorm(32,\
    \ 1280, eps=1e-05, affine=True)\n                (1): SiLU()\n               \
    \ (2): Dropout(p=0.1, inplace=False)\n                (3): Conv3d(1280, 1280,\
    \ kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n              )\n\
    \            )\n          )\n          (1): SpatialTransformer(\n            (norm):\
    \ GroupNorm(32, 1280, eps=1e-06, affine=True)\n            (proj_in): Linear(in_features=1280,\
    \ out_features=1280, bias=True)\n            (transformer_blocks): ModuleList(\n\
    \              (0): BasicTransformerBlock(\n                (attn1): CrossAttention(\n\
    \                  (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n\
    \                  (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n\
    \                  (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n\
    \                  (to_out): Sequential(\n                    (0): Linear(in_features=1280,\
    \ out_features=1280, bias=True)\n                    (1): Dropout(p=0.0, inplace=False)\n\
    \                  )\n                )\n                (ff): FeedForward(\n\
    \                  (net): Sequential(\n                    (0): GEGLU(\n     \
    \                 (proj): Linear(in_features=1280, out_features=10240, bias=True)\n\
    \                    )\n                    (1): Dropout(p=0.0, inplace=False)\n\
    \                    (2): Linear(in_features=5120, out_features=1280, bias=True)\n\
    \                  )\n                )\n                (attn2): CrossAttention(\n\
    \                  (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n\
    \                  (to_k): Linear(in_features=1024, out_features=1280, bias=False)\n\
    \                  (to_v): Linear(in_features=1024, out_features=1280, bias=False)\n\
    \                  (to_out): Sequential(\n                    (0): Linear(in_features=1280,\
    \ out_features=1280, bias=True)\n                    (1): Dropout(p=0.0, inplace=False)\n\
    \                  )\n                )\n                (norm1): LayerNorm((1280,),\
    \ eps=1e-05, elementwise_affine=True)\n                (norm2): LayerNorm((1280,),\
    \ eps=1e-05, elementwise_affine=True)\n                (norm3): LayerNorm((1280,),\
    \ eps=1e-05, elementwise_affine=True)\n              )\n            )\n      \
    \      (proj_out): Linear(in_features=1280, out_features=1280, bias=True)\n  \
    \        )\n          (2): TemporalTransformer(\n            (norm): GroupNorm(32,\
    \ 1280, eps=1e-06, affine=True)\n            (proj_in): Linear(in_features=1280,\
    \ out_features=1280, bias=True)\n            (transformer_blocks): ModuleList(\n\
    \              (0): BasicTransformerBlock(\n                (attn1): CrossAttention(\n\
    \                  (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n\
    \                  (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n\
    \                  (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n\
    \                  (to_out): Sequential(\n                    (0): Linear(in_features=1280,\
    \ out_features=1280, bias=True)\n                    (1): Dropout(p=0.0, inplace=False)\n\
    \                  )\n                )\n                (ff): FeedForward(\n\
    \                  (net): Sequential(\n                    (0): GEGLU(\n     \
    \                 (proj): Linear(in_features=1280, out_features=10240, bias=True)\n\
    \                    )\n                    (1): Dropout(p=0.0, inplace=False)\n\
    \                    (2): Linear(in_features=5120, out_features=1280, bias=True)\n\
    \                  )\n                )\n                (attn2): CrossAttention(\n\
    \                  (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n\
    \                  (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n\
    \                  (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n\
    \                  (to_out): Sequential(\n                    (0): Linear(in_features=1280,\
    \ out_features=1280, bias=True)\n                    (1): Dropout(p=0.0, inplace=False)\n\
    \                  )\n                )\n                (norm1): LayerNorm((1280,),\
    \ eps=1e-05, elementwise_affine=True)\n                (norm2): LayerNorm((1280,),\
    \ eps=1e-05, elementwise_affine=True)\n                (norm3): LayerNorm((1280,),\
    \ eps=1e-05, elementwise_affine=True)\n              )\n            )\n      \
    \      (proj_out): Linear(in_features=1280, out_features=1280, bias=True)\n  \
    \        )\n          (3): Upsample(\n            (conv): Conv2d(1280, 1280, kernel_size=(3,\
    \ 3), stride=(1, 1), padding=(1, 1))\n          )\n        )\n        (6): TimestepEmbedSequential(\n\
    \          (0): ResBlock(\n            (in_layers): Sequential(\n            \
    \  (0): GroupNormSpecific(32, 1920, eps=1e-05, affine=True)\n              (1):\
    \ SiLU()\n              (2): Conv2d(1920, 640, kernel_size=(3, 3), stride=(1,\
    \ 1), padding=(1, 1))\n            )\n            (h_upd): Identity()\n      \
    \      (x_upd): Identity()\n            (emb_layers): Sequential(\n          \
    \    (0): SiLU()\n              (1): Linear(in_features=1280, out_features=640,\
    \ bias=True)\n            )\n            (out_layers): Sequential(\n         \
    \     (0): GroupNormSpecific(32, 640, eps=1e-05, affine=True)\n              (1):\
    \ SiLU()\n              (2): Dropout(p=0.0, inplace=False)\n              (3):\
    \ Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      \
    \      )\n            (skip_connection): Conv2d(1920, 640, kernel_size=(1, 1),\
    \ stride=(1, 1))\n            (temopral_conv): TemporalConvBlock(\n          \
    \    (conv1): Sequential(\n                (0): GroupNorm(32, 640, eps=1e-05,\
    \ affine=True)\n                (1): SiLU()\n                (2): Conv3d(640,\
    \ 640, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n         \
    \     )\n              (conv2): Sequential(\n                (0): GroupNorm(32,\
    \ 640, eps=1e-05, affine=True)\n                (1): SiLU()\n                (2):\
    \ Dropout(p=0.1, inplace=False)\n                (3): Conv3d(640, 640, kernel_size=(3,\
    \ 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n              )\n             \
    \ (conv3): Sequential(\n                (0): GroupNorm(32, 640, eps=1e-05, affine=True)\n\
    \                (1): SiLU()\n                (2): Dropout(p=0.1, inplace=False)\n\
    \                (3): Conv3d(640, 640, kernel_size=(3, 1, 1), stride=(1, 1, 1),\
    \ padding=(1, 0, 0))\n              )\n              (conv4): Sequential(\n  \
    \              (0): GroupNorm(32, 640, eps=1e-05, affine=True)\n             \
    \   (1): SiLU()\n                (2): Dropout(p=0.1, inplace=False)\n        \
    \        (3): Conv3d(640, 640, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1,\
    \ 0, 0))\n              )\n            )\n          )\n          (1): SpatialTransformer(\n\
    \            (norm): GroupNorm(32, 640, eps=1e-06, affine=True)\n            (proj_in):\
    \ Linear(in_features=640, out_features=640, bias=True)\n            (transformer_blocks):\
    \ ModuleList(\n              (0): BasicTransformerBlock(\n                (attn1):\
    \ CrossAttention(\n                  (to_q): Linear(in_features=640, out_features=640,\
    \ bias=False)\n                  (to_k): Linear(in_features=640, out_features=640,\
    \ bias=False)\n                  (to_v): Linear(in_features=640, out_features=640,\
    \ bias=False)\n                  (to_out): Sequential(\n                    (0):\
    \ Linear(in_features=640, out_features=640, bias=True)\n                    (1):\
    \ Dropout(p=0.0, inplace=False)\n                  )\n                )\n    \
    \            (ff): FeedForward(\n                  (net): Sequential(\n      \
    \              (0): GEGLU(\n                      (proj): Linear(in_features=640,\
    \ out_features=5120, bias=True)\n                    )\n                    (1):\
    \ Dropout(p=0.0, inplace=False)\n                    (2): Linear(in_features=2560,\
    \ out_features=640, bias=True)\n                  )\n                )\n     \
    \           (attn2): CrossAttention(\n                  (to_q): Linear(in_features=640,\
    \ out_features=640, bias=False)\n                  (to_k): Linear(in_features=1024,\
    \ out_features=640, bias=False)\n                  (to_v): Linear(in_features=1024,\
    \ out_features=640, bias=False)\n                  (to_out): Sequential(\n   \
    \                 (0): Linear(in_features=640, out_features=640, bias=True)\n\
    \                    (1): Dropout(p=0.0, inplace=False)\n                  )\n\
    \                )\n                (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n\
    \                (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n\
    \                (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n\
    \              )\n            )\n            (proj_out): Linear(in_features=640,\
    \ out_features=640, bias=True)\n          )\n          (2): TemporalTransformer(\n\
    \            (norm): GroupNorm(32, 640, eps=1e-06, affine=True)\n            (proj_in):\
    \ Linear(in_features=640, out_features=640, bias=True)\n            (transformer_blocks):\
    \ ModuleList(\n              (0): BasicTransformerBlock(\n                (attn1):\
    \ CrossAttention(\n                  (to_q): Linear(in_features=640, out_features=640,\
    \ bias=False)\n                  (to_k): Linear(in_features=640, out_features=640,\
    \ bias=False)\n                  (to_v): Linear(in_features=640, out_features=640,\
    \ bias=False)\n                  (to_out): Sequential(\n                    (0):\
    \ Linear(in_features=640, out_features=640, bias=True)\n                    (1):\
    \ Dropout(p=0.0, inplace=False)\n                  )\n                )\n    \
    \            (ff): FeedForward(\n                  (net): Sequential(\n      \
    \              (0): GEGLU(\n                      (proj): Linear(in_features=640,\
    \ out_features=5120, bias=True)\n                    )\n                    (1):\
    \ Dropout(p=0.0, inplace=False)\n                    (2): Linear(in_features=2560,\
    \ out_features=640, bias=True)\n                  )\n                )\n     \
    \           (attn2): CrossAttention(\n                  (to_q): Linear(in_features=640,\
    \ out_features=640, bias=False)\n                  (to_k): Linear(in_features=640,\
    \ out_features=640, bias=False)\n                  (to_v): Linear(in_features=640,\
    \ out_features=640, bias=False)\n                  (to_out): Sequential(\n   \
    \                 (0): Linear(in_features=640, out_features=640, bias=True)\n\
    \                    (1): Dropout(p=0.0, inplace=False)\n                  )\n\
    \                )\n                (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n\
    \                (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n\
    \                (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n\
    \              )\n            )\n            (proj_out): Linear(in_features=640,\
    \ out_features=640, bias=True)\n          )\n        )\n        (7): TimestepEmbedSequential(\n\
    \          (0): ResBlock(\n            (in_layers): Sequential(\n            \
    \  (0): GroupNormSpecific(32, 1280, eps=1e-05, affine=True)\n              (1):\
    \ SiLU()\n              (2): Conv2d(1280, 640, kernel_size=(3, 3), stride=(1,\
    \ 1), padding=(1, 1))\n            )\n            (h_upd): Identity()\n      \
    \      (x_upd): Identity()\n            (emb_layers): Sequential(\n          \
    \    (0): SiLU()\n              (1): Linear(in_features=1280, out_features=640,\
    \ bias=True)\n            )\n            (out_layers): Sequential(\n         \
    \     (0): GroupNormSpecific(32, 640, eps=1e-05, affine=True)\n              (1):\
    \ SiLU()\n              (2): Dropout(p=0.0, inplace=False)\n              (3):\
    \ Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      \
    \      )\n            (skip_connection): Conv2d(1280, 640, kernel_size=(1, 1),\
    \ stride=(1, 1))\n            (temopral_conv): TemporalConvBlock(\n          \
    \    (conv1): Sequential(\n                (0): GroupNorm(32, 640, eps=1e-05,\
    \ affine=True)\n                (1): SiLU()\n                (2): Conv3d(640,\
    \ 640, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n         \
    \     )\n              (conv2): Sequential(\n                (0): GroupNorm(32,\
    \ 640, eps=1e-05, affine=True)\n                (1): SiLU()\n                (2):\
    \ Dropout(p=0.1, inplace=False)\n                (3): Conv3d(640, 640, kernel_size=(3,\
    \ 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n              )\n             \
    \ (conv3): Sequential(\n                (0): GroupNorm(32, 640, eps=1e-05, affine=True)\n\
    \                (1): SiLU()\n                (2): Dropout(p=0.1, inplace=False)\n\
    \                (3): Conv3d(640, 640, kernel_size=(3, 1, 1), stride=(1, 1, 1),\
    \ padding=(1, 0, 0))\n              )\n              (conv4): Sequential(\n  \
    \              (0): GroupNorm(32, 640, eps=1e-05, affine=True)\n             \
    \   (1): SiLU()\n                (2): Dropout(p=0.1, inplace=False)\n        \
    \        (3): Conv3d(640, 640, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1,\
    \ 0, 0))\n              )\n            )\n          )\n          (1): SpatialTransformer(\n\
    \            (norm): GroupNorm(32, 640, eps=1e-06, affine=True)\n            (proj_in):\
    \ Linear(in_features=640, out_features=640, bias=True)\n            (transformer_blocks):\
    \ ModuleList(\n              (0): BasicTransformerBlock(\n                (attn1):\
    \ CrossAttention(\n                  (to_q): Linear(in_features=640, out_features=640,\
    \ bias=False)\n                  (to_k): Linear(in_features=640, out_features=640,\
    \ bias=False)\n                  (to_v): Linear(in_features=640, out_features=640,\
    \ bias=False)\n                  (to_out): Sequential(\n                    (0):\
    \ Linear(in_features=640, out_features=640, bias=True)\n                    (1):\
    \ Dropout(p=0.0, inplace=False)\n                  )\n                )\n    \
    \            (ff): FeedForward(\n                  (net): Sequential(\n      \
    \              (0): GEGLU(\n                      (proj): Linear(in_features=640,\
    \ out_features=5120, bias=True)\n                    )\n                    (1):\
    \ Dropout(p=0.0, inplace=False)\n                    (2): Linear(in_features=2560,\
    \ out_features=640, bias=True)\n                  )\n                )\n     \
    \           (attn2): CrossAttention(\n                  (to_q): Linear(in_features=640,\
    \ out_features=640, bias=False)\n                  (to_k): Linear(in_features=1024,\
    \ out_features=640, bias=False)\n                  (to_v): Linear(in_features=1024,\
    \ out_features=640, bias=False)\n                  (to_out): Sequential(\n   \
    \                 (0): Linear(in_features=640, out_features=640, bias=True)\n\
    \                    (1): Dropout(p=0.0, inplace=False)\n                  )\n\
    \                )\n                (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n\
    \                (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n\
    \                (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n\
    \              )\n            )\n            (proj_out): Linear(in_features=640,\
    \ out_features=640, bias=True)\n          )\n          (2): TemporalTransformer(\n\
    \            (norm): GroupNorm(32, 640, eps=1e-06, affine=True)\n            (proj_in):\
    \ Linear(in_features=640, out_features=640, bias=True)\n            (transformer_blocks):\
    \ ModuleList(\n              (0): BasicTransformerBlock(\n                (attn1):\
    \ CrossAttention(\n                  (to_q): Linear(in_features=640, out_features=640,\
    \ bias=False)\n                  (to_k): Linear(in_features=640, out_features=640,\
    \ bias=False)\n                  (to_v): Linear(in_features=640, out_features=640,\
    \ bias=False)\n                  (to_out): Sequential(\n                    (0):\
    \ Linear(in_features=640, out_features=640, bias=True)\n                    (1):\
    \ Dropout(p=0.0, inplace=False)\n                  )\n                )\n    \
    \            (ff): FeedForward(\n                  (net): Sequential(\n      \
    \              (0): GEGLU(\n                      (proj): Linear(in_features=640,\
    \ out_features=5120, bias=True)\n                    )\n                    (1):\
    \ Dropout(p=0.0, inplace=False)\n                    (2): Linear(in_features=2560,\
    \ out_features=640, bias=True)\n                  )\n                )\n     \
    \           (attn2): CrossAttention(\n                  (to_q): Linear(in_features=640,\
    \ out_features=640, bias=False)\n                  (to_k): Linear(in_features=640,\
    \ out_features=640, bias=False)\n                  (to_v): Linear(in_features=640,\
    \ out_features=640, bias=False)\n                  (to_out): Sequential(\n   \
    \                 (0): Linear(in_features=640, out_features=640, bias=True)\n\
    \                    (1): Dropout(p=0.0, inplace=False)\n                  )\n\
    \                )\n                (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n\
    \                (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n\
    \                (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n\
    \              )\n            )\n            (proj_out): Linear(in_features=640,\
    \ out_features=640, bias=True)\n          )\n        )\n        (8): TimestepEmbedSequential(\n\
    \          (0): ResBlock(\n            (in_layers): Sequential(\n            \
    \  (0): GroupNormSpecific(32, 960, eps=1e-05, affine=True)\n              (1):\
    \ SiLU()\n              (2): Conv2d(960, 640, kernel_size=(3, 3), stride=(1, 1),\
    \ padding=(1, 1))\n            )\n            (h_upd): Identity()\n          \
    \  (x_upd): Identity()\n            (emb_layers): Sequential(\n              (0):\
    \ SiLU()\n              (1): Linear(in_features=1280, out_features=640, bias=True)\n\
    \            )\n            (out_layers): Sequential(\n              (0): GroupNormSpecific(32,\
    \ 640, eps=1e-05, affine=True)\n              (1): SiLU()\n              (2):\
    \ Dropout(p=0.0, inplace=False)\n              (3): Conv2d(640, 640, kernel_size=(3,\
    \ 3), stride=(1, 1), padding=(1, 1))\n            )\n            (skip_connection):\
    \ Conv2d(960, 640, kernel_size=(1, 1), stride=(1, 1))\n            (temopral_conv):\
    \ TemporalConvBlock(\n              (conv1): Sequential(\n                (0):\
    \ GroupNorm(32, 640, eps=1e-05, affine=True)\n                (1): SiLU()\n  \
    \              (2): Conv3d(640, 640, kernel_size=(3, 1, 1), stride=(1, 1, 1),\
    \ padding=(1, 0, 0))\n              )\n              (conv2): Sequential(\n  \
    \              (0): GroupNorm(32, 640, eps=1e-05, affine=True)\n             \
    \   (1): SiLU()\n                (2): Dropout(p=0.1, inplace=False)\n        \
    \        (3): Conv3d(640, 640, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1,\
    \ 0, 0))\n              )\n              (conv3): Sequential(\n              \
    \  (0): GroupNorm(32, 640, eps=1e-05, affine=True)\n                (1): SiLU()\n\
    \                (2): Dropout(p=0.1, inplace=False)\n                (3): Conv3d(640,\
    \ 640, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n         \
    \     )\n              (conv4): Sequential(\n                (0): GroupNorm(32,\
    \ 640, eps=1e-05, affine=True)\n                (1): SiLU()\n                (2):\
    \ Dropout(p=0.1, inplace=False)\n                (3): Conv3d(640, 640, kernel_size=(3,\
    \ 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n              )\n            )\n\
    \          )\n          (1): SpatialTransformer(\n            (norm): GroupNorm(32,\
    \ 640, eps=1e-06, affine=True)\n            (proj_in): Linear(in_features=640,\
    \ out_features=640, bias=True)\n            (transformer_blocks): ModuleList(\n\
    \              (0): BasicTransformerBlock(\n                (attn1): CrossAttention(\n\
    \                  (to_q): Linear(in_features=640, out_features=640, bias=False)\n\
    \                  (to_k): Linear(in_features=640, out_features=640, bias=False)\n\
    \                  (to_v): Linear(in_features=640, out_features=640, bias=False)\n\
    \                  (to_out): Sequential(\n                    (0): Linear(in_features=640,\
    \ out_features=640, bias=True)\n                    (1): Dropout(p=0.0, inplace=False)\n\
    \                  )\n                )\n                (ff): FeedForward(\n\
    \                  (net): Sequential(\n                    (0): GEGLU(\n     \
    \                 (proj): Linear(in_features=640, out_features=5120, bias=True)\n\
    \                    )\n                    (1): Dropout(p=0.0, inplace=False)\n\
    \                    (2): Linear(in_features=2560, out_features=640, bias=True)\n\
    \                  )\n                )\n                (attn2): CrossAttention(\n\
    \                  (to_q): Linear(in_features=640, out_features=640, bias=False)\n\
    \                  (to_k): Linear(in_features=1024, out_features=640, bias=False)\n\
    \                  (to_v): Linear(in_features=1024, out_features=640, bias=False)\n\
    \                  (to_out): Sequential(\n                    (0): Linear(in_features=640,\
    \ out_features=640, bias=True)\n                    (1): Dropout(p=0.0, inplace=False)\n\
    \                  )\n                )\n                (norm1): LayerNorm((640,),\
    \ eps=1e-05, elementwise_affine=True)\n                (norm2): LayerNorm((640,),\
    \ eps=1e-05, elementwise_affine=True)\n                (norm3): LayerNorm((640,),\
    \ eps=1e-05, elementwise_affine=True)\n              )\n            )\n      \
    \      (proj_out): Linear(in_features=640, out_features=640, bias=True)\n    \
    \      )\n          (2): TemporalTransformer(\n            (norm): GroupNorm(32,\
    \ 640, eps=1e-06, affine=True)\n            (proj_in): Linear(in_features=640,\
    \ out_features=640, bias=True)\n            (transformer_blocks): ModuleList(\n\
    \              (0): BasicTransformerBlock(\n                (attn1): CrossAttention(\n\
    \                  (to_q): Linear(in_features=640, out_features=640, bias=False)\n\
    \                  (to_k): Linear(in_features=640, out_features=640, bias=False)\n\
    \                  (to_v): Linear(in_features=640, out_features=640, bias=False)\n\
    \                  (to_out): Sequential(\n                    (0): Linear(in_features=640,\
    \ out_features=640, bias=True)\n                    (1): Dropout(p=0.0, inplace=False)\n\
    \                  )\n                )\n                (ff): FeedForward(\n\
    \                  (net): Sequential(\n                    (0): GEGLU(\n     \
    \                 (proj): Linear(in_features=640, out_features=5120, bias=True)\n\
    \                    )\n                    (1): Dropout(p=0.0, inplace=False)\n\
    \                    (2): Linear(in_features=2560, out_features=640, bias=True)\n\
    \                  )\n                )\n                (attn2): CrossAttention(\n\
    \                  (to_q): Linear(in_features=640, out_features=640, bias=False)\n\
    \                  (to_k): Linear(in_features=640, out_features=640, bias=False)\n\
    \                  (to_v): Linear(in_features=640, out_features=640, bias=False)\n\
    \                  (to_out): Sequential(\n                    (0): Linear(in_features=640,\
    \ out_features=640, bias=True)\n                    (1): Dropout(p=0.0, inplace=False)\n\
    \                  )\n                )\n                (norm1): LayerNorm((640,),\
    \ eps=1e-05, elementwise_affine=True)\n                (norm2): LayerNorm((640,),\
    \ eps=1e-05, elementwise_affine=True)\n                (norm3): LayerNorm((640,),\
    \ eps=1e-05, elementwise_affine=True)\n              )\n            )\n      \
    \      (proj_out): Linear(in_features=640, out_features=640, bias=True)\n    \
    \      )\n          (3): Upsample(\n            (conv): Conv2d(640, 640, kernel_size=(3,\
    \ 3), stride=(1, 1), padding=(1, 1))\n          )\n        )\n        (9): TimestepEmbedSequential(\n\
    \          (0): ResBlock(\n            (in_layers): Sequential(\n            \
    \  (0): GroupNormSpecific(32, 960, eps=1e-05, affine=True)\n              (1):\
    \ SiLU()\n              (2): Conv2d(960, 320, kernel_size=(3, 3), stride=(1, 1),\
    \ padding=(1, 1))\n            )\n            (h_upd): Identity()\n          \
    \  (x_upd): Identity()\n            (emb_layers): Sequential(\n              (0):\
    \ SiLU()\n              (1): Linear(in_features=1280, out_features=320, bias=True)\n\
    \            )\n            (out_layers): Sequential(\n              (0): GroupNormSpecific(32,\
    \ 320, eps=1e-05, affine=True)\n              (1): SiLU()\n              (2):\
    \ Dropout(p=0.0, inplace=False)\n              (3): Conv2d(320, 320, kernel_size=(3,\
    \ 3), stride=(1, 1), padding=(1, 1))\n            )\n            (skip_connection):\
    \ Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1))\n            (temopral_conv):\
    \ TemporalConvBlock(\n              (conv1): Sequential(\n                (0):\
    \ GroupNorm(32, 320, eps=1e-05, affine=True)\n                (1): SiLU()\n  \
    \              (2): Conv3d(320, 320, kernel_size=(3, 1, 1), stride=(1, 1, 1),\
    \ padding=(1, 0, 0))\n              )\n              (conv2): Sequential(\n  \
    \              (0): GroupNorm(32, 320, eps=1e-05, affine=True)\n             \
    \   (1): SiLU()\n                (2): Dropout(p=0.1, inplace=False)\n        \
    \        (3): Conv3d(320, 320, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1,\
    \ 0, 0))\n              )\n              (conv3): Sequential(\n              \
    \  (0): GroupNorm(32, 320, eps=1e-05, affine=True)\n                (1): SiLU()\n\
    \                (2): Dropout(p=0.1, inplace=False)\n                (3): Conv3d(320,\
    \ 320, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n         \
    \     )\n              (conv4): Sequential(\n                (0): GroupNorm(32,\
    \ 320, eps=1e-05, affine=True)\n                (1): SiLU()\n                (2):\
    \ Dropout(p=0.1, inplace=False)\n                (3): Conv3d(320, 320, kernel_size=(3,\
    \ 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n              )\n            )\n\
    \          )\n          (1): SpatialTransformer(\n            (norm): GroupNorm(32,\
    \ 320, eps=1e-06, affine=True)\n            (proj_in): Linear(in_features=320,\
    \ out_features=320, bias=True)\n            (transformer_blocks): ModuleList(\n\
    \              (0): BasicTransformerBlock(\n                (attn1): CrossAttention(\n\
    \                  (to_q): Linear(in_features=320, out_features=320, bias=False)\n\
    \                  (to_k): Linear(in_features=320, out_features=320, bias=False)\n\
    \                  (to_v): Linear(in_features=320, out_features=320, bias=False)\n\
    \                  (to_out): Sequential(\n                    (0): Linear(in_features=320,\
    \ out_features=320, bias=True)\n                    (1): Dropout(p=0.0, inplace=False)\n\
    \                  )\n                )\n                (ff): FeedForward(\n\
    \                  (net): Sequential(\n                    (0): GEGLU(\n     \
    \                 (proj): Linear(in_features=320, out_features=2560, bias=True)\n\
    \                    )\n                    (1): Dropout(p=0.0, inplace=False)\n\
    \                    (2): Linear(in_features=1280, out_features=320, bias=True)\n\
    \                  )\n                )\n                (attn2): CrossAttention(\n\
    \                  (to_q): Linear(in_features=320, out_features=320, bias=False)\n\
    \                  (to_k): Linear(in_features=1024, out_features=320, bias=False)\n\
    \                  (to_v): Linear(in_features=1024, out_features=320, bias=False)\n\
    \                  (to_out): Sequential(\n                    (0): Linear(in_features=320,\
    \ out_features=320, bias=True)\n                    (1): Dropout(p=0.0, inplace=False)\n\
    \                  )\n                )\n                (norm1): LayerNorm((320,),\
    \ eps=1e-05, elementwise_affine=True)\n                (norm2): LayerNorm((320,),\
    \ eps=1e-05, elementwise_affine=True)\n                (norm3): LayerNorm((320,),\
    \ eps=1e-05, elementwise_affine=True)\n              )\n            )\n      \
    \      (proj_out): Linear(in_features=320, out_features=320, bias=True)\n    \
    \      )\n          (2): TemporalTransformer(\n            (norm): GroupNorm(32,\
    \ 320, eps=1e-06, affine=True)\n            (proj_in): Linear(in_features=320,\
    \ out_features=320, bias=True)\n            (transformer_blocks): ModuleList(\n\
    \              (0): BasicTransformerBlock(\n                (attn1): CrossAttention(\n\
    \                  (to_q): Linear(in_features=320, out_features=320, bias=False)\n\
    \                  (to_k): Linear(in_features=320, out_features=320, bias=False)\n\
    \                  (to_v): Linear(in_features=320, out_features=320, bias=False)\n\
    \                  (to_out): Sequential(\n                    (0): Linear(in_features=320,\
    \ out_features=320, bias=True)\n                    (1): Dropout(p=0.0, inplace=False)\n\
    \                  )\n                )\n                (ff): FeedForward(\n\
    \                  (net): Sequential(\n                    (0): GEGLU(\n     \
    \                 (proj): Linear(in_features=320, out_features=2560, bias=True)\n\
    \                    )\n                    (1): Dropout(p=0.0, inplace=False)\n\
    \                    (2): Linear(in_features=1280, out_features=320, bias=True)\n\
    \                  )\n                )\n                (attn2): CrossAttention(\n\
    \                  (to_q): Linear(in_features=320, out_features=320, bias=False)\n\
    \                  (to_k): Linear(in_features=320, out_features=320, bias=False)\n\
    \                  (to_v): Linear(in_features=320, out_features=320, bias=False)\n\
    \                  (to_out): Sequential(\n                    (0): Linear(in_features=320,\
    \ out_features=320, bias=True)\n                    (1): Dropout(p=0.0, inplace=False)\n\
    \                  )\n                )\n                (norm1): LayerNorm((320,),\
    \ eps=1e-05, elementwise_affine=True)\n                (norm2): LayerNorm((320,),\
    \ eps=1e-05, elementwise_affine=True)\n                (norm3): LayerNorm((320,),\
    \ eps=1e-05, elementwise_affine=True)\n              )\n            )\n      \
    \      (proj_out): Linear(in_features=320, out_features=320, bias=True)\n    \
    \      )\n        )\n        (10-11): 2 x TimestepEmbedSequential(\n         \
    \ (0): ResBlock(\n            (in_layers): Sequential(\n              (0): GroupNormSpecific(32,\
    \ 640, eps=1e-05, affine=True)\n              (1): SiLU()\n              (2):\
    \ Conv2d(640, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      \
    \      )\n            (h_upd): Identity()\n            (x_upd): Identity()\n \
    \           (emb_layers): Sequential(\n              (0): SiLU()\n           \
    \   (1): Linear(in_features=1280, out_features=320, bias=True)\n            )\n\
    \            (out_layers): Sequential(\n              (0): GroupNormSpecific(32,\
    \ 320, eps=1e-05, affine=True)\n              (1): SiLU()\n              (2):\
    \ Dropout(p=0.0, inplace=False)\n              (3): Conv2d(320, 320, kernel_size=(3,\
    \ 3), stride=(1, 1), padding=(1, 1))\n            )\n            (skip_connection):\
    \ Conv2d(640, 320, kernel_size=(1, 1), stride=(1, 1))\n            (temopral_conv):\
    \ TemporalConvBlock(\n              (conv1): Sequential(\n                (0):\
    \ GroupNorm(32, 320, eps=1e-05, affine=True)\n                (1): SiLU()\n  \
    \              (2): Conv3d(320, 320, kernel_size=(3, 1, 1), stride=(1, 1, 1),\
    \ padding=(1, 0, 0))\n              )\n              (conv2): Sequential(\n  \
    \              (0): GroupNorm(32, 320, eps=1e-05, affine=True)\n             \
    \   (1): SiLU()\n                (2): Dropout(p=0.1, inplace=False)\n        \
    \        (3): Conv3d(320, 320, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1,\
    \ 0, 0))\n              )\n              (conv3): Sequential(\n              \
    \  (0): GroupNorm(32, 320, eps=1e-05, affine=True)\n                (1): SiLU()\n\
    \                (2): Dropout(p=0.1, inplace=False)\n                (3): Conv3d(320,\
    \ 320, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n         \
    \     )\n              (conv4): Sequential(\n                (0): GroupNorm(32,\
    \ 320, eps=1e-05, affine=True)\n                (1): SiLU()\n                (2):\
    \ Dropout(p=0.1, inplace=False)\n                (3): Conv3d(320, 320, kernel_size=(3,\
    \ 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n              )\n            )\n\
    \          )\n          (1): SpatialTransformer(\n            (norm): GroupNorm(32,\
    \ 320, eps=1e-06, affine=True)\n            (proj_in): Linear(in_features=320,\
    \ out_features=320, bias=True)\n            (transformer_blocks): ModuleList(\n\
    \              (0): BasicTransformerBlock(\n                (attn1): CrossAttention(\n\
    \                  (to_q): Linear(in_features=320, out_features=320, bias=False)\n\
    \                  (to_k): Linear(in_features=320, out_features=320, bias=False)\n\
    \                  (to_v): Linear(in_features=320, out_features=320, bias=False)\n\
    \                  (to_out): Sequential(\n                    (0): Linear(in_features=320,\
    \ out_features=320, bias=True)\n                    (1): Dropout(p=0.0, inplace=False)\n\
    \                  )\n                )\n                (ff): FeedForward(\n\
    \                  (net): Sequential(\n                    (0): GEGLU(\n     \
    \                 (proj): Linear(in_features=320, out_features=2560, bias=True)\n\
    \                    )\n                    (1): Dropout(p=0.0, inplace=False)\n\
    \                    (2): Linear(in_features=1280, out_features=320, bias=True)\n\
    \                  )\n                )\n                (attn2): CrossAttention(\n\
    \                  (to_q): Linear(in_features=320, out_features=320, bias=False)\n\
    \                  (to_k): Linear(in_features=1024, out_features=320, bias=False)\n\
    \                  (to_v): Linear(in_features=1024, out_features=320, bias=False)\n\
    \                  (to_out): Sequential(\n                    (0): Linear(in_features=320,\
    \ out_features=320, bias=True)\n                    (1): Dropout(p=0.0, inplace=False)\n\
    \                  )\n                )\n                (norm1): LayerNorm((320,),\
    \ eps=1e-05, elementwise_affine=True)\n                (norm2): LayerNorm((320,),\
    \ eps=1e-05, elementwise_affine=True)\n                (norm3): LayerNorm((320,),\
    \ eps=1e-05, elementwise_affine=True)\n              )\n            )\n      \
    \      (proj_out): Linear(in_features=320, out_features=320, bias=True)\n    \
    \      )\n          (2): TemporalTransformer(\n            (norm): GroupNorm(32,\
    \ 320, eps=1e-06, affine=True)\n            (proj_in): Linear(in_features=320,\
    \ out_features=320, bias=True)\n            (transformer_blocks): ModuleList(\n\
    \              (0): BasicTransformerBlock(\n                (attn1): CrossAttention(\n\
    \                  (to_q): Linear(in_features=320, out_features=320, bias=False)\n\
    \                  (to_k): Linear(in_features=320, out_features=320, bias=False)\n\
    \                  (to_v): Linear(in_features=320, out_features=320, bias=False)\n\
    \                  (to_out): Sequential(\n                    (0): Linear(in_features=320,\
    \ out_features=320, bias=True)\n                    (1): Dropout(p=0.0, inplace=False)\n\
    \                  )\n                )\n                (ff): FeedForward(\n\
    \                  (net): Sequential(\n                    (0): GEGLU(\n     \
    \                 (proj): Linear(in_features=320, out_features=2560, bias=True)\n\
    \                    )\n                    (1): Dropout(p=0.0, inplace=False)\n\
    \                    (2): Linear(in_features=1280, out_features=320, bias=True)\n\
    \                  )\n                )\n                (attn2): CrossAttention(\n\
    \                  (to_q): Linear(in_features=320, out_features=320, bias=False)\n\
    \                  (to_k): Linear(in_features=320, out_features=320, bias=False)\n\
    \                  (to_v): Linear(in_features=320, out_features=320, bias=False)\n\
    \                  (to_out): Sequential(\n                    (0): Linear(in_features=320,\
    \ out_features=320, bias=True)\n                    (1): Dropout(p=0.0, inplace=False)\n\
    \                  )\n                )\n                (norm1): LayerNorm((320,),\
    \ eps=1e-05, elementwise_affine=True)\n                (norm2): LayerNorm((320,),\
    \ eps=1e-05, elementwise_affine=True)\n                (norm3): LayerNorm((320,),\
    \ eps=1e-05, elementwise_affine=True)\n              )\n            )\n      \
    \      (proj_out): Linear(in_features=320, out_features=320, bias=True)\n    \
    \      )\n        )\n      )\n      (out): Sequential(\n        (0): GroupNormSpecific(32,\
    \ 320, eps=1e-05, affine=True)\n        (1): SiLU()\n        (2): Conv2d(320,\
    \ 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      )\n    )\n  )\n\
    \  (first_stage_model): AutoencoderKL(\n    (encoder): Encoder(\n      (conv_in):\
    \ Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (down):\
    \ ModuleList(\n        (0): Module(\n          (block): ModuleList(\n        \
    \    (0-1): 2 x ResnetBlock(\n              (norm1): GroupNorm(32, 128, eps=1e-06,\
    \ affine=True)\n              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1,\
    \ 1), padding=(1, 1))\n              (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n\
    \              (dropout): Dropout(p=0.0, inplace=False)\n              (conv2):\
    \ Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      \
    \      )\n          )\n          (attn): ModuleList()\n          (downsample):\
    \ Downsample(\n            (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2,\
    \ 2))\n          )\n        )\n        (1): Module(\n          (block): ModuleList(\n\
    \            (0): ResnetBlock(\n              (norm1): GroupNorm(32, 128, eps=1e-06,\
    \ affine=True)\n              (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1,\
    \ 1), padding=(1, 1))\n              (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n\
    \              (dropout): Dropout(p=0.0, inplace=False)\n              (conv2):\
    \ Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      \
    \        (nin_shortcut): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n\
    \            )\n            (1): ResnetBlock(\n              (norm1): GroupNorm(32,\
    \ 256, eps=1e-06, affine=True)\n              (conv1): Conv2d(256, 256, kernel_size=(3,\
    \ 3), stride=(1, 1), padding=(1, 1))\n              (norm2): GroupNorm(32, 256,\
    \ eps=1e-06, affine=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n\
    \              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1,\
    \ 1))\n            )\n          )\n          (attn): ModuleList()\n          (downsample):\
    \ Downsample(\n            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2,\
    \ 2))\n          )\n        )\n        (2): Module(\n          (block): ModuleList(\n\
    \            (0): ResnetBlock(\n              (norm1): GroupNorm(32, 256, eps=1e-06,\
    \ affine=True)\n              (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1,\
    \ 1), padding=(1, 1))\n              (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n\
    \              (dropout): Dropout(p=0.0, inplace=False)\n              (conv2):\
    \ Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      \
    \        (nin_shortcut): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))\n\
    \            )\n            (1): ResnetBlock(\n              (norm1): GroupNorm(32,\
    \ 512, eps=1e-06, affine=True)\n              (conv1): Conv2d(512, 512, kernel_size=(3,\
    \ 3), stride=(1, 1), padding=(1, 1))\n              (norm2): GroupNorm(32, 512,\
    \ eps=1e-06, affine=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n\
    \              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1,\
    \ 1))\n            )\n          )\n          (attn): ModuleList()\n          (downsample):\
    \ Downsample(\n            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(2,\
    \ 2))\n          )\n        )\n        (3): Module(\n          (block): ModuleList(\n\
    \            (0-1): 2 x ResnetBlock(\n              (norm1): GroupNorm(32, 512,\
    \ eps=1e-06, affine=True)\n              (conv1): Conv2d(512, 512, kernel_size=(3,\
    \ 3), stride=(1, 1), padding=(1, 1))\n              (norm2): GroupNorm(32, 512,\
    \ eps=1e-06, affine=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n\
    \              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1,\
    \ 1))\n            )\n          )\n          (attn): ModuleList()\n        )\n\
    \      )\n      (mid): Module(\n        (block_1): ResnetBlock(\n          (norm1):\
    \ GroupNorm(32, 512, eps=1e-06, affine=True)\n          (conv1): Conv2d(512, 512,\
    \ kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (norm2): GroupNorm(32,\
    \ 512, eps=1e-06, affine=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n\
    \          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1,\
    \ 1))\n        )\n        (attn_1): AttnBlock(\n          (norm): GroupNorm(32,\
    \ 512, eps=1e-06, affine=True)\n          (q): Conv2d(512, 512, kernel_size=(1,\
    \ 1), stride=(1, 1))\n          (k): Conv2d(512, 512, kernel_size=(1, 1), stride=(1,\
    \ 1))\n          (v): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n  \
    \        (proj_out): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n   \
    \     )\n        (block_2): ResnetBlock(\n          (norm1): GroupNorm(32, 512,\
    \ eps=1e-06, affine=True)\n          (conv1): Conv2d(512, 512, kernel_size=(3,\
    \ 3), stride=(1, 1), padding=(1, 1))\n          (norm2): GroupNorm(32, 512, eps=1e-06,\
    \ affine=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n         \
    \ (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n\
    \        )\n      )\n      (norm_out): GroupNorm(32, 512, eps=1e-06, affine=True)\n\
    \      (conv_out): Conv2d(512, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1,\
    \ 1))\n    )\n    (decoder): Decoder(\n      (conv_in): Conv2d(4, 512, kernel_size=(3,\
    \ 3), stride=(1, 1), padding=(1, 1))\n      (mid): Module(\n        (block_1):\
    \ ResnetBlock(\n          (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n\
    \          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1,\
    \ 1))\n          (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n       \
    \   (dropout): Dropout(p=0.0, inplace=False)\n          (conv2): Conv2d(512, 512,\
    \ kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        )\n        (attn_1):\
    \ AttnBlock(\n          (norm): GroupNorm(32, 512, eps=1e-06, affine=True)\n \
    \         (q): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n         \
    \ (k): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n          (v): Conv2d(512,\
    \ 512, kernel_size=(1, 1), stride=(1, 1))\n          (proj_out): Conv2d(512, 512,\
    \ kernel_size=(1, 1), stride=(1, 1))\n        )\n        (block_2): ResnetBlock(\n\
    \          (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n          (conv1):\
    \ Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      \
    \    (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n          (dropout):\
    \ Dropout(p=0.0, inplace=False)\n          (conv2): Conv2d(512, 512, kernel_size=(3,\
    \ 3), stride=(1, 1), padding=(1, 1))\n        )\n      )\n      (up): ModuleList(\n\
    \        (0): Module(\n          (block): ModuleList(\n            (0): ResnetBlock(\n\
    \              (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n         \
    \     (conv1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1,\
    \ 1))\n              (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n   \
    \           (dropout): Dropout(p=0.0, inplace=False)\n              (conv2): Conv2d(128,\
    \ 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n              (nin_shortcut):\
    \ Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n            )\n       \
    \     (1-2): 2 x ResnetBlock(\n              (norm1): GroupNorm(32, 128, eps=1e-06,\
    \ affine=True)\n              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1,\
    \ 1), padding=(1, 1))\n              (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n\
    \              (dropout): Dropout(p=0.0, inplace=False)\n              (conv2):\
    \ Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      \
    \      )\n          )\n          (attn): ModuleList()\n        )\n        (1):\
    \ Module(\n          (block): ModuleList(\n            (0): ResnetBlock(\n   \
    \           (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n            \
    \  (conv1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n\
    \              (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n         \
    \     (dropout): Dropout(p=0.0, inplace=False)\n              (conv2): Conv2d(256,\
    \ 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n              (nin_shortcut):\
    \ Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n            )\n       \
    \     (1-2): 2 x ResnetBlock(\n              (norm1): GroupNorm(32, 256, eps=1e-06,\
    \ affine=True)\n              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1,\
    \ 1), padding=(1, 1))\n              (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n\
    \              (dropout): Dropout(p=0.0, inplace=False)\n              (conv2):\
    \ Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      \
    \      )\n          )\n          (attn): ModuleList()\n          (upsample): Upsample(\n\
    \            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1,\
    \ 1))\n          )\n        )\n        (2-3): 2 x Module(\n          (block):\
    \ ModuleList(\n            (0-2): 3 x ResnetBlock(\n              (norm1): GroupNorm(32,\
    \ 512, eps=1e-06, affine=True)\n              (conv1): Conv2d(512, 512, kernel_size=(3,\
    \ 3), stride=(1, 1), padding=(1, 1))\n              (norm2): GroupNorm(32, 512,\
    \ eps=1e-06, affine=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n\
    \              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1,\
    \ 1))\n            )\n          )\n          (attn): ModuleList()\n          (upsample):\
    \ Upsample(\n            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1,\
    \ 1), padding=(1, 1))\n          )\n        )\n      )\n      (norm_out): GroupNorm(32,\
    \ 128, eps=1e-06, affine=True)\n      (conv_out): Conv2d(128, 3, kernel_size=(3,\
    \ 3), stride=(1, 1), padding=(1, 1))\n    )\n    (loss): Identity()\n    (quant_conv):\
    \ Conv2d(8, 8, kernel_size=(1, 1), stride=(1, 1))\n    (post_quant_conv): Conv2d(4,\
    \ 4, kernel_size=(1, 1), stride=(1, 1))\n  )\n  (cond_stage_model): FrozenOpenCLIPEmbedder(\n\
    \    (model): CLIP(\n      (transformer): Transformer(\n        (resblocks): ModuleList(\n\
    \          (0-23): 24 x ResidualAttentionBlock(\n            (ln_1): LayerNorm((1024,),\
    \ eps=1e-05, elementwise_affine=True)\n            (attn): MultiheadAttention(\n\
    \              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024,\
    \ bias=True)\n            )\n            (ls_1): Identity()\n            (ln_2):\
    \ LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (mlp): Sequential(\n\
    \              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n\
    \              (gelu): GELU(approximate='none')\n              (c_proj): Linear(in_features=4096,\
    \ out_features=1024, bias=True)\n            )\n            (ls_2): Identity()\n\
    \          )\n        )\n      )\n      (token_embedding): Embedding(49408, 1024)\n\
    \      (ln_final): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n  \
    \  )\n  )\n)"
teacher_model:
  desc: null
  value: "LatentDiffusion(\n  (model): DiffusionWrapper(\n    (diffusion_model): UNetModel(\n\
    \      (time_embed): Sequential(\n        (0): Linear(in_features=320, out_features=1280,\
    \ bias=True)\n        (1): SiLU()\n        (2): Linear(in_features=1280, out_features=1280,\
    \ bias=True)\n      )\n      (fps_embedding): Sequential(\n        (0): Linear(in_features=320,\
    \ out_features=1280, bias=True)\n        (1): SiLU()\n        (2): Linear(in_features=1280,\
    \ out_features=1280, bias=True)\n      )\n      (input_blocks): ModuleList(\n\
    \        (0): TimestepEmbedSequential(\n          (0): Conv2d(4, 320, kernel_size=(3,\
    \ 3), stride=(1, 1), padding=(1, 1))\n        )\n        (1-2): 2 x TimestepEmbedSequential(\n\
    \          (0): ResBlock(\n            (in_layers): Sequential(\n            \
    \  (0): GroupNormSpecific(32, 320, eps=1e-05, affine=True)\n              (1):\
    \ SiLU()\n              (2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1),\
    \ padding=(1, 1))\n            )\n            (h_upd): Identity()\n          \
    \  (x_upd): Identity()\n            (emb_layers): Sequential(\n              (0):\
    \ SiLU()\n              (1): Linear(in_features=1280, out_features=320, bias=True)\n\
    \            )\n            (out_layers): Sequential(\n              (0): GroupNormSpecific(32,\
    \ 320, eps=1e-05, affine=True)\n              (1): SiLU()\n              (2):\
    \ Dropout(p=0.0, inplace=False)\n              (3): Conv2d(320, 320, kernel_size=(3,\
    \ 3), stride=(1, 1), padding=(1, 1))\n            )\n            (skip_connection):\
    \ Identity()\n            (temopral_conv): TemporalConvBlock(\n              (conv1):\
    \ Sequential(\n                (0): GroupNorm(32, 320, eps=1e-05, affine=True)\n\
    \                (1): SiLU()\n                (2): Conv3d(320, 320, kernel_size=(3,\
    \ 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n              )\n             \
    \ (conv2): Sequential(\n                (0): GroupNorm(32, 320, eps=1e-05, affine=True)\n\
    \                (1): SiLU()\n                (2): Dropout(p=0.1, inplace=False)\n\
    \                (3): Conv3d(320, 320, kernel_size=(3, 1, 1), stride=(1, 1, 1),\
    \ padding=(1, 0, 0))\n              )\n              (conv3): Sequential(\n  \
    \              (0): GroupNorm(32, 320, eps=1e-05, affine=True)\n             \
    \   (1): SiLU()\n                (2): Dropout(p=0.1, inplace=False)\n        \
    \        (3): Conv3d(320, 320, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1,\
    \ 0, 0))\n              )\n              (conv4): Sequential(\n              \
    \  (0): GroupNorm(32, 320, eps=1e-05, affine=True)\n                (1): SiLU()\n\
    \                (2): Dropout(p=0.1, inplace=False)\n                (3): Conv3d(320,\
    \ 320, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n         \
    \     )\n            )\n          )\n          (1): SpatialTransformer(\n    \
    \        (norm): GroupNorm(32, 320, eps=1e-06, affine=True)\n            (proj_in):\
    \ Linear(in_features=320, out_features=320, bias=True)\n            (transformer_blocks):\
    \ ModuleList(\n              (0): BasicTransformerBlock(\n                (attn1):\
    \ CrossAttention(\n                  (to_q): Linear(in_features=320, out_features=320,\
    \ bias=False)\n                  (to_k): Linear(in_features=320, out_features=320,\
    \ bias=False)\n                  (to_v): Linear(in_features=320, out_features=320,\
    \ bias=False)\n                  (to_out): Sequential(\n                    (0):\
    \ Linear(in_features=320, out_features=320, bias=True)\n                    (1):\
    \ Dropout(p=0.0, inplace=False)\n                  )\n                )\n    \
    \            (ff): FeedForward(\n                  (net): Sequential(\n      \
    \              (0): GEGLU(\n                      (proj): Linear(in_features=320,\
    \ out_features=2560, bias=True)\n                    )\n                    (1):\
    \ Dropout(p=0.0, inplace=False)\n                    (2): Linear(in_features=1280,\
    \ out_features=320, bias=True)\n                  )\n                )\n     \
    \           (attn2): CrossAttention(\n                  (to_q): Linear(in_features=320,\
    \ out_features=320, bias=False)\n                  (to_k): Linear(in_features=1024,\
    \ out_features=320, bias=False)\n                  (to_v): Linear(in_features=1024,\
    \ out_features=320, bias=False)\n                  (to_out): Sequential(\n   \
    \                 (0): Linear(in_features=320, out_features=320, bias=True)\n\
    \                    (1): Dropout(p=0.0, inplace=False)\n                  )\n\
    \                )\n                (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n\
    \                (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n\
    \                (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n\
    \              )\n            )\n            (proj_out): Linear(in_features=320,\
    \ out_features=320, bias=True)\n          )\n          (2): TemporalTransformer(\n\
    \            (norm): GroupNorm(32, 320, eps=1e-06, affine=True)\n            (proj_in):\
    \ Linear(in_features=320, out_features=320, bias=True)\n            (transformer_blocks):\
    \ ModuleList(\n              (0): BasicTransformerBlock(\n                (attn1):\
    \ CrossAttention(\n                  (to_q): Linear(in_features=320, out_features=320,\
    \ bias=False)\n                  (to_k): Linear(in_features=320, out_features=320,\
    \ bias=False)\n                  (to_v): Linear(in_features=320, out_features=320,\
    \ bias=False)\n                  (to_out): Sequential(\n                    (0):\
    \ Linear(in_features=320, out_features=320, bias=True)\n                    (1):\
    \ Dropout(p=0.0, inplace=False)\n                  )\n                )\n    \
    \            (ff): FeedForward(\n                  (net): Sequential(\n      \
    \              (0): GEGLU(\n                      (proj): Linear(in_features=320,\
    \ out_features=2560, bias=True)\n                    )\n                    (1):\
    \ Dropout(p=0.0, inplace=False)\n                    (2): Linear(in_features=1280,\
    \ out_features=320, bias=True)\n                  )\n                )\n     \
    \           (attn2): CrossAttention(\n                  (to_q): Linear(in_features=320,\
    \ out_features=320, bias=False)\n                  (to_k): Linear(in_features=320,\
    \ out_features=320, bias=False)\n                  (to_v): Linear(in_features=320,\
    \ out_features=320, bias=False)\n                  (to_out): Sequential(\n   \
    \                 (0): Linear(in_features=320, out_features=320, bias=True)\n\
    \                    (1): Dropout(p=0.0, inplace=False)\n                  )\n\
    \                )\n                (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n\
    \                (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n\
    \                (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n\
    \              )\n            )\n            (proj_out): Linear(in_features=320,\
    \ out_features=320, bias=True)\n          )\n        )\n        (3): TimestepEmbedSequential(\n\
    \          (0): Downsample(\n            (op): Conv2d(320, 320, kernel_size=(3,\
    \ 3), stride=(2, 2), padding=(1, 1))\n          )\n        )\n        (4): TimestepEmbedSequential(\n\
    \          (0): ResBlock(\n            (in_layers): Sequential(\n            \
    \  (0): GroupNormSpecific(32, 320, eps=1e-05, affine=True)\n              (1):\
    \ SiLU()\n              (2): Conv2d(320, 640, kernel_size=(3, 3), stride=(1, 1),\
    \ padding=(1, 1))\n            )\n            (h_upd): Identity()\n          \
    \  (x_upd): Identity()\n            (emb_layers): Sequential(\n              (0):\
    \ SiLU()\n              (1): Linear(in_features=1280, out_features=640, bias=True)\n\
    \            )\n            (out_layers): Sequential(\n              (0): GroupNormSpecific(32,\
    \ 640, eps=1e-05, affine=True)\n              (1): SiLU()\n              (2):\
    \ Dropout(p=0.0, inplace=False)\n              (3): Conv2d(640, 640, kernel_size=(3,\
    \ 3), stride=(1, 1), padding=(1, 1))\n            )\n            (skip_connection):\
    \ Conv2d(320, 640, kernel_size=(1, 1), stride=(1, 1))\n            (temopral_conv):\
    \ TemporalConvBlock(\n              (conv1): Sequential(\n                (0):\
    \ GroupNorm(32, 640, eps=1e-05, affine=True)\n                (1): SiLU()\n  \
    \              (2): Conv3d(640, 640, kernel_size=(3, 1, 1), stride=(1, 1, 1),\
    \ padding=(1, 0, 0))\n              )\n              (conv2): Sequential(\n  \
    \              (0): GroupNorm(32, 640, eps=1e-05, affine=True)\n             \
    \   (1): SiLU()\n                (2): Dropout(p=0.1, inplace=False)\n        \
    \        (3): Conv3d(640, 640, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1,\
    \ 0, 0))\n              )\n              (conv3): Sequential(\n              \
    \  (0): GroupNorm(32, 640, eps=1e-05, affine=True)\n                (1): SiLU()\n\
    \                (2): Dropout(p=0.1, inplace=False)\n                (3): Conv3d(640,\
    \ 640, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n         \
    \     )\n              (conv4): Sequential(\n                (0): GroupNorm(32,\
    \ 640, eps=1e-05, affine=True)\n                (1): SiLU()\n                (2):\
    \ Dropout(p=0.1, inplace=False)\n                (3): Conv3d(640, 640, kernel_size=(3,\
    \ 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n              )\n            )\n\
    \          )\n          (1): SpatialTransformer(\n            (norm): GroupNorm(32,\
    \ 640, eps=1e-06, affine=True)\n            (proj_in): Linear(in_features=640,\
    \ out_features=640, bias=True)\n            (transformer_blocks): ModuleList(\n\
    \              (0): BasicTransformerBlock(\n                (attn1): CrossAttention(\n\
    \                  (to_q): Linear(in_features=640, out_features=640, bias=False)\n\
    \                  (to_k): Linear(in_features=640, out_features=640, bias=False)\n\
    \                  (to_v): Linear(in_features=640, out_features=640, bias=False)\n\
    \                  (to_out): Sequential(\n                    (0): Linear(in_features=640,\
    \ out_features=640, bias=True)\n                    (1): Dropout(p=0.0, inplace=False)\n\
    \                  )\n                )\n                (ff): FeedForward(\n\
    \                  (net): Sequential(\n                    (0): GEGLU(\n     \
    \                 (proj): Linear(in_features=640, out_features=5120, bias=True)\n\
    \                    )\n                    (1): Dropout(p=0.0, inplace=False)\n\
    \                    (2): Linear(in_features=2560, out_features=640, bias=True)\n\
    \                  )\n                )\n                (attn2): CrossAttention(\n\
    \                  (to_q): Linear(in_features=640, out_features=640, bias=False)\n\
    \                  (to_k): Linear(in_features=1024, out_features=640, bias=False)\n\
    \                  (to_v): Linear(in_features=1024, out_features=640, bias=False)\n\
    \                  (to_out): Sequential(\n                    (0): Linear(in_features=640,\
    \ out_features=640, bias=True)\n                    (1): Dropout(p=0.0, inplace=False)\n\
    \                  )\n                )\n                (norm1): LayerNorm((640,),\
    \ eps=1e-05, elementwise_affine=True)\n                (norm2): LayerNorm((640,),\
    \ eps=1e-05, elementwise_affine=True)\n                (norm3): LayerNorm((640,),\
    \ eps=1e-05, elementwise_affine=True)\n              )\n            )\n      \
    \      (proj_out): Linear(in_features=640, out_features=640, bias=True)\n    \
    \      )\n          (2): TemporalTransformer(\n            (norm): GroupNorm(32,\
    \ 640, eps=1e-06, affine=True)\n            (proj_in): Linear(in_features=640,\
    \ out_features=640, bias=True)\n            (transformer_blocks): ModuleList(\n\
    \              (0): BasicTransformerBlock(\n                (attn1): CrossAttention(\n\
    \                  (to_q): Linear(in_features=640, out_features=640, bias=False)\n\
    \                  (to_k): Linear(in_features=640, out_features=640, bias=False)\n\
    \                  (to_v): Linear(in_features=640, out_features=640, bias=False)\n\
    \                  (to_out): Sequential(\n                    (0): Linear(in_features=640,\
    \ out_features=640, bias=True)\n                    (1): Dropout(p=0.0, inplace=False)\n\
    \                  )\n                )\n                (ff): FeedForward(\n\
    \                  (net): Sequential(\n                    (0): GEGLU(\n     \
    \                 (proj): Linear(in_features=640, out_features=5120, bias=True)\n\
    \                    )\n                    (1): Dropout(p=0.0, inplace=False)\n\
    \                    (2): Linear(in_features=2560, out_features=640, bias=True)\n\
    \                  )\n                )\n                (attn2): CrossAttention(\n\
    \                  (to_q): Linear(in_features=640, out_features=640, bias=False)\n\
    \                  (to_k): Linear(in_features=640, out_features=640, bias=False)\n\
    \                  (to_v): Linear(in_features=640, out_features=640, bias=False)\n\
    \                  (to_out): Sequential(\n                    (0): Linear(in_features=640,\
    \ out_features=640, bias=True)\n                    (1): Dropout(p=0.0, inplace=False)\n\
    \                  )\n                )\n                (norm1): LayerNorm((640,),\
    \ eps=1e-05, elementwise_affine=True)\n                (norm2): LayerNorm((640,),\
    \ eps=1e-05, elementwise_affine=True)\n                (norm3): LayerNorm((640,),\
    \ eps=1e-05, elementwise_affine=True)\n              )\n            )\n      \
    \      (proj_out): Linear(in_features=640, out_features=640, bias=True)\n    \
    \      )\n        )\n        (5): TimestepEmbedSequential(\n          (0): ResBlock(\n\
    \            (in_layers): Sequential(\n              (0): GroupNormSpecific(32,\
    \ 640, eps=1e-05, affine=True)\n              (1): SiLU()\n              (2):\
    \ Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      \
    \      )\n            (h_upd): Identity()\n            (x_upd): Identity()\n \
    \           (emb_layers): Sequential(\n              (0): SiLU()\n           \
    \   (1): Linear(in_features=1280, out_features=640, bias=True)\n            )\n\
    \            (out_layers): Sequential(\n              (0): GroupNormSpecific(32,\
    \ 640, eps=1e-05, affine=True)\n              (1): SiLU()\n              (2):\
    \ Dropout(p=0.0, inplace=False)\n              (3): Conv2d(640, 640, kernel_size=(3,\
    \ 3), stride=(1, 1), padding=(1, 1))\n            )\n            (skip_connection):\
    \ Identity()\n            (temopral_conv): TemporalConvBlock(\n              (conv1):\
    \ Sequential(\n                (0): GroupNorm(32, 640, eps=1e-05, affine=True)\n\
    \                (1): SiLU()\n                (2): Conv3d(640, 640, kernel_size=(3,\
    \ 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n              )\n             \
    \ (conv2): Sequential(\n                (0): GroupNorm(32, 640, eps=1e-05, affine=True)\n\
    \                (1): SiLU()\n                (2): Dropout(p=0.1, inplace=False)\n\
    \                (3): Conv3d(640, 640, kernel_size=(3, 1, 1), stride=(1, 1, 1),\
    \ padding=(1, 0, 0))\n              )\n              (conv3): Sequential(\n  \
    \              (0): GroupNorm(32, 640, eps=1e-05, affine=True)\n             \
    \   (1): SiLU()\n                (2): Dropout(p=0.1, inplace=False)\n        \
    \        (3): Conv3d(640, 640, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1,\
    \ 0, 0))\n              )\n              (conv4): Sequential(\n              \
    \  (0): GroupNorm(32, 640, eps=1e-05, affine=True)\n                (1): SiLU()\n\
    \                (2): Dropout(p=0.1, inplace=False)\n                (3): Conv3d(640,\
    \ 640, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n         \
    \     )\n            )\n          )\n          (1): SpatialTransformer(\n    \
    \        (norm): GroupNorm(32, 640, eps=1e-06, affine=True)\n            (proj_in):\
    \ Linear(in_features=640, out_features=640, bias=True)\n            (transformer_blocks):\
    \ ModuleList(\n              (0): BasicTransformerBlock(\n                (attn1):\
    \ CrossAttention(\n                  (to_q): Linear(in_features=640, out_features=640,\
    \ bias=False)\n                  (to_k): Linear(in_features=640, out_features=640,\
    \ bias=False)\n                  (to_v): Linear(in_features=640, out_features=640,\
    \ bias=False)\n                  (to_out): Sequential(\n                    (0):\
    \ Linear(in_features=640, out_features=640, bias=True)\n                    (1):\
    \ Dropout(p=0.0, inplace=False)\n                  )\n                )\n    \
    \            (ff): FeedForward(\n                  (net): Sequential(\n      \
    \              (0): GEGLU(\n                      (proj): Linear(in_features=640,\
    \ out_features=5120, bias=True)\n                    )\n                    (1):\
    \ Dropout(p=0.0, inplace=False)\n                    (2): Linear(in_features=2560,\
    \ out_features=640, bias=True)\n                  )\n                )\n     \
    \           (attn2): CrossAttention(\n                  (to_q): Linear(in_features=640,\
    \ out_features=640, bias=False)\n                  (to_k): Linear(in_features=1024,\
    \ out_features=640, bias=False)\n                  (to_v): Linear(in_features=1024,\
    \ out_features=640, bias=False)\n                  (to_out): Sequential(\n   \
    \                 (0): Linear(in_features=640, out_features=640, bias=True)\n\
    \                    (1): Dropout(p=0.0, inplace=False)\n                  )\n\
    \                )\n                (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n\
    \                (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n\
    \                (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n\
    \              )\n            )\n            (proj_out): Linear(in_features=640,\
    \ out_features=640, bias=True)\n          )\n          (2): TemporalTransformer(\n\
    \            (norm): GroupNorm(32, 640, eps=1e-06, affine=True)\n            (proj_in):\
    \ Linear(in_features=640, out_features=640, bias=True)\n            (transformer_blocks):\
    \ ModuleList(\n              (0): BasicTransformerBlock(\n                (attn1):\
    \ CrossAttention(\n                  (to_q): Linear(in_features=640, out_features=640,\
    \ bias=False)\n                  (to_k): Linear(in_features=640, out_features=640,\
    \ bias=False)\n                  (to_v): Linear(in_features=640, out_features=640,\
    \ bias=False)\n                  (to_out): Sequential(\n                    (0):\
    \ Linear(in_features=640, out_features=640, bias=True)\n                    (1):\
    \ Dropout(p=0.0, inplace=False)\n                  )\n                )\n    \
    \            (ff): FeedForward(\n                  (net): Sequential(\n      \
    \              (0): GEGLU(\n                      (proj): Linear(in_features=640,\
    \ out_features=5120, bias=True)\n                    )\n                    (1):\
    \ Dropout(p=0.0, inplace=False)\n                    (2): Linear(in_features=2560,\
    \ out_features=640, bias=True)\n                  )\n                )\n     \
    \           (attn2): CrossAttention(\n                  (to_q): Linear(in_features=640,\
    \ out_features=640, bias=False)\n                  (to_k): Linear(in_features=640,\
    \ out_features=640, bias=False)\n                  (to_v): Linear(in_features=640,\
    \ out_features=640, bias=False)\n                  (to_out): Sequential(\n   \
    \                 (0): Linear(in_features=640, out_features=640, bias=True)\n\
    \                    (1): Dropout(p=0.0, inplace=False)\n                  )\n\
    \                )\n                (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n\
    \                (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n\
    \                (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n\
    \              )\n            )\n            (proj_out): Linear(in_features=640,\
    \ out_features=640, bias=True)\n          )\n        )\n        (6): TimestepEmbedSequential(\n\
    \          (0): Downsample(\n            (op): Conv2d(640, 640, kernel_size=(3,\
    \ 3), stride=(2, 2), padding=(1, 1))\n          )\n        )\n        (7): TimestepEmbedSequential(\n\
    \          (0): ResBlock(\n            (in_layers): Sequential(\n            \
    \  (0): GroupNormSpecific(32, 640, eps=1e-05, affine=True)\n              (1):\
    \ SiLU()\n              (2): Conv2d(640, 1280, kernel_size=(3, 3), stride=(1,\
    \ 1), padding=(1, 1))\n            )\n            (h_upd): Identity()\n      \
    \      (x_upd): Identity()\n            (emb_layers): Sequential(\n          \
    \    (0): SiLU()\n              (1): Linear(in_features=1280, out_features=1280,\
    \ bias=True)\n            )\n            (out_layers): Sequential(\n         \
    \     (0): GroupNormSpecific(32, 1280, eps=1e-05, affine=True)\n             \
    \ (1): SiLU()\n              (2): Dropout(p=0.0, inplace=False)\n            \
    \  (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n\
    \            )\n            (skip_connection): Conv2d(640, 1280, kernel_size=(1,\
    \ 1), stride=(1, 1))\n            (temopral_conv): TemporalConvBlock(\n      \
    \        (conv1): Sequential(\n                (0): GroupNorm(32, 1280, eps=1e-05,\
    \ affine=True)\n                (1): SiLU()\n                (2): Conv3d(1280,\
    \ 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n        \
    \      )\n              (conv2): Sequential(\n                (0): GroupNorm(32,\
    \ 1280, eps=1e-05, affine=True)\n                (1): SiLU()\n               \
    \ (2): Dropout(p=0.1, inplace=False)\n                (3): Conv3d(1280, 1280,\
    \ kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n              )\n\
    \              (conv3): Sequential(\n                (0): GroupNorm(32, 1280,\
    \ eps=1e-05, affine=True)\n                (1): SiLU()\n                (2): Dropout(p=0.1,\
    \ inplace=False)\n                (3): Conv3d(1280, 1280, kernel_size=(3, 1, 1),\
    \ stride=(1, 1, 1), padding=(1, 0, 0))\n              )\n              (conv4):\
    \ Sequential(\n                (0): GroupNorm(32, 1280, eps=1e-05, affine=True)\n\
    \                (1): SiLU()\n                (2): Dropout(p=0.1, inplace=False)\n\
    \                (3): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1,\
    \ 1), padding=(1, 0, 0))\n              )\n            )\n          )\n      \
    \    (1): SpatialTransformer(\n            (norm): GroupNorm(32, 1280, eps=1e-06,\
    \ affine=True)\n            (proj_in): Linear(in_features=1280, out_features=1280,\
    \ bias=True)\n            (transformer_blocks): ModuleList(\n              (0):\
    \ BasicTransformerBlock(\n                (attn1): CrossAttention(\n         \
    \         (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n  \
    \                (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n\
    \                  (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n\
    \                  (to_out): Sequential(\n                    (0): Linear(in_features=1280,\
    \ out_features=1280, bias=True)\n                    (1): Dropout(p=0.0, inplace=False)\n\
    \                  )\n                )\n                (ff): FeedForward(\n\
    \                  (net): Sequential(\n                    (0): GEGLU(\n     \
    \                 (proj): Linear(in_features=1280, out_features=10240, bias=True)\n\
    \                    )\n                    (1): Dropout(p=0.0, inplace=False)\n\
    \                    (2): Linear(in_features=5120, out_features=1280, bias=True)\n\
    \                  )\n                )\n                (attn2): CrossAttention(\n\
    \                  (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n\
    \                  (to_k): Linear(in_features=1024, out_features=1280, bias=False)\n\
    \                  (to_v): Linear(in_features=1024, out_features=1280, bias=False)\n\
    \                  (to_out): Sequential(\n                    (0): Linear(in_features=1280,\
    \ out_features=1280, bias=True)\n                    (1): Dropout(p=0.0, inplace=False)\n\
    \                  )\n                )\n                (norm1): LayerNorm((1280,),\
    \ eps=1e-05, elementwise_affine=True)\n                (norm2): LayerNorm((1280,),\
    \ eps=1e-05, elementwise_affine=True)\n                (norm3): LayerNorm((1280,),\
    \ eps=1e-05, elementwise_affine=True)\n              )\n            )\n      \
    \      (proj_out): Linear(in_features=1280, out_features=1280, bias=True)\n  \
    \        )\n          (2): TemporalTransformer(\n            (norm): GroupNorm(32,\
    \ 1280, eps=1e-06, affine=True)\n            (proj_in): Linear(in_features=1280,\
    \ out_features=1280, bias=True)\n            (transformer_blocks): ModuleList(\n\
    \              (0): BasicTransformerBlock(\n                (attn1): CrossAttention(\n\
    \                  (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n\
    \                  (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n\
    \                  (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n\
    \                  (to_out): Sequential(\n                    (0): Linear(in_features=1280,\
    \ out_features=1280, bias=True)\n                    (1): Dropout(p=0.0, inplace=False)\n\
    \                  )\n                )\n                (ff): FeedForward(\n\
    \                  (net): Sequential(\n                    (0): GEGLU(\n     \
    \                 (proj): Linear(in_features=1280, out_features=10240, bias=True)\n\
    \                    )\n                    (1): Dropout(p=0.0, inplace=False)\n\
    \                    (2): Linear(in_features=5120, out_features=1280, bias=True)\n\
    \                  )\n                )\n                (attn2): CrossAttention(\n\
    \                  (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n\
    \                  (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n\
    \                  (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n\
    \                  (to_out): Sequential(\n                    (0): Linear(in_features=1280,\
    \ out_features=1280, bias=True)\n                    (1): Dropout(p=0.0, inplace=False)\n\
    \                  )\n                )\n                (norm1): LayerNorm((1280,),\
    \ eps=1e-05, elementwise_affine=True)\n                (norm2): LayerNorm((1280,),\
    \ eps=1e-05, elementwise_affine=True)\n                (norm3): LayerNorm((1280,),\
    \ eps=1e-05, elementwise_affine=True)\n              )\n            )\n      \
    \      (proj_out): Linear(in_features=1280, out_features=1280, bias=True)\n  \
    \        )\n        )\n        (8): TimestepEmbedSequential(\n          (0): ResBlock(\n\
    \            (in_layers): Sequential(\n              (0): GroupNormSpecific(32,\
    \ 1280, eps=1e-05, affine=True)\n              (1): SiLU()\n              (2):\
    \ Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    \
    \        )\n            (h_upd): Identity()\n            (x_upd): Identity()\n\
    \            (emb_layers): Sequential(\n              (0): SiLU()\n          \
    \    (1): Linear(in_features=1280, out_features=1280, bias=True)\n           \
    \ )\n            (out_layers): Sequential(\n              (0): GroupNormSpecific(32,\
    \ 1280, eps=1e-05, affine=True)\n              (1): SiLU()\n              (2):\
    \ Dropout(p=0.0, inplace=False)\n              (3): Conv2d(1280, 1280, kernel_size=(3,\
    \ 3), stride=(1, 1), padding=(1, 1))\n            )\n            (skip_connection):\
    \ Identity()\n            (temopral_conv): TemporalConvBlock(\n              (conv1):\
    \ Sequential(\n                (0): GroupNorm(32, 1280, eps=1e-05, affine=True)\n\
    \                (1): SiLU()\n                (2): Conv3d(1280, 1280, kernel_size=(3,\
    \ 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n              )\n             \
    \ (conv2): Sequential(\n                (0): GroupNorm(32, 1280, eps=1e-05, affine=True)\n\
    \                (1): SiLU()\n                (2): Dropout(p=0.1, inplace=False)\n\
    \                (3): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1,\
    \ 1), padding=(1, 0, 0))\n              )\n              (conv3): Sequential(\n\
    \                (0): GroupNorm(32, 1280, eps=1e-05, affine=True)\n          \
    \      (1): SiLU()\n                (2): Dropout(p=0.1, inplace=False)\n     \
    \           (3): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1,\
    \ 0, 0))\n              )\n              (conv4): Sequential(\n              \
    \  (0): GroupNorm(32, 1280, eps=1e-05, affine=True)\n                (1): SiLU()\n\
    \                (2): Dropout(p=0.1, inplace=False)\n                (3): Conv3d(1280,\
    \ 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n        \
    \      )\n            )\n          )\n          (1): SpatialTransformer(\n   \
    \         (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n            (proj_in):\
    \ Linear(in_features=1280, out_features=1280, bias=True)\n            (transformer_blocks):\
    \ ModuleList(\n              (0): BasicTransformerBlock(\n                (attn1):\
    \ CrossAttention(\n                  (to_q): Linear(in_features=1280, out_features=1280,\
    \ bias=False)\n                  (to_k): Linear(in_features=1280, out_features=1280,\
    \ bias=False)\n                  (to_v): Linear(in_features=1280, out_features=1280,\
    \ bias=False)\n                  (to_out): Sequential(\n                    (0):\
    \ Linear(in_features=1280, out_features=1280, bias=True)\n                   \
    \ (1): Dropout(p=0.0, inplace=False)\n                  )\n                )\n\
    \                (ff): FeedForward(\n                  (net): Sequential(\n  \
    \                  (0): GEGLU(\n                      (proj): Linear(in_features=1280,\
    \ out_features=10240, bias=True)\n                    )\n                    (1):\
    \ Dropout(p=0.0, inplace=False)\n                    (2): Linear(in_features=5120,\
    \ out_features=1280, bias=True)\n                  )\n                )\n    \
    \            (attn2): CrossAttention(\n                  (to_q): Linear(in_features=1280,\
    \ out_features=1280, bias=False)\n                  (to_k): Linear(in_features=1024,\
    \ out_features=1280, bias=False)\n                  (to_v): Linear(in_features=1024,\
    \ out_features=1280, bias=False)\n                  (to_out): Sequential(\n  \
    \                  (0): Linear(in_features=1280, out_features=1280, bias=True)\n\
    \                    (1): Dropout(p=0.0, inplace=False)\n                  )\n\
    \                )\n                (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n\
    \                (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n\
    \                (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n\
    \              )\n            )\n            (proj_out): Linear(in_features=1280,\
    \ out_features=1280, bias=True)\n          )\n          (2): TemporalTransformer(\n\
    \            (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n           \
    \ (proj_in): Linear(in_features=1280, out_features=1280, bias=True)\n        \
    \    (transformer_blocks): ModuleList(\n              (0): BasicTransformerBlock(\n\
    \                (attn1): CrossAttention(\n                  (to_q): Linear(in_features=1280,\
    \ out_features=1280, bias=False)\n                  (to_k): Linear(in_features=1280,\
    \ out_features=1280, bias=False)\n                  (to_v): Linear(in_features=1280,\
    \ out_features=1280, bias=False)\n                  (to_out): Sequential(\n  \
    \                  (0): Linear(in_features=1280, out_features=1280, bias=True)\n\
    \                    (1): Dropout(p=0.0, inplace=False)\n                  )\n\
    \                )\n                (ff): FeedForward(\n                  (net):\
    \ Sequential(\n                    (0): GEGLU(\n                      (proj):\
    \ Linear(in_features=1280, out_features=10240, bias=True)\n                  \
    \  )\n                    (1): Dropout(p=0.0, inplace=False)\n               \
    \     (2): Linear(in_features=5120, out_features=1280, bias=True)\n          \
    \        )\n                )\n                (attn2): CrossAttention(\n    \
    \              (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n\
    \                  (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n\
    \                  (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n\
    \                  (to_out): Sequential(\n                    (0): Linear(in_features=1280,\
    \ out_features=1280, bias=True)\n                    (1): Dropout(p=0.0, inplace=False)\n\
    \                  )\n                )\n                (norm1): LayerNorm((1280,),\
    \ eps=1e-05, elementwise_affine=True)\n                (norm2): LayerNorm((1280,),\
    \ eps=1e-05, elementwise_affine=True)\n                (norm3): LayerNorm((1280,),\
    \ eps=1e-05, elementwise_affine=True)\n              )\n            )\n      \
    \      (proj_out): Linear(in_features=1280, out_features=1280, bias=True)\n  \
    \        )\n        )\n        (9): TimestepEmbedSequential(\n          (0): Downsample(\n\
    \            (op): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(2, 2), padding=(1,\
    \ 1))\n          )\n        )\n        (10-11): 2 x TimestepEmbedSequential(\n\
    \          (0): ResBlock(\n            (in_layers): Sequential(\n            \
    \  (0): GroupNormSpecific(32, 1280, eps=1e-05, affine=True)\n              (1):\
    \ SiLU()\n              (2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1,\
    \ 1), padding=(1, 1))\n            )\n            (h_upd): Identity()\n      \
    \      (x_upd): Identity()\n            (emb_layers): Sequential(\n          \
    \    (0): SiLU()\n              (1): Linear(in_features=1280, out_features=1280,\
    \ bias=True)\n            )\n            (out_layers): Sequential(\n         \
    \     (0): GroupNormSpecific(32, 1280, eps=1e-05, affine=True)\n             \
    \ (1): SiLU()\n              (2): Dropout(p=0.0, inplace=False)\n            \
    \  (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n\
    \            )\n            (skip_connection): Identity()\n            (temopral_conv):\
    \ TemporalConvBlock(\n              (conv1): Sequential(\n                (0):\
    \ GroupNorm(32, 1280, eps=1e-05, affine=True)\n                (1): SiLU()\n \
    \               (2): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1),\
    \ padding=(1, 0, 0))\n              )\n              (conv2): Sequential(\n  \
    \              (0): GroupNorm(32, 1280, eps=1e-05, affine=True)\n            \
    \    (1): SiLU()\n                (2): Dropout(p=0.1, inplace=False)\n       \
    \         (3): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1,\
    \ 0, 0))\n              )\n              (conv3): Sequential(\n              \
    \  (0): GroupNorm(32, 1280, eps=1e-05, affine=True)\n                (1): SiLU()\n\
    \                (2): Dropout(p=0.1, inplace=False)\n                (3): Conv3d(1280,\
    \ 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n        \
    \      )\n              (conv4): Sequential(\n                (0): GroupNorm(32,\
    \ 1280, eps=1e-05, affine=True)\n                (1): SiLU()\n               \
    \ (2): Dropout(p=0.1, inplace=False)\n                (3): Conv3d(1280, 1280,\
    \ kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n              )\n\
    \            )\n          )\n        )\n      )\n      (init_attn): TimestepEmbedSequential(\n\
    \        (0): TemporalTransformer(\n          (norm): GroupNorm(32, 320, eps=1e-06,\
    \ affine=True)\n          (proj_in): Conv1d(320, 512, kernel_size=(1,), stride=(1,))\n\
    \          (transformer_blocks): ModuleList(\n            (0): BasicTransformerBlock(\n\
    \              (attn1): CrossAttention(\n                (to_q): Linear(in_features=512,\
    \ out_features=512, bias=False)\n                (to_k): Linear(in_features=512,\
    \ out_features=512, bias=False)\n                (to_v): Linear(in_features=512,\
    \ out_features=512, bias=False)\n                (to_out): Sequential(\n     \
    \             (0): Linear(in_features=512, out_features=512, bias=True)\n    \
    \              (1): Dropout(p=0.0, inplace=False)\n                )\n       \
    \       )\n              (ff): FeedForward(\n                (net): Sequential(\n\
    \                  (0): GEGLU(\n                    (proj): Linear(in_features=512,\
    \ out_features=4096, bias=True)\n                  )\n                  (1): Dropout(p=0.0,\
    \ inplace=False)\n                  (2): Linear(in_features=2048, out_features=512,\
    \ bias=True)\n                )\n              )\n              (attn2): CrossAttention(\n\
    \                (to_q): Linear(in_features=512, out_features=512, bias=False)\n\
    \                (to_k): Linear(in_features=512, out_features=512, bias=False)\n\
    \                (to_v): Linear(in_features=512, out_features=512, bias=False)\n\
    \                (to_out): Sequential(\n                  (0): Linear(in_features=512,\
    \ out_features=512, bias=True)\n                  (1): Dropout(p=0.0, inplace=False)\n\
    \                )\n              )\n              (norm1): LayerNorm((512,),\
    \ eps=1e-05, elementwise_affine=True)\n              (norm2): LayerNorm((512,),\
    \ eps=1e-05, elementwise_affine=True)\n              (norm3): LayerNorm((512,),\
    \ eps=1e-05, elementwise_affine=True)\n            )\n          )\n          (proj_out):\
    \ Conv1d(512, 320, kernel_size=(1,), stride=(1,))\n        )\n      )\n      (middle_block):\
    \ TimestepEmbedSequential(\n        (0): ResBlock(\n          (in_layers): Sequential(\n\
    \            (0): GroupNormSpecific(32, 1280, eps=1e-05, affine=True)\n      \
    \      (1): SiLU()\n            (2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1,\
    \ 1), padding=(1, 1))\n          )\n          (h_upd): Identity()\n          (x_upd):\
    \ Identity()\n          (emb_layers): Sequential(\n            (0): SiLU()\n \
    \           (1): Linear(in_features=1280, out_features=1280, bias=True)\n    \
    \      )\n          (out_layers): Sequential(\n            (0): GroupNormSpecific(32,\
    \ 1280, eps=1e-05, affine=True)\n            (1): SiLU()\n            (2): Dropout(p=0.0,\
    \ inplace=False)\n            (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1,\
    \ 1), padding=(1, 1))\n          )\n          (skip_connection): Identity()\n\
    \          (temopral_conv): TemporalConvBlock(\n            (conv1): Sequential(\n\
    \              (0): GroupNorm(32, 1280, eps=1e-05, affine=True)\n            \
    \  (1): SiLU()\n              (2): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1,\
    \ 1, 1), padding=(1, 0, 0))\n            )\n            (conv2): Sequential(\n\
    \              (0): GroupNorm(32, 1280, eps=1e-05, affine=True)\n            \
    \  (1): SiLU()\n              (2): Dropout(p=0.1, inplace=False)\n           \
    \   (3): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1,\
    \ 0, 0))\n            )\n            (conv3): Sequential(\n              (0):\
    \ GroupNorm(32, 1280, eps=1e-05, affine=True)\n              (1): SiLU()\n   \
    \           (2): Dropout(p=0.1, inplace=False)\n              (3): Conv3d(1280,\
    \ 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n        \
    \    )\n            (conv4): Sequential(\n              (0): GroupNorm(32, 1280,\
    \ eps=1e-05, affine=True)\n              (1): SiLU()\n              (2): Dropout(p=0.1,\
    \ inplace=False)\n              (3): Conv3d(1280, 1280, kernel_size=(3, 1, 1),\
    \ stride=(1, 1, 1), padding=(1, 0, 0))\n            )\n          )\n        )\n\
    \        (1): SpatialTransformer(\n          (norm): GroupNorm(32, 1280, eps=1e-06,\
    \ affine=True)\n          (proj_in): Linear(in_features=1280, out_features=1280,\
    \ bias=True)\n          (transformer_blocks): ModuleList(\n            (0): BasicTransformerBlock(\n\
    \              (attn1): CrossAttention(\n                (to_q): Linear(in_features=1280,\
    \ out_features=1280, bias=False)\n                (to_k): Linear(in_features=1280,\
    \ out_features=1280, bias=False)\n                (to_v): Linear(in_features=1280,\
    \ out_features=1280, bias=False)\n                (to_out): Sequential(\n    \
    \              (0): Linear(in_features=1280, out_features=1280, bias=True)\n \
    \                 (1): Dropout(p=0.0, inplace=False)\n                )\n    \
    \          )\n              (ff): FeedForward(\n                (net): Sequential(\n\
    \                  (0): GEGLU(\n                    (proj): Linear(in_features=1280,\
    \ out_features=10240, bias=True)\n                  )\n                  (1):\
    \ Dropout(p=0.0, inplace=False)\n                  (2): Linear(in_features=5120,\
    \ out_features=1280, bias=True)\n                )\n              )\n        \
    \      (attn2): CrossAttention(\n                (to_q): Linear(in_features=1280,\
    \ out_features=1280, bias=False)\n                (to_k): Linear(in_features=1024,\
    \ out_features=1280, bias=False)\n                (to_v): Linear(in_features=1024,\
    \ out_features=1280, bias=False)\n                (to_out): Sequential(\n    \
    \              (0): Linear(in_features=1280, out_features=1280, bias=True)\n \
    \                 (1): Dropout(p=0.0, inplace=False)\n                )\n    \
    \          )\n              (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n\
    \              (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n\
    \              (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n\
    \            )\n          )\n          (proj_out): Linear(in_features=1280, out_features=1280,\
    \ bias=True)\n        )\n        (2): TemporalTransformer(\n          (norm):\
    \ GroupNorm(32, 1280, eps=1e-06, affine=True)\n          (proj_in): Linear(in_features=1280,\
    \ out_features=1280, bias=True)\n          (transformer_blocks): ModuleList(\n\
    \            (0): BasicTransformerBlock(\n              (attn1): CrossAttention(\n\
    \                (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n\
    \                (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n\
    \                (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n\
    \                (to_out): Sequential(\n                  (0): Linear(in_features=1280,\
    \ out_features=1280, bias=True)\n                  (1): Dropout(p=0.0, inplace=False)\n\
    \                )\n              )\n              (ff): FeedForward(\n      \
    \          (net): Sequential(\n                  (0): GEGLU(\n               \
    \     (proj): Linear(in_features=1280, out_features=10240, bias=True)\n      \
    \            )\n                  (1): Dropout(p=0.0, inplace=False)\n       \
    \           (2): Linear(in_features=5120, out_features=1280, bias=True)\n    \
    \            )\n              )\n              (attn2): CrossAttention(\n    \
    \            (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n\
    \                (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n\
    \                (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n\
    \                (to_out): Sequential(\n                  (0): Linear(in_features=1280,\
    \ out_features=1280, bias=True)\n                  (1): Dropout(p=0.0, inplace=False)\n\
    \                )\n              )\n              (norm1): LayerNorm((1280,),\
    \ eps=1e-05, elementwise_affine=True)\n              (norm2): LayerNorm((1280,),\
    \ eps=1e-05, elementwise_affine=True)\n              (norm3): LayerNorm((1280,),\
    \ eps=1e-05, elementwise_affine=True)\n            )\n          )\n          (proj_out):\
    \ Linear(in_features=1280, out_features=1280, bias=True)\n        )\n        (3):\
    \ ResBlock(\n          (in_layers): Sequential(\n            (0): GroupNormSpecific(32,\
    \ 1280, eps=1e-05, affine=True)\n            (1): SiLU()\n            (2): Conv2d(1280,\
    \ 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          )\n    \
    \      (h_upd): Identity()\n          (x_upd): Identity()\n          (emb_layers):\
    \ Sequential(\n            (0): SiLU()\n            (1): Linear(in_features=1280,\
    \ out_features=1280, bias=True)\n          )\n          (out_layers): Sequential(\n\
    \            (0): GroupNormSpecific(32, 1280, eps=1e-05, affine=True)\n      \
    \      (1): SiLU()\n            (2): Dropout(p=0.0, inplace=False)\n         \
    \   (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n\
    \          )\n          (skip_connection): Identity()\n          (temopral_conv):\
    \ TemporalConvBlock(\n            (conv1): Sequential(\n              (0): GroupNorm(32,\
    \ 1280, eps=1e-05, affine=True)\n              (1): SiLU()\n              (2):\
    \ Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n\
    \            )\n            (conv2): Sequential(\n              (0): GroupNorm(32,\
    \ 1280, eps=1e-05, affine=True)\n              (1): SiLU()\n              (2):\
    \ Dropout(p=0.1, inplace=False)\n              (3): Conv3d(1280, 1280, kernel_size=(3,\
    \ 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n            )\n            (conv3):\
    \ Sequential(\n              (0): GroupNorm(32, 1280, eps=1e-05, affine=True)\n\
    \              (1): SiLU()\n              (2): Dropout(p=0.1, inplace=False)\n\
    \              (3): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1),\
    \ padding=(1, 0, 0))\n            )\n            (conv4): Sequential(\n      \
    \        (0): GroupNorm(32, 1280, eps=1e-05, affine=True)\n              (1):\
    \ SiLU()\n              (2): Dropout(p=0.1, inplace=False)\n              (3):\
    \ Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n\
    \            )\n          )\n        )\n      )\n      (output_blocks): ModuleList(\n\
    \        (0-1): 2 x TimestepEmbedSequential(\n          (0): ResBlock(\n     \
    \       (in_layers): Sequential(\n              (0): GroupNormSpecific(32, 2560,\
    \ eps=1e-05, affine=True)\n              (1): SiLU()\n              (2): Conv2d(2560,\
    \ 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            )\n  \
    \          (h_upd): Identity()\n            (x_upd): Identity()\n            (emb_layers):\
    \ Sequential(\n              (0): SiLU()\n              (1): Linear(in_features=1280,\
    \ out_features=1280, bias=True)\n            )\n            (out_layers): Sequential(\n\
    \              (0): GroupNormSpecific(32, 1280, eps=1e-05, affine=True)\n    \
    \          (1): SiLU()\n              (2): Dropout(p=0.0, inplace=False)\n   \
    \           (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1,\
    \ 1))\n            )\n            (skip_connection): Conv2d(2560, 1280, kernel_size=(1,\
    \ 1), stride=(1, 1))\n            (temopral_conv): TemporalConvBlock(\n      \
    \        (conv1): Sequential(\n                (0): GroupNorm(32, 1280, eps=1e-05,\
    \ affine=True)\n                (1): SiLU()\n                (2): Conv3d(1280,\
    \ 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n        \
    \      )\n              (conv2): Sequential(\n                (0): GroupNorm(32,\
    \ 1280, eps=1e-05, affine=True)\n                (1): SiLU()\n               \
    \ (2): Dropout(p=0.1, inplace=False)\n                (3): Conv3d(1280, 1280,\
    \ kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n              )\n\
    \              (conv3): Sequential(\n                (0): GroupNorm(32, 1280,\
    \ eps=1e-05, affine=True)\n                (1): SiLU()\n                (2): Dropout(p=0.1,\
    \ inplace=False)\n                (3): Conv3d(1280, 1280, kernel_size=(3, 1, 1),\
    \ stride=(1, 1, 1), padding=(1, 0, 0))\n              )\n              (conv4):\
    \ Sequential(\n                (0): GroupNorm(32, 1280, eps=1e-05, affine=True)\n\
    \                (1): SiLU()\n                (2): Dropout(p=0.1, inplace=False)\n\
    \                (3): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1,\
    \ 1), padding=(1, 0, 0))\n              )\n            )\n          )\n      \
    \  )\n        (2): TimestepEmbedSequential(\n          (0): ResBlock(\n      \
    \      (in_layers): Sequential(\n              (0): GroupNormSpecific(32, 2560,\
    \ eps=1e-05, affine=True)\n              (1): SiLU()\n              (2): Conv2d(2560,\
    \ 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            )\n  \
    \          (h_upd): Identity()\n            (x_upd): Identity()\n            (emb_layers):\
    \ Sequential(\n              (0): SiLU()\n              (1): Linear(in_features=1280,\
    \ out_features=1280, bias=True)\n            )\n            (out_layers): Sequential(\n\
    \              (0): GroupNormSpecific(32, 1280, eps=1e-05, affine=True)\n    \
    \          (1): SiLU()\n              (2): Dropout(p=0.0, inplace=False)\n   \
    \           (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1,\
    \ 1))\n            )\n            (skip_connection): Conv2d(2560, 1280, kernel_size=(1,\
    \ 1), stride=(1, 1))\n            (temopral_conv): TemporalConvBlock(\n      \
    \        (conv1): Sequential(\n                (0): GroupNorm(32, 1280, eps=1e-05,\
    \ affine=True)\n                (1): SiLU()\n                (2): Conv3d(1280,\
    \ 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n        \
    \      )\n              (conv2): Sequential(\n                (0): GroupNorm(32,\
    \ 1280, eps=1e-05, affine=True)\n                (1): SiLU()\n               \
    \ (2): Dropout(p=0.1, inplace=False)\n                (3): Conv3d(1280, 1280,\
    \ kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n              )\n\
    \              (conv3): Sequential(\n                (0): GroupNorm(32, 1280,\
    \ eps=1e-05, affine=True)\n                (1): SiLU()\n                (2): Dropout(p=0.1,\
    \ inplace=False)\n                (3): Conv3d(1280, 1280, kernel_size=(3, 1, 1),\
    \ stride=(1, 1, 1), padding=(1, 0, 0))\n              )\n              (conv4):\
    \ Sequential(\n                (0): GroupNorm(32, 1280, eps=1e-05, affine=True)\n\
    \                (1): SiLU()\n                (2): Dropout(p=0.1, inplace=False)\n\
    \                (3): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1,\
    \ 1), padding=(1, 0, 0))\n              )\n            )\n          )\n      \
    \    (1): Upsample(\n            (conv): Conv2d(1280, 1280, kernel_size=(3, 3),\
    \ stride=(1, 1), padding=(1, 1))\n          )\n        )\n        (3-4): 2 x TimestepEmbedSequential(\n\
    \          (0): ResBlock(\n            (in_layers): Sequential(\n            \
    \  (0): GroupNormSpecific(32, 2560, eps=1e-05, affine=True)\n              (1):\
    \ SiLU()\n              (2): Conv2d(2560, 1280, kernel_size=(3, 3), stride=(1,\
    \ 1), padding=(1, 1))\n            )\n            (h_upd): Identity()\n      \
    \      (x_upd): Identity()\n            (emb_layers): Sequential(\n          \
    \    (0): SiLU()\n              (1): Linear(in_features=1280, out_features=1280,\
    \ bias=True)\n            )\n            (out_layers): Sequential(\n         \
    \     (0): GroupNormSpecific(32, 1280, eps=1e-05, affine=True)\n             \
    \ (1): SiLU()\n              (2): Dropout(p=0.0, inplace=False)\n            \
    \  (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n\
    \            )\n            (skip_connection): Conv2d(2560, 1280, kernel_size=(1,\
    \ 1), stride=(1, 1))\n            (temopral_conv): TemporalConvBlock(\n      \
    \        (conv1): Sequential(\n                (0): GroupNorm(32, 1280, eps=1e-05,\
    \ affine=True)\n                (1): SiLU()\n                (2): Conv3d(1280,\
    \ 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n        \
    \      )\n              (conv2): Sequential(\n                (0): GroupNorm(32,\
    \ 1280, eps=1e-05, affine=True)\n                (1): SiLU()\n               \
    \ (2): Dropout(p=0.1, inplace=False)\n                (3): Conv3d(1280, 1280,\
    \ kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n              )\n\
    \              (conv3): Sequential(\n                (0): GroupNorm(32, 1280,\
    \ eps=1e-05, affine=True)\n                (1): SiLU()\n                (2): Dropout(p=0.1,\
    \ inplace=False)\n                (3): Conv3d(1280, 1280, kernel_size=(3, 1, 1),\
    \ stride=(1, 1, 1), padding=(1, 0, 0))\n              )\n              (conv4):\
    \ Sequential(\n                (0): GroupNorm(32, 1280, eps=1e-05, affine=True)\n\
    \                (1): SiLU()\n                (2): Dropout(p=0.1, inplace=False)\n\
    \                (3): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1,\
    \ 1), padding=(1, 0, 0))\n              )\n            )\n          )\n      \
    \    (1): SpatialTransformer(\n            (norm): GroupNorm(32, 1280, eps=1e-06,\
    \ affine=True)\n            (proj_in): Linear(in_features=1280, out_features=1280,\
    \ bias=True)\n            (transformer_blocks): ModuleList(\n              (0):\
    \ BasicTransformerBlock(\n                (attn1): CrossAttention(\n         \
    \         (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n  \
    \                (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n\
    \                  (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n\
    \                  (to_out): Sequential(\n                    (0): Linear(in_features=1280,\
    \ out_features=1280, bias=True)\n                    (1): Dropout(p=0.0, inplace=False)\n\
    \                  )\n                )\n                (ff): FeedForward(\n\
    \                  (net): Sequential(\n                    (0): GEGLU(\n     \
    \                 (proj): Linear(in_features=1280, out_features=10240, bias=True)\n\
    \                    )\n                    (1): Dropout(p=0.0, inplace=False)\n\
    \                    (2): Linear(in_features=5120, out_features=1280, bias=True)\n\
    \                  )\n                )\n                (attn2): CrossAttention(\n\
    \                  (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n\
    \                  (to_k): Linear(in_features=1024, out_features=1280, bias=False)\n\
    \                  (to_v): Linear(in_features=1024, out_features=1280, bias=False)\n\
    \                  (to_out): Sequential(\n                    (0): Linear(in_features=1280,\
    \ out_features=1280, bias=True)\n                    (1): Dropout(p=0.0, inplace=False)\n\
    \                  )\n                )\n                (norm1): LayerNorm((1280,),\
    \ eps=1e-05, elementwise_affine=True)\n                (norm2): LayerNorm((1280,),\
    \ eps=1e-05, elementwise_affine=True)\n                (norm3): LayerNorm((1280,),\
    \ eps=1e-05, elementwise_affine=True)\n              )\n            )\n      \
    \      (proj_out): Linear(in_features=1280, out_features=1280, bias=True)\n  \
    \        )\n          (2): TemporalTransformer(\n            (norm): GroupNorm(32,\
    \ 1280, eps=1e-06, affine=True)\n            (proj_in): Linear(in_features=1280,\
    \ out_features=1280, bias=True)\n            (transformer_blocks): ModuleList(\n\
    \              (0): BasicTransformerBlock(\n                (attn1): CrossAttention(\n\
    \                  (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n\
    \                  (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n\
    \                  (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n\
    \                  (to_out): Sequential(\n                    (0): Linear(in_features=1280,\
    \ out_features=1280, bias=True)\n                    (1): Dropout(p=0.0, inplace=False)\n\
    \                  )\n                )\n                (ff): FeedForward(\n\
    \                  (net): Sequential(\n                    (0): GEGLU(\n     \
    \                 (proj): Linear(in_features=1280, out_features=10240, bias=True)\n\
    \                    )\n                    (1): Dropout(p=0.0, inplace=False)\n\
    \                    (2): Linear(in_features=5120, out_features=1280, bias=True)\n\
    \                  )\n                )\n                (attn2): CrossAttention(\n\
    \                  (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n\
    \                  (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n\
    \                  (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n\
    \                  (to_out): Sequential(\n                    (0): Linear(in_features=1280,\
    \ out_features=1280, bias=True)\n                    (1): Dropout(p=0.0, inplace=False)\n\
    \                  )\n                )\n                (norm1): LayerNorm((1280,),\
    \ eps=1e-05, elementwise_affine=True)\n                (norm2): LayerNorm((1280,),\
    \ eps=1e-05, elementwise_affine=True)\n                (norm3): LayerNorm((1280,),\
    \ eps=1e-05, elementwise_affine=True)\n              )\n            )\n      \
    \      (proj_out): Linear(in_features=1280, out_features=1280, bias=True)\n  \
    \        )\n        )\n        (5): TimestepEmbedSequential(\n          (0): ResBlock(\n\
    \            (in_layers): Sequential(\n              (0): GroupNormSpecific(32,\
    \ 1920, eps=1e-05, affine=True)\n              (1): SiLU()\n              (2):\
    \ Conv2d(1920, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    \
    \        )\n            (h_upd): Identity()\n            (x_upd): Identity()\n\
    \            (emb_layers): Sequential(\n              (0): SiLU()\n          \
    \    (1): Linear(in_features=1280, out_features=1280, bias=True)\n           \
    \ )\n            (out_layers): Sequential(\n              (0): GroupNormSpecific(32,\
    \ 1280, eps=1e-05, affine=True)\n              (1): SiLU()\n              (2):\
    \ Dropout(p=0.0, inplace=False)\n              (3): Conv2d(1280, 1280, kernel_size=(3,\
    \ 3), stride=(1, 1), padding=(1, 1))\n            )\n            (skip_connection):\
    \ Conv2d(1920, 1280, kernel_size=(1, 1), stride=(1, 1))\n            (temopral_conv):\
    \ TemporalConvBlock(\n              (conv1): Sequential(\n                (0):\
    \ GroupNorm(32, 1280, eps=1e-05, affine=True)\n                (1): SiLU()\n \
    \               (2): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1),\
    \ padding=(1, 0, 0))\n              )\n              (conv2): Sequential(\n  \
    \              (0): GroupNorm(32, 1280, eps=1e-05, affine=True)\n            \
    \    (1): SiLU()\n                (2): Dropout(p=0.1, inplace=False)\n       \
    \         (3): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1,\
    \ 0, 0))\n              )\n              (conv3): Sequential(\n              \
    \  (0): GroupNorm(32, 1280, eps=1e-05, affine=True)\n                (1): SiLU()\n\
    \                (2): Dropout(p=0.1, inplace=False)\n                (3): Conv3d(1280,\
    \ 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n        \
    \      )\n              (conv4): Sequential(\n                (0): GroupNorm(32,\
    \ 1280, eps=1e-05, affine=True)\n                (1): SiLU()\n               \
    \ (2): Dropout(p=0.1, inplace=False)\n                (3): Conv3d(1280, 1280,\
    \ kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n              )\n\
    \            )\n          )\n          (1): SpatialTransformer(\n            (norm):\
    \ GroupNorm(32, 1280, eps=1e-06, affine=True)\n            (proj_in): Linear(in_features=1280,\
    \ out_features=1280, bias=True)\n            (transformer_blocks): ModuleList(\n\
    \              (0): BasicTransformerBlock(\n                (attn1): CrossAttention(\n\
    \                  (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n\
    \                  (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n\
    \                  (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n\
    \                  (to_out): Sequential(\n                    (0): Linear(in_features=1280,\
    \ out_features=1280, bias=True)\n                    (1): Dropout(p=0.0, inplace=False)\n\
    \                  )\n                )\n                (ff): FeedForward(\n\
    \                  (net): Sequential(\n                    (0): GEGLU(\n     \
    \                 (proj): Linear(in_features=1280, out_features=10240, bias=True)\n\
    \                    )\n                    (1): Dropout(p=0.0, inplace=False)\n\
    \                    (2): Linear(in_features=5120, out_features=1280, bias=True)\n\
    \                  )\n                )\n                (attn2): CrossAttention(\n\
    \                  (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n\
    \                  (to_k): Linear(in_features=1024, out_features=1280, bias=False)\n\
    \                  (to_v): Linear(in_features=1024, out_features=1280, bias=False)\n\
    \                  (to_out): Sequential(\n                    (0): Linear(in_features=1280,\
    \ out_features=1280, bias=True)\n                    (1): Dropout(p=0.0, inplace=False)\n\
    \                  )\n                )\n                (norm1): LayerNorm((1280,),\
    \ eps=1e-05, elementwise_affine=True)\n                (norm2): LayerNorm((1280,),\
    \ eps=1e-05, elementwise_affine=True)\n                (norm3): LayerNorm((1280,),\
    \ eps=1e-05, elementwise_affine=True)\n              )\n            )\n      \
    \      (proj_out): Linear(in_features=1280, out_features=1280, bias=True)\n  \
    \        )\n          (2): TemporalTransformer(\n            (norm): GroupNorm(32,\
    \ 1280, eps=1e-06, affine=True)\n            (proj_in): Linear(in_features=1280,\
    \ out_features=1280, bias=True)\n            (transformer_blocks): ModuleList(\n\
    \              (0): BasicTransformerBlock(\n                (attn1): CrossAttention(\n\
    \                  (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n\
    \                  (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n\
    \                  (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n\
    \                  (to_out): Sequential(\n                    (0): Linear(in_features=1280,\
    \ out_features=1280, bias=True)\n                    (1): Dropout(p=0.0, inplace=False)\n\
    \                  )\n                )\n                (ff): FeedForward(\n\
    \                  (net): Sequential(\n                    (0): GEGLU(\n     \
    \                 (proj): Linear(in_features=1280, out_features=10240, bias=True)\n\
    \                    )\n                    (1): Dropout(p=0.0, inplace=False)\n\
    \                    (2): Linear(in_features=5120, out_features=1280, bias=True)\n\
    \                  )\n                )\n                (attn2): CrossAttention(\n\
    \                  (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n\
    \                  (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n\
    \                  (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n\
    \                  (to_out): Sequential(\n                    (0): Linear(in_features=1280,\
    \ out_features=1280, bias=True)\n                    (1): Dropout(p=0.0, inplace=False)\n\
    \                  )\n                )\n                (norm1): LayerNorm((1280,),\
    \ eps=1e-05, elementwise_affine=True)\n                (norm2): LayerNorm((1280,),\
    \ eps=1e-05, elementwise_affine=True)\n                (norm3): LayerNorm((1280,),\
    \ eps=1e-05, elementwise_affine=True)\n              )\n            )\n      \
    \      (proj_out): Linear(in_features=1280, out_features=1280, bias=True)\n  \
    \        )\n          (3): Upsample(\n            (conv): Conv2d(1280, 1280, kernel_size=(3,\
    \ 3), stride=(1, 1), padding=(1, 1))\n          )\n        )\n        (6): TimestepEmbedSequential(\n\
    \          (0): ResBlock(\n            (in_layers): Sequential(\n            \
    \  (0): GroupNormSpecific(32, 1920, eps=1e-05, affine=True)\n              (1):\
    \ SiLU()\n              (2): Conv2d(1920, 640, kernel_size=(3, 3), stride=(1,\
    \ 1), padding=(1, 1))\n            )\n            (h_upd): Identity()\n      \
    \      (x_upd): Identity()\n            (emb_layers): Sequential(\n          \
    \    (0): SiLU()\n              (1): Linear(in_features=1280, out_features=640,\
    \ bias=True)\n            )\n            (out_layers): Sequential(\n         \
    \     (0): GroupNormSpecific(32, 640, eps=1e-05, affine=True)\n              (1):\
    \ SiLU()\n              (2): Dropout(p=0.0, inplace=False)\n              (3):\
    \ Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      \
    \      )\n            (skip_connection): Conv2d(1920, 640, kernel_size=(1, 1),\
    \ stride=(1, 1))\n            (temopral_conv): TemporalConvBlock(\n          \
    \    (conv1): Sequential(\n                (0): GroupNorm(32, 640, eps=1e-05,\
    \ affine=True)\n                (1): SiLU()\n                (2): Conv3d(640,\
    \ 640, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n         \
    \     )\n              (conv2): Sequential(\n                (0): GroupNorm(32,\
    \ 640, eps=1e-05, affine=True)\n                (1): SiLU()\n                (2):\
    \ Dropout(p=0.1, inplace=False)\n                (3): Conv3d(640, 640, kernel_size=(3,\
    \ 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n              )\n             \
    \ (conv3): Sequential(\n                (0): GroupNorm(32, 640, eps=1e-05, affine=True)\n\
    \                (1): SiLU()\n                (2): Dropout(p=0.1, inplace=False)\n\
    \                (3): Conv3d(640, 640, kernel_size=(3, 1, 1), stride=(1, 1, 1),\
    \ padding=(1, 0, 0))\n              )\n              (conv4): Sequential(\n  \
    \              (0): GroupNorm(32, 640, eps=1e-05, affine=True)\n             \
    \   (1): SiLU()\n                (2): Dropout(p=0.1, inplace=False)\n        \
    \        (3): Conv3d(640, 640, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1,\
    \ 0, 0))\n              )\n            )\n          )\n          (1): SpatialTransformer(\n\
    \            (norm): GroupNorm(32, 640, eps=1e-06, affine=True)\n            (proj_in):\
    \ Linear(in_features=640, out_features=640, bias=True)\n            (transformer_blocks):\
    \ ModuleList(\n              (0): BasicTransformerBlock(\n                (attn1):\
    \ CrossAttention(\n                  (to_q): Linear(in_features=640, out_features=640,\
    \ bias=False)\n                  (to_k): Linear(in_features=640, out_features=640,\
    \ bias=False)\n                  (to_v): Linear(in_features=640, out_features=640,\
    \ bias=False)\n                  (to_out): Sequential(\n                    (0):\
    \ Linear(in_features=640, out_features=640, bias=True)\n                    (1):\
    \ Dropout(p=0.0, inplace=False)\n                  )\n                )\n    \
    \            (ff): FeedForward(\n                  (net): Sequential(\n      \
    \              (0): GEGLU(\n                      (proj): Linear(in_features=640,\
    \ out_features=5120, bias=True)\n                    )\n                    (1):\
    \ Dropout(p=0.0, inplace=False)\n                    (2): Linear(in_features=2560,\
    \ out_features=640, bias=True)\n                  )\n                )\n     \
    \           (attn2): CrossAttention(\n                  (to_q): Linear(in_features=640,\
    \ out_features=640, bias=False)\n                  (to_k): Linear(in_features=1024,\
    \ out_features=640, bias=False)\n                  (to_v): Linear(in_features=1024,\
    \ out_features=640, bias=False)\n                  (to_out): Sequential(\n   \
    \                 (0): Linear(in_features=640, out_features=640, bias=True)\n\
    \                    (1): Dropout(p=0.0, inplace=False)\n                  )\n\
    \                )\n                (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n\
    \                (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n\
    \                (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n\
    \              )\n            )\n            (proj_out): Linear(in_features=640,\
    \ out_features=640, bias=True)\n          )\n          (2): TemporalTransformer(\n\
    \            (norm): GroupNorm(32, 640, eps=1e-06, affine=True)\n            (proj_in):\
    \ Linear(in_features=640, out_features=640, bias=True)\n            (transformer_blocks):\
    \ ModuleList(\n              (0): BasicTransformerBlock(\n                (attn1):\
    \ CrossAttention(\n                  (to_q): Linear(in_features=640, out_features=640,\
    \ bias=False)\n                  (to_k): Linear(in_features=640, out_features=640,\
    \ bias=False)\n                  (to_v): Linear(in_features=640, out_features=640,\
    \ bias=False)\n                  (to_out): Sequential(\n                    (0):\
    \ Linear(in_features=640, out_features=640, bias=True)\n                    (1):\
    \ Dropout(p=0.0, inplace=False)\n                  )\n                )\n    \
    \            (ff): FeedForward(\n                  (net): Sequential(\n      \
    \              (0): GEGLU(\n                      (proj): Linear(in_features=640,\
    \ out_features=5120, bias=True)\n                    )\n                    (1):\
    \ Dropout(p=0.0, inplace=False)\n                    (2): Linear(in_features=2560,\
    \ out_features=640, bias=True)\n                  )\n                )\n     \
    \           (attn2): CrossAttention(\n                  (to_q): Linear(in_features=640,\
    \ out_features=640, bias=False)\n                  (to_k): Linear(in_features=640,\
    \ out_features=640, bias=False)\n                  (to_v): Linear(in_features=640,\
    \ out_features=640, bias=False)\n                  (to_out): Sequential(\n   \
    \                 (0): Linear(in_features=640, out_features=640, bias=True)\n\
    \                    (1): Dropout(p=0.0, inplace=False)\n                  )\n\
    \                )\n                (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n\
    \                (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n\
    \                (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n\
    \              )\n            )\n            (proj_out): Linear(in_features=640,\
    \ out_features=640, bias=True)\n          )\n        )\n        (7): TimestepEmbedSequential(\n\
    \          (0): ResBlock(\n            (in_layers): Sequential(\n            \
    \  (0): GroupNormSpecific(32, 1280, eps=1e-05, affine=True)\n              (1):\
    \ SiLU()\n              (2): Conv2d(1280, 640, kernel_size=(3, 3), stride=(1,\
    \ 1), padding=(1, 1))\n            )\n            (h_upd): Identity()\n      \
    \      (x_upd): Identity()\n            (emb_layers): Sequential(\n          \
    \    (0): SiLU()\n              (1): Linear(in_features=1280, out_features=640,\
    \ bias=True)\n            )\n            (out_layers): Sequential(\n         \
    \     (0): GroupNormSpecific(32, 640, eps=1e-05, affine=True)\n              (1):\
    \ SiLU()\n              (2): Dropout(p=0.0, inplace=False)\n              (3):\
    \ Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      \
    \      )\n            (skip_connection): Conv2d(1280, 640, kernel_size=(1, 1),\
    \ stride=(1, 1))\n            (temopral_conv): TemporalConvBlock(\n          \
    \    (conv1): Sequential(\n                (0): GroupNorm(32, 640, eps=1e-05,\
    \ affine=True)\n                (1): SiLU()\n                (2): Conv3d(640,\
    \ 640, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n         \
    \     )\n              (conv2): Sequential(\n                (0): GroupNorm(32,\
    \ 640, eps=1e-05, affine=True)\n                (1): SiLU()\n                (2):\
    \ Dropout(p=0.1, inplace=False)\n                (3): Conv3d(640, 640, kernel_size=(3,\
    \ 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n              )\n             \
    \ (conv3): Sequential(\n                (0): GroupNorm(32, 640, eps=1e-05, affine=True)\n\
    \                (1): SiLU()\n                (2): Dropout(p=0.1, inplace=False)\n\
    \                (3): Conv3d(640, 640, kernel_size=(3, 1, 1), stride=(1, 1, 1),\
    \ padding=(1, 0, 0))\n              )\n              (conv4): Sequential(\n  \
    \              (0): GroupNorm(32, 640, eps=1e-05, affine=True)\n             \
    \   (1): SiLU()\n                (2): Dropout(p=0.1, inplace=False)\n        \
    \        (3): Conv3d(640, 640, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1,\
    \ 0, 0))\n              )\n            )\n          )\n          (1): SpatialTransformer(\n\
    \            (norm): GroupNorm(32, 640, eps=1e-06, affine=True)\n            (proj_in):\
    \ Linear(in_features=640, out_features=640, bias=True)\n            (transformer_blocks):\
    \ ModuleList(\n              (0): BasicTransformerBlock(\n                (attn1):\
    \ CrossAttention(\n                  (to_q): Linear(in_features=640, out_features=640,\
    \ bias=False)\n                  (to_k): Linear(in_features=640, out_features=640,\
    \ bias=False)\n                  (to_v): Linear(in_features=640, out_features=640,\
    \ bias=False)\n                  (to_out): Sequential(\n                    (0):\
    \ Linear(in_features=640, out_features=640, bias=True)\n                    (1):\
    \ Dropout(p=0.0, inplace=False)\n                  )\n                )\n    \
    \            (ff): FeedForward(\n                  (net): Sequential(\n      \
    \              (0): GEGLU(\n                      (proj): Linear(in_features=640,\
    \ out_features=5120, bias=True)\n                    )\n                    (1):\
    \ Dropout(p=0.0, inplace=False)\n                    (2): Linear(in_features=2560,\
    \ out_features=640, bias=True)\n                  )\n                )\n     \
    \           (attn2): CrossAttention(\n                  (to_q): Linear(in_features=640,\
    \ out_features=640, bias=False)\n                  (to_k): Linear(in_features=1024,\
    \ out_features=640, bias=False)\n                  (to_v): Linear(in_features=1024,\
    \ out_features=640, bias=False)\n                  (to_out): Sequential(\n   \
    \                 (0): Linear(in_features=640, out_features=640, bias=True)\n\
    \                    (1): Dropout(p=0.0, inplace=False)\n                  )\n\
    \                )\n                (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n\
    \                (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n\
    \                (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n\
    \              )\n            )\n            (proj_out): Linear(in_features=640,\
    \ out_features=640, bias=True)\n          )\n          (2): TemporalTransformer(\n\
    \            (norm): GroupNorm(32, 640, eps=1e-06, affine=True)\n            (proj_in):\
    \ Linear(in_features=640, out_features=640, bias=True)\n            (transformer_blocks):\
    \ ModuleList(\n              (0): BasicTransformerBlock(\n                (attn1):\
    \ CrossAttention(\n                  (to_q): Linear(in_features=640, out_features=640,\
    \ bias=False)\n                  (to_k): Linear(in_features=640, out_features=640,\
    \ bias=False)\n                  (to_v): Linear(in_features=640, out_features=640,\
    \ bias=False)\n                  (to_out): Sequential(\n                    (0):\
    \ Linear(in_features=640, out_features=640, bias=True)\n                    (1):\
    \ Dropout(p=0.0, inplace=False)\n                  )\n                )\n    \
    \            (ff): FeedForward(\n                  (net): Sequential(\n      \
    \              (0): GEGLU(\n                      (proj): Linear(in_features=640,\
    \ out_features=5120, bias=True)\n                    )\n                    (1):\
    \ Dropout(p=0.0, inplace=False)\n                    (2): Linear(in_features=2560,\
    \ out_features=640, bias=True)\n                  )\n                )\n     \
    \           (attn2): CrossAttention(\n                  (to_q): Linear(in_features=640,\
    \ out_features=640, bias=False)\n                  (to_k): Linear(in_features=640,\
    \ out_features=640, bias=False)\n                  (to_v): Linear(in_features=640,\
    \ out_features=640, bias=False)\n                  (to_out): Sequential(\n   \
    \                 (0): Linear(in_features=640, out_features=640, bias=True)\n\
    \                    (1): Dropout(p=0.0, inplace=False)\n                  )\n\
    \                )\n                (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n\
    \                (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n\
    \                (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n\
    \              )\n            )\n            (proj_out): Linear(in_features=640,\
    \ out_features=640, bias=True)\n          )\n        )\n        (8): TimestepEmbedSequential(\n\
    \          (0): ResBlock(\n            (in_layers): Sequential(\n            \
    \  (0): GroupNormSpecific(32, 960, eps=1e-05, affine=True)\n              (1):\
    \ SiLU()\n              (2): Conv2d(960, 640, kernel_size=(3, 3), stride=(1, 1),\
    \ padding=(1, 1))\n            )\n            (h_upd): Identity()\n          \
    \  (x_upd): Identity()\n            (emb_layers): Sequential(\n              (0):\
    \ SiLU()\n              (1): Linear(in_features=1280, out_features=640, bias=True)\n\
    \            )\n            (out_layers): Sequential(\n              (0): GroupNormSpecific(32,\
    \ 640, eps=1e-05, affine=True)\n              (1): SiLU()\n              (2):\
    \ Dropout(p=0.0, inplace=False)\n              (3): Conv2d(640, 640, kernel_size=(3,\
    \ 3), stride=(1, 1), padding=(1, 1))\n            )\n            (skip_connection):\
    \ Conv2d(960, 640, kernel_size=(1, 1), stride=(1, 1))\n            (temopral_conv):\
    \ TemporalConvBlock(\n              (conv1): Sequential(\n                (0):\
    \ GroupNorm(32, 640, eps=1e-05, affine=True)\n                (1): SiLU()\n  \
    \              (2): Conv3d(640, 640, kernel_size=(3, 1, 1), stride=(1, 1, 1),\
    \ padding=(1, 0, 0))\n              )\n              (conv2): Sequential(\n  \
    \              (0): GroupNorm(32, 640, eps=1e-05, affine=True)\n             \
    \   (1): SiLU()\n                (2): Dropout(p=0.1, inplace=False)\n        \
    \        (3): Conv3d(640, 640, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1,\
    \ 0, 0))\n              )\n              (conv3): Sequential(\n              \
    \  (0): GroupNorm(32, 640, eps=1e-05, affine=True)\n                (1): SiLU()\n\
    \                (2): Dropout(p=0.1, inplace=False)\n                (3): Conv3d(640,\
    \ 640, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n         \
    \     )\n              (conv4): Sequential(\n                (0): GroupNorm(32,\
    \ 640, eps=1e-05, affine=True)\n                (1): SiLU()\n                (2):\
    \ Dropout(p=0.1, inplace=False)\n                (3): Conv3d(640, 640, kernel_size=(3,\
    \ 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n              )\n            )\n\
    \          )\n          (1): SpatialTransformer(\n            (norm): GroupNorm(32,\
    \ 640, eps=1e-06, affine=True)\n            (proj_in): Linear(in_features=640,\
    \ out_features=640, bias=True)\n            (transformer_blocks): ModuleList(\n\
    \              (0): BasicTransformerBlock(\n                (attn1): CrossAttention(\n\
    \                  (to_q): Linear(in_features=640, out_features=640, bias=False)\n\
    \                  (to_k): Linear(in_features=640, out_features=640, bias=False)\n\
    \                  (to_v): Linear(in_features=640, out_features=640, bias=False)\n\
    \                  (to_out): Sequential(\n                    (0): Linear(in_features=640,\
    \ out_features=640, bias=True)\n                    (1): Dropout(p=0.0, inplace=False)\n\
    \                  )\n                )\n                (ff): FeedForward(\n\
    \                  (net): Sequential(\n                    (0): GEGLU(\n     \
    \                 (proj): Linear(in_features=640, out_features=5120, bias=True)\n\
    \                    )\n                    (1): Dropout(p=0.0, inplace=False)\n\
    \                    (2): Linear(in_features=2560, out_features=640, bias=True)\n\
    \                  )\n                )\n                (attn2): CrossAttention(\n\
    \                  (to_q): Linear(in_features=640, out_features=640, bias=False)\n\
    \                  (to_k): Linear(in_features=1024, out_features=640, bias=False)\n\
    \                  (to_v): Linear(in_features=1024, out_features=640, bias=False)\n\
    \                  (to_out): Sequential(\n                    (0): Linear(in_features=640,\
    \ out_features=640, bias=True)\n                    (1): Dropout(p=0.0, inplace=False)\n\
    \                  )\n                )\n                (norm1): LayerNorm((640,),\
    \ eps=1e-05, elementwise_affine=True)\n                (norm2): LayerNorm((640,),\
    \ eps=1e-05, elementwise_affine=True)\n                (norm3): LayerNorm((640,),\
    \ eps=1e-05, elementwise_affine=True)\n              )\n            )\n      \
    \      (proj_out): Linear(in_features=640, out_features=640, bias=True)\n    \
    \      )\n          (2): TemporalTransformer(\n            (norm): GroupNorm(32,\
    \ 640, eps=1e-06, affine=True)\n            (proj_in): Linear(in_features=640,\
    \ out_features=640, bias=True)\n            (transformer_blocks): ModuleList(\n\
    \              (0): BasicTransformerBlock(\n                (attn1): CrossAttention(\n\
    \                  (to_q): Linear(in_features=640, out_features=640, bias=False)\n\
    \                  (to_k): Linear(in_features=640, out_features=640, bias=False)\n\
    \                  (to_v): Linear(in_features=640, out_features=640, bias=False)\n\
    \                  (to_out): Sequential(\n                    (0): Linear(in_features=640,\
    \ out_features=640, bias=True)\n                    (1): Dropout(p=0.0, inplace=False)\n\
    \                  )\n                )\n                (ff): FeedForward(\n\
    \                  (net): Sequential(\n                    (0): GEGLU(\n     \
    \                 (proj): Linear(in_features=640, out_features=5120, bias=True)\n\
    \                    )\n                    (1): Dropout(p=0.0, inplace=False)\n\
    \                    (2): Linear(in_features=2560, out_features=640, bias=True)\n\
    \                  )\n                )\n                (attn2): CrossAttention(\n\
    \                  (to_q): Linear(in_features=640, out_features=640, bias=False)\n\
    \                  (to_k): Linear(in_features=640, out_features=640, bias=False)\n\
    \                  (to_v): Linear(in_features=640, out_features=640, bias=False)\n\
    \                  (to_out): Sequential(\n                    (0): Linear(in_features=640,\
    \ out_features=640, bias=True)\n                    (1): Dropout(p=0.0, inplace=False)\n\
    \                  )\n                )\n                (norm1): LayerNorm((640,),\
    \ eps=1e-05, elementwise_affine=True)\n                (norm2): LayerNorm((640,),\
    \ eps=1e-05, elementwise_affine=True)\n                (norm3): LayerNorm((640,),\
    \ eps=1e-05, elementwise_affine=True)\n              )\n            )\n      \
    \      (proj_out): Linear(in_features=640, out_features=640, bias=True)\n    \
    \      )\n          (3): Upsample(\n            (conv): Conv2d(640, 640, kernel_size=(3,\
    \ 3), stride=(1, 1), padding=(1, 1))\n          )\n        )\n        (9): TimestepEmbedSequential(\n\
    \          (0): ResBlock(\n            (in_layers): Sequential(\n            \
    \  (0): GroupNormSpecific(32, 960, eps=1e-05, affine=True)\n              (1):\
    \ SiLU()\n              (2): Conv2d(960, 320, kernel_size=(3, 3), stride=(1, 1),\
    \ padding=(1, 1))\n            )\n            (h_upd): Identity()\n          \
    \  (x_upd): Identity()\n            (emb_layers): Sequential(\n              (0):\
    \ SiLU()\n              (1): Linear(in_features=1280, out_features=320, bias=True)\n\
    \            )\n            (out_layers): Sequential(\n              (0): GroupNormSpecific(32,\
    \ 320, eps=1e-05, affine=True)\n              (1): SiLU()\n              (2):\
    \ Dropout(p=0.0, inplace=False)\n              (3): Conv2d(320, 320, kernel_size=(3,\
    \ 3), stride=(1, 1), padding=(1, 1))\n            )\n            (skip_connection):\
    \ Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1))\n            (temopral_conv):\
    \ TemporalConvBlock(\n              (conv1): Sequential(\n                (0):\
    \ GroupNorm(32, 320, eps=1e-05, affine=True)\n                (1): SiLU()\n  \
    \              (2): Conv3d(320, 320, kernel_size=(3, 1, 1), stride=(1, 1, 1),\
    \ padding=(1, 0, 0))\n              )\n              (conv2): Sequential(\n  \
    \              (0): GroupNorm(32, 320, eps=1e-05, affine=True)\n             \
    \   (1): SiLU()\n                (2): Dropout(p=0.1, inplace=False)\n        \
    \        (3): Conv3d(320, 320, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1,\
    \ 0, 0))\n              )\n              (conv3): Sequential(\n              \
    \  (0): GroupNorm(32, 320, eps=1e-05, affine=True)\n                (1): SiLU()\n\
    \                (2): Dropout(p=0.1, inplace=False)\n                (3): Conv3d(320,\
    \ 320, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n         \
    \     )\n              (conv4): Sequential(\n                (0): GroupNorm(32,\
    \ 320, eps=1e-05, affine=True)\n                (1): SiLU()\n                (2):\
    \ Dropout(p=0.1, inplace=False)\n                (3): Conv3d(320, 320, kernel_size=(3,\
    \ 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n              )\n            )\n\
    \          )\n          (1): SpatialTransformer(\n            (norm): GroupNorm(32,\
    \ 320, eps=1e-06, affine=True)\n            (proj_in): Linear(in_features=320,\
    \ out_features=320, bias=True)\n            (transformer_blocks): ModuleList(\n\
    \              (0): BasicTransformerBlock(\n                (attn1): CrossAttention(\n\
    \                  (to_q): Linear(in_features=320, out_features=320, bias=False)\n\
    \                  (to_k): Linear(in_features=320, out_features=320, bias=False)\n\
    \                  (to_v): Linear(in_features=320, out_features=320, bias=False)\n\
    \                  (to_out): Sequential(\n                    (0): Linear(in_features=320,\
    \ out_features=320, bias=True)\n                    (1): Dropout(p=0.0, inplace=False)\n\
    \                  )\n                )\n                (ff): FeedForward(\n\
    \                  (net): Sequential(\n                    (0): GEGLU(\n     \
    \                 (proj): Linear(in_features=320, out_features=2560, bias=True)\n\
    \                    )\n                    (1): Dropout(p=0.0, inplace=False)\n\
    \                    (2): Linear(in_features=1280, out_features=320, bias=True)\n\
    \                  )\n                )\n                (attn2): CrossAttention(\n\
    \                  (to_q): Linear(in_features=320, out_features=320, bias=False)\n\
    \                  (to_k): Linear(in_features=1024, out_features=320, bias=False)\n\
    \                  (to_v): Linear(in_features=1024, out_features=320, bias=False)\n\
    \                  (to_out): Sequential(\n                    (0): Linear(in_features=320,\
    \ out_features=320, bias=True)\n                    (1): Dropout(p=0.0, inplace=False)\n\
    \                  )\n                )\n                (norm1): LayerNorm((320,),\
    \ eps=1e-05, elementwise_affine=True)\n                (norm2): LayerNorm((320,),\
    \ eps=1e-05, elementwise_affine=True)\n                (norm3): LayerNorm((320,),\
    \ eps=1e-05, elementwise_affine=True)\n              )\n            )\n      \
    \      (proj_out): Linear(in_features=320, out_features=320, bias=True)\n    \
    \      )\n          (2): TemporalTransformer(\n            (norm): GroupNorm(32,\
    \ 320, eps=1e-06, affine=True)\n            (proj_in): Linear(in_features=320,\
    \ out_features=320, bias=True)\n            (transformer_blocks): ModuleList(\n\
    \              (0): BasicTransformerBlock(\n                (attn1): CrossAttention(\n\
    \                  (to_q): Linear(in_features=320, out_features=320, bias=False)\n\
    \                  (to_k): Linear(in_features=320, out_features=320, bias=False)\n\
    \                  (to_v): Linear(in_features=320, out_features=320, bias=False)\n\
    \                  (to_out): Sequential(\n                    (0): Linear(in_features=320,\
    \ out_features=320, bias=True)\n                    (1): Dropout(p=0.0, inplace=False)\n\
    \                  )\n                )\n                (ff): FeedForward(\n\
    \                  (net): Sequential(\n                    (0): GEGLU(\n     \
    \                 (proj): Linear(in_features=320, out_features=2560, bias=True)\n\
    \                    )\n                    (1): Dropout(p=0.0, inplace=False)\n\
    \                    (2): Linear(in_features=1280, out_features=320, bias=True)\n\
    \                  )\n                )\n                (attn2): CrossAttention(\n\
    \                  (to_q): Linear(in_features=320, out_features=320, bias=False)\n\
    \                  (to_k): Linear(in_features=320, out_features=320, bias=False)\n\
    \                  (to_v): Linear(in_features=320, out_features=320, bias=False)\n\
    \                  (to_out): Sequential(\n                    (0): Linear(in_features=320,\
    \ out_features=320, bias=True)\n                    (1): Dropout(p=0.0, inplace=False)\n\
    \                  )\n                )\n                (norm1): LayerNorm((320,),\
    \ eps=1e-05, elementwise_affine=True)\n                (norm2): LayerNorm((320,),\
    \ eps=1e-05, elementwise_affine=True)\n                (norm3): LayerNorm((320,),\
    \ eps=1e-05, elementwise_affine=True)\n              )\n            )\n      \
    \      (proj_out): Linear(in_features=320, out_features=320, bias=True)\n    \
    \      )\n        )\n        (10-11): 2 x TimestepEmbedSequential(\n         \
    \ (0): ResBlock(\n            (in_layers): Sequential(\n              (0): GroupNormSpecific(32,\
    \ 640, eps=1e-05, affine=True)\n              (1): SiLU()\n              (2):\
    \ Conv2d(640, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      \
    \      )\n            (h_upd): Identity()\n            (x_upd): Identity()\n \
    \           (emb_layers): Sequential(\n              (0): SiLU()\n           \
    \   (1): Linear(in_features=1280, out_features=320, bias=True)\n            )\n\
    \            (out_layers): Sequential(\n              (0): GroupNormSpecific(32,\
    \ 320, eps=1e-05, affine=True)\n              (1): SiLU()\n              (2):\
    \ Dropout(p=0.0, inplace=False)\n              (3): Conv2d(320, 320, kernel_size=(3,\
    \ 3), stride=(1, 1), padding=(1, 1))\n            )\n            (skip_connection):\
    \ Conv2d(640, 320, kernel_size=(1, 1), stride=(1, 1))\n            (temopral_conv):\
    \ TemporalConvBlock(\n              (conv1): Sequential(\n                (0):\
    \ GroupNorm(32, 320, eps=1e-05, affine=True)\n                (1): SiLU()\n  \
    \              (2): Conv3d(320, 320, kernel_size=(3, 1, 1), stride=(1, 1, 1),\
    \ padding=(1, 0, 0))\n              )\n              (conv2): Sequential(\n  \
    \              (0): GroupNorm(32, 320, eps=1e-05, affine=True)\n             \
    \   (1): SiLU()\n                (2): Dropout(p=0.1, inplace=False)\n        \
    \        (3): Conv3d(320, 320, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1,\
    \ 0, 0))\n              )\n              (conv3): Sequential(\n              \
    \  (0): GroupNorm(32, 320, eps=1e-05, affine=True)\n                (1): SiLU()\n\
    \                (2): Dropout(p=0.1, inplace=False)\n                (3): Conv3d(320,\
    \ 320, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n         \
    \     )\n              (conv4): Sequential(\n                (0): GroupNorm(32,\
    \ 320, eps=1e-05, affine=True)\n                (1): SiLU()\n                (2):\
    \ Dropout(p=0.1, inplace=False)\n                (3): Conv3d(320, 320, kernel_size=(3,\
    \ 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n              )\n            )\n\
    \          )\n          (1): SpatialTransformer(\n            (norm): GroupNorm(32,\
    \ 320, eps=1e-06, affine=True)\n            (proj_in): Linear(in_features=320,\
    \ out_features=320, bias=True)\n            (transformer_blocks): ModuleList(\n\
    \              (0): BasicTransformerBlock(\n                (attn1): CrossAttention(\n\
    \                  (to_q): Linear(in_features=320, out_features=320, bias=False)\n\
    \                  (to_k): Linear(in_features=320, out_features=320, bias=False)\n\
    \                  (to_v): Linear(in_features=320, out_features=320, bias=False)\n\
    \                  (to_out): Sequential(\n                    (0): Linear(in_features=320,\
    \ out_features=320, bias=True)\n                    (1): Dropout(p=0.0, inplace=False)\n\
    \                  )\n                )\n                (ff): FeedForward(\n\
    \                  (net): Sequential(\n                    (0): GEGLU(\n     \
    \                 (proj): Linear(in_features=320, out_features=2560, bias=True)\n\
    \                    )\n                    (1): Dropout(p=0.0, inplace=False)\n\
    \                    (2): Linear(in_features=1280, out_features=320, bias=True)\n\
    \                  )\n                )\n                (attn2): CrossAttention(\n\
    \                  (to_q): Linear(in_features=320, out_features=320, bias=False)\n\
    \                  (to_k): Linear(in_features=1024, out_features=320, bias=False)\n\
    \                  (to_v): Linear(in_features=1024, out_features=320, bias=False)\n\
    \                  (to_out): Sequential(\n                    (0): Linear(in_features=320,\
    \ out_features=320, bias=True)\n                    (1): Dropout(p=0.0, inplace=False)\n\
    \                  )\n                )\n                (norm1): LayerNorm((320,),\
    \ eps=1e-05, elementwise_affine=True)\n                (norm2): LayerNorm((320,),\
    \ eps=1e-05, elementwise_affine=True)\n                (norm3): LayerNorm((320,),\
    \ eps=1e-05, elementwise_affine=True)\n              )\n            )\n      \
    \      (proj_out): Linear(in_features=320, out_features=320, bias=True)\n    \
    \      )\n          (2): TemporalTransformer(\n            (norm): GroupNorm(32,\
    \ 320, eps=1e-06, affine=True)\n            (proj_in): Linear(in_features=320,\
    \ out_features=320, bias=True)\n            (transformer_blocks): ModuleList(\n\
    \              (0): BasicTransformerBlock(\n                (attn1): CrossAttention(\n\
    \                  (to_q): Linear(in_features=320, out_features=320, bias=False)\n\
    \                  (to_k): Linear(in_features=320, out_features=320, bias=False)\n\
    \                  (to_v): Linear(in_features=320, out_features=320, bias=False)\n\
    \                  (to_out): Sequential(\n                    (0): Linear(in_features=320,\
    \ out_features=320, bias=True)\n                    (1): Dropout(p=0.0, inplace=False)\n\
    \                  )\n                )\n                (ff): FeedForward(\n\
    \                  (net): Sequential(\n                    (0): GEGLU(\n     \
    \                 (proj): Linear(in_features=320, out_features=2560, bias=True)\n\
    \                    )\n                    (1): Dropout(p=0.0, inplace=False)\n\
    \                    (2): Linear(in_features=1280, out_features=320, bias=True)\n\
    \                  )\n                )\n                (attn2): CrossAttention(\n\
    \                  (to_q): Linear(in_features=320, out_features=320, bias=False)\n\
    \                  (to_k): Linear(in_features=320, out_features=320, bias=False)\n\
    \                  (to_v): Linear(in_features=320, out_features=320, bias=False)\n\
    \                  (to_out): Sequential(\n                    (0): Linear(in_features=320,\
    \ out_features=320, bias=True)\n                    (1): Dropout(p=0.0, inplace=False)\n\
    \                  )\n                )\n                (norm1): LayerNorm((320,),\
    \ eps=1e-05, elementwise_affine=True)\n                (norm2): LayerNorm((320,),\
    \ eps=1e-05, elementwise_affine=True)\n                (norm3): LayerNorm((320,),\
    \ eps=1e-05, elementwise_affine=True)\n              )\n            )\n      \
    \      (proj_out): Linear(in_features=320, out_features=320, bias=True)\n    \
    \      )\n        )\n      )\n      (out): Sequential(\n        (0): GroupNormSpecific(32,\
    \ 320, eps=1e-05, affine=True)\n        (1): SiLU()\n        (2): Conv2d(320,\
    \ 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      )\n    )\n  )\n\
    \  (first_stage_model): AutoencoderKL(\n    (encoder): Encoder(\n      (conv_in):\
    \ Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (down):\
    \ ModuleList(\n        (0): Module(\n          (block): ModuleList(\n        \
    \    (0-1): 2 x ResnetBlock(\n              (norm1): GroupNorm(32, 128, eps=1e-06,\
    \ affine=True)\n              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1,\
    \ 1), padding=(1, 1))\n              (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n\
    \              (dropout): Dropout(p=0.0, inplace=False)\n              (conv2):\
    \ Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      \
    \      )\n          )\n          (attn): ModuleList()\n          (downsample):\
    \ Downsample(\n            (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2,\
    \ 2))\n          )\n        )\n        (1): Module(\n          (block): ModuleList(\n\
    \            (0): ResnetBlock(\n              (norm1): GroupNorm(32, 128, eps=1e-06,\
    \ affine=True)\n              (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1,\
    \ 1), padding=(1, 1))\n              (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n\
    \              (dropout): Dropout(p=0.0, inplace=False)\n              (conv2):\
    \ Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      \
    \        (nin_shortcut): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n\
    \            )\n            (1): ResnetBlock(\n              (norm1): GroupNorm(32,\
    \ 256, eps=1e-06, affine=True)\n              (conv1): Conv2d(256, 256, kernel_size=(3,\
    \ 3), stride=(1, 1), padding=(1, 1))\n              (norm2): GroupNorm(32, 256,\
    \ eps=1e-06, affine=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n\
    \              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1,\
    \ 1))\n            )\n          )\n          (attn): ModuleList()\n          (downsample):\
    \ Downsample(\n            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2,\
    \ 2))\n          )\n        )\n        (2): Module(\n          (block): ModuleList(\n\
    \            (0): ResnetBlock(\n              (norm1): GroupNorm(32, 256, eps=1e-06,\
    \ affine=True)\n              (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1,\
    \ 1), padding=(1, 1))\n              (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n\
    \              (dropout): Dropout(p=0.0, inplace=False)\n              (conv2):\
    \ Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      \
    \        (nin_shortcut): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))\n\
    \            )\n            (1): ResnetBlock(\n              (norm1): GroupNorm(32,\
    \ 512, eps=1e-06, affine=True)\n              (conv1): Conv2d(512, 512, kernel_size=(3,\
    \ 3), stride=(1, 1), padding=(1, 1))\n              (norm2): GroupNorm(32, 512,\
    \ eps=1e-06, affine=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n\
    \              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1,\
    \ 1))\n            )\n          )\n          (attn): ModuleList()\n          (downsample):\
    \ Downsample(\n            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(2,\
    \ 2))\n          )\n        )\n        (3): Module(\n          (block): ModuleList(\n\
    \            (0-1): 2 x ResnetBlock(\n              (norm1): GroupNorm(32, 512,\
    \ eps=1e-06, affine=True)\n              (conv1): Conv2d(512, 512, kernel_size=(3,\
    \ 3), stride=(1, 1), padding=(1, 1))\n              (norm2): GroupNorm(32, 512,\
    \ eps=1e-06, affine=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n\
    \              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1,\
    \ 1))\n            )\n          )\n          (attn): ModuleList()\n        )\n\
    \      )\n      (mid): Module(\n        (block_1): ResnetBlock(\n          (norm1):\
    \ GroupNorm(32, 512, eps=1e-06, affine=True)\n          (conv1): Conv2d(512, 512,\
    \ kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (norm2): GroupNorm(32,\
    \ 512, eps=1e-06, affine=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n\
    \          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1,\
    \ 1))\n        )\n        (attn_1): AttnBlock(\n          (norm): GroupNorm(32,\
    \ 512, eps=1e-06, affine=True)\n          (q): Conv2d(512, 512, kernel_size=(1,\
    \ 1), stride=(1, 1))\n          (k): Conv2d(512, 512, kernel_size=(1, 1), stride=(1,\
    \ 1))\n          (v): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n  \
    \        (proj_out): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n   \
    \     )\n        (block_2): ResnetBlock(\n          (norm1): GroupNorm(32, 512,\
    \ eps=1e-06, affine=True)\n          (conv1): Conv2d(512, 512, kernel_size=(3,\
    \ 3), stride=(1, 1), padding=(1, 1))\n          (norm2): GroupNorm(32, 512, eps=1e-06,\
    \ affine=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n         \
    \ (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n\
    \        )\n      )\n      (norm_out): GroupNorm(32, 512, eps=1e-06, affine=True)\n\
    \      (conv_out): Conv2d(512, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1,\
    \ 1))\n    )\n    (decoder): Decoder(\n      (conv_in): Conv2d(4, 512, kernel_size=(3,\
    \ 3), stride=(1, 1), padding=(1, 1))\n      (mid): Module(\n        (block_1):\
    \ ResnetBlock(\n          (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n\
    \          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1,\
    \ 1))\n          (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n       \
    \   (dropout): Dropout(p=0.0, inplace=False)\n          (conv2): Conv2d(512, 512,\
    \ kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        )\n        (attn_1):\
    \ AttnBlock(\n          (norm): GroupNorm(32, 512, eps=1e-06, affine=True)\n \
    \         (q): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n         \
    \ (k): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n          (v): Conv2d(512,\
    \ 512, kernel_size=(1, 1), stride=(1, 1))\n          (proj_out): Conv2d(512, 512,\
    \ kernel_size=(1, 1), stride=(1, 1))\n        )\n        (block_2): ResnetBlock(\n\
    \          (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n          (conv1):\
    \ Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      \
    \    (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n          (dropout):\
    \ Dropout(p=0.0, inplace=False)\n          (conv2): Conv2d(512, 512, kernel_size=(3,\
    \ 3), stride=(1, 1), padding=(1, 1))\n        )\n      )\n      (up): ModuleList(\n\
    \        (0): Module(\n          (block): ModuleList(\n            (0): ResnetBlock(\n\
    \              (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n         \
    \     (conv1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1,\
    \ 1))\n              (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n   \
    \           (dropout): Dropout(p=0.0, inplace=False)\n              (conv2): Conv2d(128,\
    \ 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n              (nin_shortcut):\
    \ Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n            )\n       \
    \     (1-2): 2 x ResnetBlock(\n              (norm1): GroupNorm(32, 128, eps=1e-06,\
    \ affine=True)\n              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1,\
    \ 1), padding=(1, 1))\n              (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n\
    \              (dropout): Dropout(p=0.0, inplace=False)\n              (conv2):\
    \ Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      \
    \      )\n          )\n          (attn): ModuleList()\n        )\n        (1):\
    \ Module(\n          (block): ModuleList(\n            (0): ResnetBlock(\n   \
    \           (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n            \
    \  (conv1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n\
    \              (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n         \
    \     (dropout): Dropout(p=0.0, inplace=False)\n              (conv2): Conv2d(256,\
    \ 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n              (nin_shortcut):\
    \ Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n            )\n       \
    \     (1-2): 2 x ResnetBlock(\n              (norm1): GroupNorm(32, 256, eps=1e-06,\
    \ affine=True)\n              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1,\
    \ 1), padding=(1, 1))\n              (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n\
    \              (dropout): Dropout(p=0.0, inplace=False)\n              (conv2):\
    \ Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      \
    \      )\n          )\n          (attn): ModuleList()\n          (upsample): Upsample(\n\
    \            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1,\
    \ 1))\n          )\n        )\n        (2-3): 2 x Module(\n          (block):\
    \ ModuleList(\n            (0-2): 3 x ResnetBlock(\n              (norm1): GroupNorm(32,\
    \ 512, eps=1e-06, affine=True)\n              (conv1): Conv2d(512, 512, kernel_size=(3,\
    \ 3), stride=(1, 1), padding=(1, 1))\n              (norm2): GroupNorm(32, 512,\
    \ eps=1e-06, affine=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n\
    \              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1,\
    \ 1))\n            )\n          )\n          (attn): ModuleList()\n          (upsample):\
    \ Upsample(\n            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1,\
    \ 1), padding=(1, 1))\n          )\n        )\n      )\n      (norm_out): GroupNorm(32,\
    \ 128, eps=1e-06, affine=True)\n      (conv_out): Conv2d(128, 3, kernel_size=(3,\
    \ 3), stride=(1, 1), padding=(1, 1))\n    )\n    (loss): Identity()\n    (quant_conv):\
    \ Conv2d(8, 8, kernel_size=(1, 1), stride=(1, 1))\n    (post_quant_conv): Conv2d(4,\
    \ 4, kernel_size=(1, 1), stride=(1, 1))\n  )\n  (cond_stage_model): FrozenOpenCLIPEmbedder(\n\
    \    (model): CLIP(\n      (transformer): Transformer(\n        (resblocks): ModuleList(\n\
    \          (0-23): 24 x ResidualAttentionBlock(\n            (ln_1): LayerNorm((1024,),\
    \ eps=1e-05, elementwise_affine=True)\n            (attn): MultiheadAttention(\n\
    \              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024,\
    \ bias=True)\n            )\n            (ls_1): Identity()\n            (ln_2):\
    \ LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (mlp): Sequential(\n\
    \              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n\
    \              (gelu): GELU(approximate='none')\n              (c_proj): Linear(in_features=4096,\
    \ out_features=1024, bias=True)\n            )\n            (ls_2): Identity()\n\
    \          )\n        )\n      )\n      (token_embedding): Embedding(49408, 1024)\n\
    \      (ln_final): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n  \
    \  )\n  )\n)"
name:
  desc: null
  value: inference_t2v_512_v2.0
folder_name:
  desc: null
  value: inference_t2v_512_v2.0-2024-08-06T03-58-39
output_dir:
  desc: null
  value: outputs/inference_t2v_512_v2.0-2024-08-06T03-58-39
args:
  desc: null
  value: Namespace(adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, adam_weight_decay=0.01,
    bs=2, cfg_random_null_text=True, cfg_random_null_text_ratio=0.1, checkpointing_epochs=5,
    checkpointing_steps=-1, ckpt_path='ckpt/model.ckpt', cond_input=None, config='configs/inference_t2v_512_v2.0.yaml',
    config2='configs/training_t2v_512_v2.0.yaml', ddim_eta=1.0, ddim_steps=1, ema_decay=0.9999,
    enable_xformers_memory_efficient_attention=True, fps=28, frames=-1, global_seed=42,
    gradient_accumulation_steps=1, gradient_checkpointing=False, height=128, image_finetune=False,
    is_debug=False, launcher='pytorch', learning_rate=3e-05, lr_scheduler='constant',
    lr_warmup_steps=0, max_grad_norm=1.0, max_train_epoch=-1, max_train_steps=100,
    mixed_precision_training=False, mode='base', n_samples=1, name=None, noise_scheduler_kwargs=None,
    num_workers=32, output_dir='outputs', pretrained_model_path=None, prompt_file=None,
    savedir='results/base_512_v2', savefps=10, scale_lr=False, seed=123, train_batch_size=4,
    train_data=None, trainable_modules=(None,), unconditional_guidance_scale=12.0,
    unconditional_guidance_scale_temporal=None, unet_additional_kwargs={}, unet_checkpoint_path='',
    validation_data=None, validation_steps=100, validation_steps_tuple=(-1,), wandb=False,
    width=128)
_wandb:
  desc: null
  value:
    python_version: 3.8.5
    cli_version: 0.17.5
    framework: huggingface
    huggingface_version: 4.25.1
    is_jupyter_run: false
    is_kaggle_kernel: false
    start_time: 1722884321
    t:
      1:
      - 1
      - 9
      - 11
      - 41
      - 49
      - 55
      - 63
      - 79
      - 83
      - 105
      2:
      - 1
      - 9
      - 11
      - 41
      - 49
      - 55
      - 63
      - 79
      - 83
      - 105
      3:
      - 13
      - 16
      - 23
      4: 3.8.5
      5: 0.17.5
      6: 4.25.1
      8:
      - 5
      13: linux-x86_64
