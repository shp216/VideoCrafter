Number of trainable parameters: 1413284420
Number of mmg_model parameters: 1850970924
loading annotations from scripts/evaluation/data/audiocaps.csv ...
data scale: 876
Processing video 0: {'audiocap_id': '3', 'youtube_id': '--0w1YA1Hm4', 'start_time': '30', 'caption': 'People talking with the dull roar of a vehicle on the road', 'video_Caption': 'In this video, we see a car driving down a street at night. The video lasts for 8 seconds, and we see different objects and their attributes, such as the street, the car, the night, and the road. '}
pixel_values.shape:  torch.Size([16, 3, 256, 256])
Processing video 1: {'audiocap_id': '104906', 'youtube_id': '#NAME?', 'start_time': '30', 'caption': 'Plastic crinkling followed by footsteps on concrete as metal clanging and a group of people talk in the background', 'video_Caption': "In this video, we see a person's hand reaching into a plastic container filled with soil. The hand is holding a small plant, and we can see the roots of the plant. The container is placed on a concrete floor. "}
Video file scripts/evaluation/data/test_trimmed_audiocaps/audiocaps_#NAME?.mp4 not found.
Processing video 274: {'audiocap_id': '103869', 'youtube_id': 'DNtF_mGzQes', 'start_time': '30', 'caption': 'A man talking on loudspeakers along with an idling truck and a bustling crowd', 'video_Caption': 'In this video, we see a fire truck arriving at a building, and firefighters getting out to put out a fire. '}
pixel_values.shape:  torch.Size([16, 3, 256, 256])
torch.Size([2, 16, 3, 256, 256]) ['People talking with the dull roar of a vehicle on the road', 'A man talking on loudspeakers along with an idling truck and a bustling crowd']
/home/sehwan/anaconda3/envs/videocrafter/lib/python3.8/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(