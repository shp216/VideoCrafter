2024-08-06 00:09:33,033 INFO    MainThread:38608 [wandb_setup.py:_flush():76] Current SDK version is 0.17.5
2024-08-06 00:09:33,033 INFO    MainThread:38608 [wandb_setup.py:_flush():76] Configure stats pid to 38608
2024-08-06 00:09:33,033 INFO    MainThread:38608 [wandb_setup.py:_flush():76] Loading settings from /home/sehwan/.config/wandb/settings
2024-08-06 00:09:33,033 INFO    MainThread:38608 [wandb_setup.py:_flush():76] Loading settings from /home/sehwan/MIIL/VideoCrafter/wandb/settings
2024-08-06 00:09:33,033 INFO    MainThread:38608 [wandb_setup.py:_flush():76] Loading settings from environment variables: {}
2024-08-06 00:09:33,033 INFO    MainThread:38608 [wandb_setup.py:_flush():76] Applying setup settings: {'_disable_service': False}
2024-08-06 00:09:33,033 INFO    MainThread:38608 [wandb_setup.py:_flush():76] Inferring run settings from compute environment: {'program_relpath': 'scripts/evaluation/train_mmg.py', 'program_abspath': '/home/sehwan/MIIL/VideoCrafter/scripts/evaluation/train_mmg.py', 'program': 'scripts/evaluation/train_mmg.py'}
2024-08-06 00:09:33,034 INFO    MainThread:38608 [wandb_setup.py:_flush():76] Applying login settings: {}
2024-08-06 00:09:33,034 INFO    MainThread:38608 [wandb_init.py:_log_setup():529] Logging user logs to /home/sehwan/MIIL/VideoCrafter/wandb/run-20240806_000933-xfwsbs2g/logs/debug.log
2024-08-06 00:09:33,034 INFO    MainThread:38608 [wandb_init.py:_log_setup():530] Logging internal logs to /home/sehwan/MIIL/VideoCrafter/wandb/run-20240806_000933-xfwsbs2g/logs/debug-internal.log
2024-08-06 00:09:33,034 INFO    MainThread:38608 [wandb_init.py:init():569] calling init triggers
2024-08-06 00:09:33,058 INFO    MainThread:38608 [wandb_init.py:init():576] wandb.init called with sweep_config: {}
config: {'args': Namespace(adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, adam_weight_decay=0.01, bs=2, cfg_random_null_text=True, cfg_random_null_text_ratio=0.1, checkpointing_epochs=5, checkpointing_steps=-1, ckpt_path='ckpt/model.ckpt', cond_input=None, config='configs/inference_t2v_512_v2.0.yaml', config2='configs/training_t2v_512_v2.0.yaml', ddim_eta=1.0, ddim_steps=1, ema_decay=0.9999, enable_xformers_memory_efficient_attention=True, fps=28, frames=-1, global_seed=42, gradient_accumulation_steps=1, gradient_checkpointing=False, height=128, image_finetune=True, is_debug=False, launcher='pytorch', learning_rate=3e-05, lr_scheduler='constant', lr_warmup_steps=0, max_grad_norm=1.0, max_train_epoch=-1, max_train_steps=100, mixed_precision_training=True, mode='base', n_samples=1, name=None, noise_scheduler_kwargs=None, num_workers=32, output_dir='outputs', pretrained_model_path=None, prompt_file=None, savedir='results/base_512_v2', savefps=10, scale_lr=False, seed=123, train_batch_size=1, train_data=None, trainable_modules=(None,), unconditional_guidance_scale=12.0, unconditional_guidance_scale_temporal=None, unet_additional_kwargs={}, unet_checkpoint_path='', validation_data=None, validation_steps=100, validation_steps_tuple=(-1,), wandb=False, width=128), 'gpu_num': 1, 'gpu_no': 0, 'kwargs': {}, 'config': {}, 'model_config': {'target': 'lvdm.models.ddpm3d.LatentDiffusion', 'params': {'linear_start': 0.00085, 'linear_end': 0.012, 'num_timesteps_cond': 1, 'timesteps': 1000, 'first_stage_key': 'video', 'cond_stage_key': 'caption', 'cond_stage_trainable': False, 'conditioning_key': 'crossattn', 'image_size': [40, 64], 'channels': 4, 'scale_by_std': False, 'scale_factor': 0.18215, 'use_ema': False, 'uncond_type': 'empty_seq', 'use_scale': True, 'scale_b': 0.7, 'unet_config': {'target': 'lvdm.modules.networks.openaimodel3d.UNetModel', 'params': {'in_channels': 4, 'out_channels': 4, 'model_channels': 320, 'attention_resolutions': [4, 2, 1], 'num_res_blocks': 2, 'channel_mult': [1, 2, 4, 4], 'num_head_channels': 64, 'transformer_depth': 1, 'context_dim': 1024, 'use_linear': True, 'use_checkpoint': True, 'temporal_conv': True, 'temporal_attention': True, 'temporal_selfatt_only': True, 'use_relative_position': False, 'use_causal_attention': False, 'temporal_length': 16, 'addition_attention': True, 'fps_cond': True}}, 'first_stage_config': {'target': 'lvdm.models.autoencoder.AutoencoderKL', 'params': {'embed_dim': 4, 'monitor': 'val/rec_loss', 'ddconfig': {'double_z': True, 'z_channels': 4, 'resolution': 512, 'in_channels': 3, 'out_ch': 3, 'ch': 128, 'ch_mult': [1, 2, 4, 4], 'num_res_blocks': 2, 'attn_resolutions': [], 'dropout': 0.0}, 'lossconfig': {'target': 'torch.nn.Identity'}}}, 'cond_stage_config': {'target': 'lvdm.modules.encoders.condition.FrozenOpenCLIPEmbedder', 'params': {'freeze': True, 'layer': 'penultimate'}}}}, 'mmg_model': LatentDiffusion(
  (model): DiffusionWrapper(
    (diffusion_model): UNetModel(
      (time_embed): Sequential(
        (0): Linear(in_features=320, out_features=1280, bias=True)
        (1): SiLU()
        (2): Linear(in_features=1280, out_features=1280, bias=True)
      )
      (fps_embedding): Sequential(
        (0): Linear(in_features=320, out_features=1280, bias=True)
        (1): SiLU()
        (2): Linear(in_features=1280, out_features=1280, bias=True)
      )
      (input_blocks): ModuleList(
        (0): TimestepEmbedSequential(
          (0): Conv2d(4, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (1-2): 2 x TimestepEmbedSequential(
          (0): ResBlock(
            (in_layers): Sequential(
              (0): GroupNormSpecific(32, 320, eps=1e-05, affine=True)
              (1): SiLU()
              (2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
            (h_upd): Identity()
            (x_upd): Identity()
            (emb_layers): Sequential(
              (0): SiLU()
              (1): Linear(in_features=1280, out_features=320, bias=True)
            )
            (out_layers): Sequential(
              (0): GroupNormSpecific(32, 320, eps=1e-05, affine=True)
              (1): SiLU()
              (2): Dropout(p=0.0, inplace=False)
              (3): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
            (skip_connection): Identity()
            (temopral_conv): TemporalConvBlock(
              (conv1): Sequential(
                (0): GroupNorm(32, 320, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Conv3d(320, 320, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
              )
              (conv2): Sequential(
                (0): GroupNorm(32, 320, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Dropout(p=0.1, inplace=False)
                (3): Conv3d(320, 320, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
              )
              (conv3): Sequential(
                (0): GroupNorm(32, 320, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Dropout(p=0.1, inplace=False)
                (3): Conv3d(320, 320, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
              )
              (conv4): Sequential(
                (0): GroupNorm(32, 320, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Dropout(p=0.1, inplace=False)
                (3): Conv3d(320, 320, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
              )
            )
          )
          (1): SpatialTransformer(
            (norm): GroupNorm(32, 320, eps=1e-06, affine=True)
            (proj_in): Linear(in_features=320, out_features=320, bias=True)
            (transformer_blocks): ModuleList(
              (0): BasicTransformerBlock(
                (attn1): CrossAttention(
                  (to_q): Linear(in_features=320, out_features=320, bias=False)
                  (to_k): Linear(in_features=320, out_features=320, bias=False)
                  (to_v): Linear(in_features=320, out_features=320, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=320, out_features=320, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
                (ff): FeedForward(
                  (net): Sequential(
                    (0): GEGLU(
                      (proj): Linear(in_features=320, out_features=2560, bias=True)
                    )
                    (1): Dropout(p=0.0, inplace=False)
                    (2): Linear(in_features=1280, out_features=320, bias=True)
                  )
                )
                (attn2): CrossAttention(
                  (to_q): Linear(in_features=320, out_features=320, bias=False)
                  (to_k): Linear(in_features=1024, out_features=320, bias=False)
                  (to_v): Linear(in_features=1024, out_features=320, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=320, out_features=320, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
                (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
                (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
              )
            )
            (proj_out): Linear(in_features=320, out_features=320, bias=True)
          )
          (2): TemporalTransformer(
            (norm): GroupNorm(32, 320, eps=1e-06, affine=True)
            (proj_in): Linear(in_features=320, out_features=320, bias=True)
            (transformer_blocks): ModuleList(
              (0): BasicTransformerBlock(
                (attn1): CrossAttention(
                  (to_q): Linear(in_features=320, out_features=320, bias=False)
                  (to_k): Linear(in_features=320, out_features=320, bias=False)
                  (to_v): Linear(in_features=320, out_features=320, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=320, out_features=320, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
                (ff): FeedForward(
                  (net): Sequential(
                    (0): GEGLU(
                      (proj): Linear(in_features=320, out_features=2560, bias=True)
                    )
                    (1): Dropout(p=0.0, inplace=False)
                    (2): Linear(in_features=1280, out_features=320, bias=True)
                  )
                )
                (attn2): CrossAttention(
                  (to_q): Linear(in_features=320, out_features=320, bias=False)
                  (to_k): Linear(in_features=320, out_features=320, bias=False)
                  (to_v): Linear(in_features=320, out_features=320, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=320, out_features=320, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
                (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
                (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
              )
            )
            (proj_out): Linear(in_features=320, out_features=320, bias=True)
          )
        )
        (3): TimestepEmbedSequential(
          (0): Downsample(
            (op): Conv2d(320, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
          )
        )
        (4): TimestepEmbedSequential(
          (0): ResBlock(
            (in_layers): Sequential(
              (0): GroupNormSpecific(32, 320, eps=1e-05, affine=True)
              (1): SiLU()
              (2): Conv2d(320, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
            (h_upd): Identity()
            (x_upd): Identity()
            (emb_layers): Sequential(
              (0): SiLU()
              (1): Linear(in_features=1280, out_features=640, bias=True)
            )
            (out_layers): Sequential(
              (0): GroupNormSpecific(32, 640, eps=1e-05, affine=True)
              (1): SiLU()
              (2): Dropout(p=0.0, inplace=False)
              (3): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
            (skip_connection): Conv2d(320, 640, kernel_size=(1, 1), stride=(1, 1))
            (temopral_conv): TemporalConvBlock(
              (conv1): Sequential(
                (0): GroupNorm(32, 640, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Conv3d(640, 640, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
              )
              (conv2): Sequential(
                (0): GroupNorm(32, 640, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Dropout(p=0.1, inplace=False)
                (3): Conv3d(640, 640, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
              )
              (conv3): Sequential(
                (0): GroupNorm(32, 640, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Dropout(p=0.1, inplace=False)
                (3): Conv3d(640, 640, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
              )
              (conv4): Sequential(
                (0): GroupNorm(32, 640, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Dropout(p=0.1, inplace=False)
                (3): Conv3d(640, 640, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
              )
            )
          )
          (1): SpatialTransformer(
            (norm): GroupNorm(32, 640, eps=1e-06, affine=True)
            (proj_in): Linear(in_features=640, out_features=640, bias=True)
            (transformer_blocks): ModuleList(
              (0): BasicTransformerBlock(
                (attn1): CrossAttention(
                  (to_q): Linear(in_features=640, out_features=640, bias=False)
                  (to_k): Linear(in_features=640, out_features=640, bias=False)
                  (to_v): Linear(in_features=640, out_features=640, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=640, out_features=640, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
                (ff): FeedForward(
                  (net): Sequential(
                    (0): GEGLU(
                      (proj): Linear(in_features=640, out_features=5120, bias=True)
                    )
                    (1): Dropout(p=0.0, inplace=False)
                    (2): Linear(in_features=2560, out_features=640, bias=True)
                  )
                )
                (attn2): CrossAttention(
                  (to_q): Linear(in_features=640, out_features=640, bias=False)
                  (to_k): Linear(in_features=1024, out_features=640, bias=False)
                  (to_v): Linear(in_features=1024, out_features=640, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=640, out_features=640, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
                (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
                (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
              )
            )
            (proj_out): Linear(in_features=640, out_features=640, bias=True)
          )
          (2): TemporalTransformer(
            (norm): GroupNorm(32, 640, eps=1e-06, affine=True)
            (proj_in): Linear(in_features=640, out_features=640, bias=True)
            (transformer_blocks): ModuleList(
              (0): BasicTransformerBlock(
                (attn1): CrossAttention(
                  (to_q): Linear(in_features=640, out_features=640, bias=False)
                  (to_k): Linear(in_features=640, out_features=640, bias=False)
                  (to_v): Linear(in_features=640, out_features=640, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=640, out_features=640, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
                (ff): FeedForward(
                  (net): Sequential(
                    (0): GEGLU(
                      (proj): Linear(in_features=640, out_features=5120, bias=True)
                    )
                    (1): Dropout(p=0.0, inplace=False)
                    (2): Linear(in_features=2560, out_features=640, bias=True)
                  )
                )
                (attn2): CrossAttention(
                  (to_q): Linear(in_features=640, out_features=640, bias=False)
                  (to_k): Linear(in_features=640, out_features=640, bias=False)
                  (to_v): Linear(in_features=640, out_features=640, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=640, out_features=640, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
                (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
                (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
              )
            )
            (proj_out): Linear(in_features=640, out_features=640, bias=True)
          )
        )
        (5): TimestepEmbedSequential(
          (0): ResBlock(
            (in_layers): Sequential(
              (0): GroupNormSpecific(32, 640, eps=1e-05, affine=True)
              (1): SiLU()
              (2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
            (h_upd): Identity()
            (x_upd): Identity()
            (emb_layers): Sequential(
              (0): SiLU()
              (1): Linear(in_features=1280, out_features=640, bias=True)
            )
            (out_layers): Sequential(
              (0): GroupNormSpecific(32, 640, eps=1e-05, affine=True)
              (1): SiLU()
              (2): Dropout(p=0.0, inplace=False)
              (3): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
            (skip_connection): Identity()
            (temopral_conv): TemporalConvBlock(
              (conv1): Sequential(
                (0): GroupNorm(32, 640, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Conv3d(640, 640, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
              )
              (conv2): Sequential(
                (0): GroupNorm(32, 640, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Dropout(p=0.1, inplace=False)
                (3): Conv3d(640, 640, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
              )
              (conv3): Sequential(
                (0): GroupNorm(32, 640, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Dropout(p=0.1, inplace=False)
                (3): Conv3d(640, 640, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
              )
              (conv4): Sequential(
                (0): GroupNorm(32, 640, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Dropout(p=0.1, inplace=False)
                (3): Conv3d(640, 640, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
              )
            )
          )
          (1): SpatialTransformer(
            (norm): GroupNorm(32, 640, eps=1e-06, affine=True)
            (proj_in): Linear(in_features=640, out_features=640, bias=True)
            (transformer_blocks): ModuleList(
              (0): BasicTransformerBlock(
                (attn1): CrossAttention(
                  (to_q): Linear(in_features=640, out_features=640, bias=False)
                  (to_k): Linear(in_features=640, out_features=640, bias=False)
                  (to_v): Linear(in_features=640, out_features=640, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=640, out_features=640, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
                (ff): FeedForward(
                  (net): Sequential(
                    (0): GEGLU(
                      (proj): Linear(in_features=640, out_features=5120, bias=True)
                    )
                    (1): Dropout(p=0.0, inplace=False)
                    (2): Linear(in_features=2560, out_features=640, bias=True)
                  )
                )
                (attn2): CrossAttention(
                  (to_q): Linear(in_features=640, out_features=640, bias=False)
                  (to_k): Linear(in_features=1024, out_features=640, bias=False)
                  (to_v): Linear(in_features=1024, out_features=640, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=640, out_features=640, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
                (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
                (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
              )
            )
            (proj_out): Linear(in_features=640, out_features=640, bias=True)
          )
          (2): TemporalTransformer(
            (norm): GroupNorm(32, 640, eps=1e-06, affine=True)
            (proj_in): Linear(in_features=640, out_features=640, bias=True)
            (transformer_blocks): ModuleList(
              (0): BasicTransformerBlock(
                (attn1): CrossAttention(
                  (to_q): Linear(in_features=640, out_features=640, bias=False)
                  (to_k): Linear(in_features=640, out_features=640, bias=False)
                  (to_v): Linear(in_features=640, out_features=640, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=640, out_features=640, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
                (ff): FeedForward(
                  (net): Sequential(
                    (0): GEGLU(
                      (proj): Linear(in_features=640, out_features=5120, bias=True)
                    )
                    (1): Dropout(p=0.0, inplace=False)
                    (2): Linear(in_features=2560, out_features=640, bias=True)
                  )
                )
                (attn2): CrossAttention(
                  (to_q): Linear(in_features=640, out_features=640, bias=False)
                  (to_k): Linear(in_features=640, out_features=640, bias=False)
                  (to_v): Linear(in_features=640, out_features=640, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=640, out_features=640, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
                (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
                (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
              )
            )
            (proj_out): Linear(in_features=640, out_features=640, bias=True)
          )
        )
        (6): TimestepEmbedSequential(
          (0): Downsample(
            (op): Conv2d(640, 640, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
          )
        )
        (7): TimestepEmbedSequential(
          (0): ResBlock(
            (in_layers): Sequential(
              (0): GroupNormSpecific(32, 640, eps=1e-05, affine=True)
              (1): SiLU()
              (2): Conv2d(640, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
            (h_upd): Identity()
            (x_upd): Identity()
            (emb_layers): Sequential(
              (0): SiLU()
              (1): Linear(in_features=1280, out_features=1280, bias=True)
            )
            (out_layers): Sequential(
              (0): GroupNormSpecific(32, 1280, eps=1e-05, affine=True)
              (1): SiLU()
              (2): Dropout(p=0.0, inplace=False)
              (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
            (skip_connection): Conv2d(640, 1280, kernel_size=(1, 1), stride=(1, 1))
            (temopral_conv): TemporalConvBlock(
              (conv1): Sequential(
                (0): GroupNorm(32, 1280, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
              )
              (conv2): Sequential(
                (0): GroupNorm(32, 1280, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Dropout(p=0.1, inplace=False)
                (3): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
              )
              (conv3): Sequential(
                (0): GroupNorm(32, 1280, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Dropout(p=0.1, inplace=False)
                (3): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
              )
              (conv4): Sequential(
                (0): GroupNorm(32, 1280, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Dropout(p=0.1, inplace=False)
                (3): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
              )
            )
          )
          (1): SpatialTransformer(
            (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)
            (proj_in): Linear(in_features=1280, out_features=1280, bias=True)
            (transformer_blocks): ModuleList(
              (0): BasicTransformerBlock(
                (attn1): CrossAttention(
                  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                  (to_k): Linear(in_features=1280, out_features=1280, bias=False)
                  (to_v): Linear(in_features=1280, out_features=1280, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=1280, out_features=1280, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
                (ff): FeedForward(
                  (net): Sequential(
                    (0): GEGLU(
                      (proj): Linear(in_features=1280, out_features=10240, bias=True)
                    )
                    (1): Dropout(p=0.0, inplace=False)
                    (2): Linear(in_features=5120, out_features=1280, bias=True)
                  )
                )
                (attn2): CrossAttention(
                  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                  (to_k): Linear(in_features=1024, out_features=1280, bias=False)
                  (to_v): Linear(in_features=1024, out_features=1280, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=1280, out_features=1280, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
                (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
                (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              )
            )
            (proj_out): Linear(in_features=1280, out_features=1280, bias=True)
          )
          (2): TemporalTransformer(
            (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)
            (proj_in): Linear(in_features=1280, out_features=1280, bias=True)
            (transformer_blocks): ModuleList(
              (0): BasicTransformerBlock(
                (attn1): CrossAttention(
                  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                  (to_k): Linear(in_features=1280, out_features=1280, bias=False)
                  (to_v): Linear(in_features=1280, out_features=1280, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=1280, out_features=1280, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
                (ff): FeedForward(
                  (net): Sequential(
                    (0): GEGLU(
                      (proj): Linear(in_features=1280, out_features=10240, bias=True)
                    )
                    (1): Dropout(p=0.0, inplace=False)
                    (2): Linear(in_features=5120, out_features=1280, bias=True)
                  )
                )
                (attn2): CrossAttention(
                  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                  (to_k): Linear(in_features=1280, out_features=1280, bias=False)
                  (to_v): Linear(in_features=1280, out_features=1280, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=1280, out_features=1280, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
                (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
                (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              )
            )
            (proj_out): Linear(in_features=1280, out_features=1280, bias=True)
          )
        )
        (8): TimestepEmbedSequential(
          (0): ResBlock(
            (in_layers): Sequential(
              (0): GroupNormSpecific(32, 1280, eps=1e-05, affine=True)
              (1): SiLU()
              (2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
            (h_upd): Identity()
            (x_upd): Identity()
            (emb_layers): Sequential(
              (0): SiLU()
              (1): Linear(in_features=1280, out_features=1280, bias=True)
            )
            (out_layers): Sequential(
              (0): GroupNormSpecific(32, 1280, eps=1e-05, affine=True)
              (1): SiLU()
              (2): Dropout(p=0.0, inplace=False)
              (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
            (skip_connection): Identity()
            (temopral_conv): TemporalConvBlock(
              (conv1): Sequential(
                (0): GroupNorm(32, 1280, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
              )
              (conv2): Sequential(
                (0): GroupNorm(32, 1280, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Dropout(p=0.1, inplace=False)
                (3): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
              )
              (conv3): Sequential(
                (0): GroupNorm(32, 1280, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Dropout(p=0.1, inplace=False)
                (3): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
              )
              (conv4): Sequential(
                (0): GroupNorm(32, 1280, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Dropout(p=0.1, inplace=False)
                (3): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
              )
            )
          )
          (1): SpatialTransformer(
            (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)
            (proj_in): Linear(in_features=1280, out_features=1280, bias=True)
            (transformer_blocks): ModuleList(
              (0): BasicTransformerBlock(
                (attn1): CrossAttention(
                  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                  (to_k): Linear(in_features=1280, out_features=1280, bias=False)
                  (to_v): Linear(in_features=1280, out_features=1280, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=1280, out_features=1280, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
                (ff): FeedForward(
                  (net): Sequential(
                    (0): GEGLU(
                      (proj): Linear(in_features=1280, out_features=10240, bias=True)
                    )
                    (1): Dropout(p=0.0, inplace=False)
                    (2): Linear(in_features=5120, out_features=1280, bias=True)
                  )
                )
                (attn2): CrossAttention(
                  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                  (to_k): Linear(in_features=1024, out_features=1280, bias=False)
                  (to_v): Linear(in_features=1024, out_features=1280, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=1280, out_features=1280, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
                (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
                (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              )
            )
            (proj_out): Linear(in_features=1280, out_features=1280, bias=True)
          )
          (2): TemporalTransformer(
            (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)
            (proj_in): Linear(in_features=1280, out_features=1280, bias=True)
            (transformer_blocks): ModuleList(
              (0): BasicTransformerBlock(
                (attn1): CrossAttention(
                  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                  (to_k): Linear(in_features=1280, out_features=1280, bias=False)
                  (to_v): Linear(in_features=1280, out_features=1280, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=1280, out_features=1280, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
                (ff): FeedForward(
                  (net): Sequential(
                    (0): GEGLU(
                      (proj): Linear(in_features=1280, out_features=10240, bias=True)
                    )
                    (1): Dropout(p=0.0, inplace=False)
                    (2): Linear(in_features=5120, out_features=1280, bias=True)
                  )
                )
                (attn2): CrossAttention(
                  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                  (to_k): Linear(in_features=1280, out_features=1280, bias=False)
                  (to_v): Linear(in_features=1280, out_features=1280, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=1280, out_features=1280, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
                (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
                (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              )
            )
            (proj_out): Linear(in_features=1280, out_features=1280, bias=True)
          )
        )
        (9): TimestepEmbedSequential(
          (0): Downsample(
            (op): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
          )
        )
        (10-11): 2 x TimestepEmbedSequential(
          (0): ResBlock(
            (in_layers): Sequential(
              (0): GroupNormSpecific(32, 1280, eps=1e-05, affine=True)
              (1): SiLU()
              (2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
            (h_upd): Identity()
            (x_upd): Identity()
            (emb_layers): Sequential(
              (0): SiLU()
              (1): Linear(in_features=1280, out_features=1280, bias=True)
            )
            (out_layers): Sequential(
              (0): GroupNormSpecific(32, 1280, eps=1e-05, affine=True)
              (1): SiLU()
              (2): Dropout(p=0.0, inplace=False)
              (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
            (skip_connection): Identity()
            (temopral_conv): TemporalConvBlock(
              (conv1): Sequential(
                (0): GroupNorm(32, 1280, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
              )
              (conv2): Sequential(
                (0): GroupNorm(32, 1280, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Dropout(p=0.1, inplace=False)
                (3): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
              )
              (conv3): Sequential(
                (0): GroupNorm(32, 1280, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Dropout(p=0.1, inplace=False)
                (3): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
              )
              (conv4): Sequential(
                (0): GroupNorm(32, 1280, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Dropout(p=0.1, inplace=False)
                (3): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
              )
            )
          )
        )
      )
      (init_attn): TimestepEmbedSequential(
        (0): TemporalTransformer(
          (norm): GroupNorm(32, 320, eps=1e-06, affine=True)
          (proj_in): Conv1d(320, 512, kernel_size=(1,), stride=(1,))
          (transformer_blocks): ModuleList(
            (0): BasicTransformerBlock(
              (attn1): CrossAttention(
                (to_q): Linear(in_features=512, out_features=512, bias=False)
                (to_k): Linear(in_features=512, out_features=512, bias=False)
                (to_v): Linear(in_features=512, out_features=512, bias=False)
                (to_out): Sequential(
                  (0): Linear(in_features=512, out_features=512, bias=True)
                  (1): Dropout(p=0.0, inplace=False)
                )
              )
              (ff): FeedForward(
                (net): Sequential(
                  (0): GEGLU(
                    (proj): Linear(in_features=512, out_features=4096, bias=True)
                  )
                  (1): Dropout(p=0.0, inplace=False)
                  (2): Linear(in_features=2048, out_features=512, bias=True)
                )
              )
              (attn2): CrossAttention(
                (to_q): Linear(in_features=512, out_features=512, bias=False)
                (to_k): Linear(in_features=512, out_features=512, bias=False)
                (to_v): Linear(in_features=512, out_features=512, bias=False)
                (to_out): Sequential(
                  (0): Linear(in_features=512, out_features=512, bias=True)
                  (1): Dropout(p=0.0, inplace=False)
                )
              )
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            )
          )
          (proj_out): Conv1d(512, 320, kernel_size=(1,), stride=(1,))
        )
      )
      (middle_block): TimestepEmbedSequential(
        (0): ResBlock(
          (in_layers): Sequential(
            (0): GroupNormSpecific(32, 1280, eps=1e-05, affine=True)
            (1): SiLU()
            (2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
          (h_upd): Identity()
          (x_upd): Identity()
          (emb_layers): Sequential(
            (0): SiLU()
            (1): Linear(in_features=1280, out_features=1280, bias=True)
          )
          (out_layers): Sequential(
            (0): GroupNormSpecific(32, 1280, eps=1e-05, affine=True)
            (1): SiLU()
            (2): Dropout(p=0.0, inplace=False)
            (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
          (skip_connection): Identity()
          (temopral_conv): TemporalConvBlock(
            (conv1): Sequential(
              (0): GroupNorm(32, 1280, eps=1e-05, affine=True)
              (1): SiLU()
              (2): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
            )
            (conv2): Sequential(
              (0): GroupNorm(32, 1280, eps=1e-05, affine=True)
              (1): SiLU()
              (2): Dropout(p=0.1, inplace=False)
              (3): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
            )
            (conv3): Sequential(
              (0): GroupNorm(32, 1280, eps=1e-05, affine=True)
              (1): SiLU()
              (2): Dropout(p=0.1, inplace=False)
              (3): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
            )
            (conv4): Sequential(
              (0): GroupNorm(32, 1280, eps=1e-05, affine=True)
              (1): SiLU()
              (2): Dropout(p=0.1, inplace=False)
              (3): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
            )
          )
        )
        (1): SpatialTransformer(
          (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)
          (proj_in): Linear(in_features=1280, out_features=1280, bias=True)
          (transformer_blocks): ModuleList(
            (0): BasicTransformerBlock(
              (attn1): CrossAttention(
                (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                (to_k): Linear(in_features=1280, out_features=1280, bias=False)
                (to_v): Linear(in_features=1280, out_features=1280, bias=False)
                (to_out): Sequential(
                  (0): Linear(in_features=1280, out_features=1280, bias=True)
                  (1): Dropout(p=0.0, inplace=False)
                )
              )
              (ff): FeedForward(
                (net): Sequential(
                  (0): GEGLU(
                    (proj): Linear(in_features=1280, out_features=10240, bias=True)
                  )
                  (1): Dropout(p=0.0, inplace=False)
                  (2): Linear(in_features=5120, out_features=1280, bias=True)
                )
              )
              (attn2): CrossAttention(
                (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                (to_k): Linear(in_features=1024, out_features=1280, bias=False)
                (to_v): Linear(in_features=1024, out_features=1280, bias=False)
                (to_out): Sequential(
                  (0): Linear(in_features=1280, out_features=1280, bias=True)
                  (1): Dropout(p=0.0, inplace=False)
                )
              )
              (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            )
          )
          (proj_out): Linear(in_features=1280, out_features=1280, bias=True)
        )
        (2): TemporalTransformer(
          (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)
          (proj_in): Linear(in_features=1280, out_features=1280, bias=True)
          (transformer_blocks): ModuleList(
            (0): BasicTransformerBlock(
              (attn1): CrossAttention(
                (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                (to_k): Linear(in_features=1280, out_features=1280, bias=False)
                (to_v): Linear(in_features=1280, out_features=1280, bias=False)
                (to_out): Sequential(
                  (0): Linear(in_features=1280, out_features=1280, bias=True)
                  (1): Dropout(p=0.0, inplace=False)
                )
              )
              (ff): FeedForward(
                (net): Sequential(
                  (0): GEGLU(
                    (proj): Linear(in_features=1280, out_features=10240, bias=True)
                  )
                  (1): Dropout(p=0.0, inplace=False)
                  (2): Linear(in_features=5120, out_features=1280, bias=True)
                )
              )
              (attn2): CrossAttention(
                (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                (to_k): Linear(in_features=1280, out_features=1280, bias=False)
                (to_v): Linear(in_features=1280, out_features=1280, bias=False)
                (to_out): Sequential(
                  (0): Linear(in_features=1280, out_features=1280, bias=True)
                  (1): Dropout(p=0.0, inplace=False)
                )
              )
              (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            )
          )
          (proj_out): Linear(in_features=1280, out_features=1280, bias=True)
        )
        (3): ResBlock(
          (in_layers): Sequential(
            (0): GroupNormSpecific(32, 1280, eps=1e-05, affine=True)
            (1): SiLU()
            (2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
          (h_upd): Identity()
          (x_upd): Identity()
          (emb_layers): Sequential(
            (0): SiLU()
            (1): Linear(in_features=1280, out_features=1280, bias=True)
          )
          (out_layers): Sequential(
            (0): GroupNormSpecific(32, 1280, eps=1e-05, affine=True)
            (1): SiLU()
            (2): Dropout(p=0.0, inplace=False)
            (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
          (skip_connection): Identity()
          (temopral_conv): TemporalConvBlock(
            (conv1): Sequential(
              (0): GroupNorm(32, 1280, eps=1e-05, affine=True)
              (1): SiLU()
              (2): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
            )
            (conv2): Sequential(
              (0): GroupNorm(32, 1280, eps=1e-05, affine=True)
              (1): SiLU()
              (2): Dropout(p=0.1, inplace=False)
              (3): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
            )
            (conv3): Sequential(
              (0): GroupNorm(32, 1280, eps=1e-05, affine=True)
              (1): SiLU()
              (2): Dropout(p=0.1, inplace=False)
              (3): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
            )
            (conv4): Sequential(
              (0): GroupNorm(32, 1280, eps=1e-05, affine=True)
              (1): SiLU()
              (2): Dropout(p=0.1, inplace=False)
              (3): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
            )
          )
        )
      )
      (output_blocks): ModuleList(
        (0-1): 2 x TimestepEmbedSequential(
          (0): ResBlock(
            (in_layers): Sequential(
              (0): GroupNormSpecific(32, 2560, eps=1e-05, affine=True)
              (1): SiLU()
              (2): Conv2d(2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
            (h_upd): Identity()
            (x_upd): Identity()
            (emb_layers): Sequential(
              (0): SiLU()
              (1): Linear(in_features=1280, out_features=1280, bias=True)
            )
            (out_layers): Sequential(
              (0): GroupNormSpecific(32, 1280, eps=1e-05, affine=True)
              (1): SiLU()
              (2): Dropout(p=0.0, inplace=False)
              (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
            (skip_connection): Conv2d(2560, 1280, kernel_size=(1, 1), stride=(1, 1))
            (temopral_conv): TemporalConvBlock(
              (conv1): Sequential(
                (0): GroupNorm(32, 1280, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
              )
              (conv2): Sequential(
                (0): GroupNorm(32, 1280, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Dropout(p=0.1, inplace=False)
                (3): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
              )
              (conv3): Sequential(
                (0): GroupNorm(32, 1280, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Dropout(p=0.1, inplace=False)
                (3): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
              )
              (conv4): Sequential(
                (0): GroupNorm(32, 1280, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Dropout(p=0.1, inplace=False)
                (3): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
              )
            )
          )
        )
        (2): TimestepEmbedSequential(
          (0): ResBlock(
            (in_layers): Sequential(
              (0): GroupNormSpecific(32, 2560, eps=1e-05, affine=True)
              (1): SiLU()
              (2): Conv2d(2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
            (h_upd): Identity()
            (x_upd): Identity()
            (emb_layers): Sequential(
              (0): SiLU()
              (1): Linear(in_features=1280, out_features=1280, bias=True)
            )
            (out_layers): Sequential(
              (0): GroupNormSpecific(32, 1280, eps=1e-05, affine=True)
              (1): SiLU()
              (2): Dropout(p=0.0, inplace=False)
              (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
            (skip_connection): Conv2d(2560, 1280, kernel_size=(1, 1), stride=(1, 1))
            (temopral_conv): TemporalConvBlock(
              (conv1): Sequential(
                (0): GroupNorm(32, 1280, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
              )
              (conv2): Sequential(
                (0): GroupNorm(32, 1280, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Dropout(p=0.1, inplace=False)
                (3): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
              )
              (conv3): Sequential(
                (0): GroupNorm(32, 1280, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Dropout(p=0.1, inplace=False)
                (3): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
              )
              (conv4): Sequential(
                (0): GroupNorm(32, 1280, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Dropout(p=0.1, inplace=False)
                (3): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
              )
            )
          )
          (1): Upsample(
            (conv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
        )
        (3-4): 2 x TimestepEmbedSequential(
          (0): ResBlock(
            (in_layers): Sequential(
              (0): GroupNormSpecific(32, 2560, eps=1e-05, affine=True)
              (1): SiLU()
              (2): Conv2d(2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
            (h_upd): Identity()
            (x_upd): Identity()
            (emb_layers): Sequential(
              (0): SiLU()
              (1): Linear(in_features=1280, out_features=1280, bias=True)
            )
            (out_layers): Sequential(
              (0): GroupNormSpecific(32, 1280, eps=1e-05, affine=True)
              (1): SiLU()
              (2): Dropout(p=0.0, inplace=False)
              (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
            (skip_connection): Conv2d(2560, 1280, kernel_size=(1, 1), stride=(1, 1))
            (temopral_conv): TemporalConvBlock(
              (conv1): Sequential(
                (0): GroupNorm(32, 1280, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
              )
              (conv2): Sequential(
                (0): GroupNorm(32, 1280, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Dropout(p=0.1, inplace=False)
                (3): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
              )
              (conv3): Sequential(
                (0): GroupNorm(32, 1280, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Dropout(p=0.1, inplace=False)
                (3): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
              )
              (conv4): Sequential(
                (0): GroupNorm(32, 1280, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Dropout(p=0.1, inplace=False)
                (3): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
              )
            )
          )
          (1): SpatialTransformer(
            (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)
            (proj_in): Linear(in_features=1280, out_features=1280, bias=True)
            (transformer_blocks): ModuleList(
              (0): BasicTransformerBlock(
                (attn1): CrossAttention(
                  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                  (to_k): Linear(in_features=1280, out_features=1280, bias=False)
                  (to_v): Linear(in_features=1280, out_features=1280, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=1280, out_features=1280, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
                (ff): FeedForward(
                  (net): Sequential(
                    (0): GEGLU(
                      (proj): Linear(in_features=1280, out_features=10240, bias=True)
                    )
                    (1): Dropout(p=0.0, inplace=False)
                    (2): Linear(in_features=5120, out_features=1280, bias=True)
                  )
                )
                (attn2): CrossAttention(
                  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                  (to_k): Linear(in_features=1024, out_features=1280, bias=False)
                  (to_v): Linear(in_features=1024, out_features=1280, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=1280, out_features=1280, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
                (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
                (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              )
            )
            (proj_out): Linear(in_features=1280, out_features=1280, bias=True)
          )
          (2): TemporalTransformer(
            (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)
            (proj_in): Linear(in_features=1280, out_features=1280, bias=True)
            (transformer_blocks): ModuleList(
              (0): BasicTransformerBlock(
                (attn1): CrossAttention(
                  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                  (to_k): Linear(in_features=1280, out_features=1280, bias=False)
                  (to_v): Linear(in_features=1280, out_features=1280, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=1280, out_features=1280, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
                (ff): FeedForward(
                  (net): Sequential(
                    (0): GEGLU(
                      (proj): Linear(in_features=1280, out_features=10240, bias=True)
                    )
                    (1): Dropout(p=0.0, inplace=False)
                    (2): Linear(in_features=5120, out_features=1280, bias=True)
                  )
                )
                (attn2): CrossAttention(
                  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                  (to_k): Linear(in_features=1280, out_features=1280, bias=False)
                  (to_v): Linear(in_features=1280, out_features=1280, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=1280, out_features=1280, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
                (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
                (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              )
            )
            (proj_out): Linear(in_features=1280, out_features=1280, bias=True)
          )
        )
        (5): TimestepEmbedSequential(
          (0): ResBlock(
            (in_layers): Sequential(
              (0): GroupNormSpecific(32, 1920, eps=1e-05, affine=True)
              (1): SiLU()
              (2): Conv2d(1920, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
            (h_upd): Identity()
            (x_upd): Identity()
            (emb_layers): Sequential(
              (0): SiLU()
              (1): Linear(in_features=1280, out_features=1280, bias=True)
            )
            (out_layers): Sequential(
              (0): GroupNormSpecific(32, 1280, eps=1e-05, affine=True)
              (1): SiLU()
              (2): Dropout(p=0.0, inplace=False)
              (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
            (skip_connection): Conv2d(1920, 1280, kernel_size=(1, 1), stride=(1, 1))
            (temopral_conv): TemporalConvBlock(
              (conv1): Sequential(
                (0): GroupNorm(32, 1280, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
              )
              (conv2): Sequential(
                (0): GroupNorm(32, 1280, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Dropout(p=0.1, inplace=False)
                (3): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
              )
              (conv3): Sequential(
                (0): GroupNorm(32, 1280, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Dropout(p=0.1, inplace=False)
                (3): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
              )
              (conv4): Sequential(
                (0): GroupNorm(32, 1280, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Dropout(p=0.1, inplace=False)
                (3): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
              )
            )
          )
          (1): SpatialTransformer(
            (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)
            (proj_in): Linear(in_features=1280, out_features=1280, bias=True)
            (transformer_blocks): ModuleList(
              (0): BasicTransformerBlock(
                (attn1): CrossAttention(
                  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                  (to_k): Linear(in_features=1280, out_features=1280, bias=False)
                  (to_v): Linear(in_features=1280, out_features=1280, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=1280, out_features=1280, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
                (ff): FeedForward(
                  (net): Sequential(
                    (0): GEGLU(
                      (proj): Linear(in_features=1280, out_features=10240, bias=True)
                    )
                    (1): Dropout(p=0.0, inplace=False)
                    (2): Linear(in_features=5120, out_features=1280, bias=True)
                  )
                )
                (attn2): CrossAttention(
                  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                  (to_k): Linear(in_features=1024, out_features=1280, bias=False)
                  (to_v): Linear(in_features=1024, out_features=1280, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=1280, out_features=1280, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
                (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
                (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              )
            )
            (proj_out): Linear(in_features=1280, out_features=1280, bias=True)
          )
          (2): TemporalTransformer(
            (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)
            (proj_in): Linear(in_features=1280, out_features=1280, bias=True)
            (transformer_blocks): ModuleList(
              (0): BasicTransformerBlock(
                (attn1): CrossAttention(
                  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                  (to_k): Linear(in_features=1280, out_features=1280, bias=False)
                  (to_v): Linear(in_features=1280, out_features=1280, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=1280, out_features=1280, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
                (ff): FeedForward(
                  (net): Sequential(
                    (0): GEGLU(
                      (proj): Linear(in_features=1280, out_features=10240, bias=True)
                    )
                    (1): Dropout(p=0.0, inplace=False)
                    (2): Linear(in_features=5120, out_features=1280, bias=True)
                  )
                )
                (attn2): CrossAttention(
                  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                  (to_k): Linear(in_features=1280, out_features=1280, bias=False)
                  (to_v): Linear(in_features=1280, out_features=1280, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=1280, out_features=1280, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
                (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
                (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              )
            )
            (proj_out): Linear(in_features=1280, out_features=1280, bias=True)
          )
          (3): Upsample(
            (conv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
        )
        (6): TimestepEmbedSequential(
          (0): ResBlock(
            (in_layers): Sequential(
              (0): GroupNormSpecific(32, 1920, eps=1e-05, affine=True)
              (1): SiLU()
              (2): Conv2d(1920, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
            (h_upd): Identity()
            (x_upd): Identity()
            (emb_layers): Sequential(
              (0): SiLU()
              (1): Linear(in_features=1280, out_features=640, bias=True)
            )
            (out_layers): Sequential(
              (0): GroupNormSpecific(32, 640, eps=1e-05, affine=True)
              (1): SiLU()
              (2): Dropout(p=0.0, inplace=False)
              (3): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
            (skip_connection): Conv2d(1920, 640, kernel_size=(1, 1), stride=(1, 1))
            (temopral_conv): TemporalConvBlock(
              (conv1): Sequential(
                (0): GroupNorm(32, 640, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Conv3d(640, 640, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
              )
              (conv2): Sequential(
                (0): GroupNorm(32, 640, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Dropout(p=0.1, inplace=False)
                (3): Conv3d(640, 640, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
              )
              (conv3): Sequential(
                (0): GroupNorm(32, 640, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Dropout(p=0.1, inplace=False)
                (3): Conv3d(640, 640, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
              )
              (conv4): Sequential(
                (0): GroupNorm(32, 640, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Dropout(p=0.1, inplace=False)
                (3): Conv3d(640, 640, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
              )
            )
          )
          (1): SpatialTransformer(
            (norm): GroupNorm(32, 640, eps=1e-06, affine=True)
            (proj_in): Linear(in_features=640, out_features=640, bias=True)
            (transformer_blocks): ModuleList(
              (0): BasicTransformerBlock(
                (attn1): CrossAttention(
                  (to_q): Linear(in_features=640, out_features=640, bias=False)
                  (to_k): Linear(in_features=640, out_features=640, bias=False)
                  (to_v): Linear(in_features=640, out_features=640, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=640, out_features=640, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
                (ff): FeedForward(
                  (net): Sequential(
                    (0): GEGLU(
                      (proj): Linear(in_features=640, out_features=5120, bias=True)
                    )
                    (1): Dropout(p=0.0, inplace=False)
                    (2): Linear(in_features=2560, out_features=640, bias=True)
                  )
                )
                (attn2): CrossAttention(
                  (to_q): Linear(in_features=640, out_features=640, bias=False)
                  (to_k): Linear(in_features=1024, out_features=640, bias=False)
                  (to_v): Linear(in_features=1024, out_features=640, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=640, out_features=640, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
                (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
                (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
              )
            )
            (proj_out): Linear(in_features=640, out_features=640, bias=True)
          )
          (2): TemporalTransformer(
            (norm): GroupNorm(32, 640, eps=1e-06, affine=True)
            (proj_in): Linear(in_features=640, out_features=640, bias=True)
            (transformer_blocks): ModuleList(
              (0): BasicTransformerBlock(
                (attn1): CrossAttention(
                  (to_q): Linear(in_features=640, out_features=640, bias=False)
                  (to_k): Linear(in_features=640, out_features=640, bias=False)
                  (to_v): Linear(in_features=640, out_features=640, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=640, out_features=640, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
                (ff): FeedForward(
                  (net): Sequential(
                    (0): GEGLU(
                      (proj): Linear(in_features=640, out_features=5120, bias=True)
                    )
                    (1): Dropout(p=0.0, inplace=False)
                    (2): Linear(in_features=2560, out_features=640, bias=True)
                  )
                )
                (attn2): CrossAttention(
                  (to_q): Linear(in_features=640, out_features=640, bias=False)
                  (to_k): Linear(in_features=640, out_features=640, bias=False)
                  (to_v): Linear(in_features=640, out_features=640, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=640, out_features=640, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
                (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
                (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
              )
            )
            (proj_out): Linear(in_features=640, out_features=640, bias=True)
          )
        )
        (7): TimestepEmbedSequential(
          (0): ResBlock(
            (in_layers): Sequential(
              (0): GroupNormSpecific(32, 1280, eps=1e-05, affine=True)
              (1): SiLU()
              (2): Conv2d(1280, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
            (h_upd): Identity()
            (x_upd): Identity()
            (emb_layers): Sequential(
              (0): SiLU()
              (1): Linear(in_features=1280, out_features=640, bias=True)
            )
            (out_layers): Sequential(
              (0): GroupNormSpecific(32, 640, eps=1e-05, affine=True)
              (1): SiLU()
              (2): Dropout(p=0.0, inplace=False)
              (3): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
            (skip_connection): Conv2d(1280, 640, kernel_size=(1, 1), stride=(1, 1))
            (temopral_conv): TemporalConvBlock(
              (conv1): Sequential(
                (0): GroupNorm(32, 640, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Conv3d(640, 640, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
              )
              (conv2): Sequential(
                (0): GroupNorm(32, 640, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Dropout(p=0.1, inplace=False)
                (3): Conv3d(640, 640, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
              )
              (conv3): Sequential(
                (0): GroupNorm(32, 640, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Dropout(p=0.1, inplace=False)
                (3): Conv3d(640, 640, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
              )
              (conv4): Sequential(
                (0): GroupNorm(32, 640, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Dropout(p=0.1, inplace=False)
                (3): Conv3d(640, 640, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
              )
            )
          )
          (1): SpatialTransformer(
            (norm): GroupNorm(32, 640, eps=1e-06, affine=True)
            (proj_in): Linear(in_features=640, out_features=640, bias=True)
            (transformer_blocks): ModuleList(
              (0): BasicTransformerBlock(
                (attn1): CrossAttention(
                  (to_q): Linear(in_features=640, out_features=640, bias=False)
                  (to_k): Linear(in_features=640, out_features=640, bias=False)
                  (to_v): Linear(in_features=640, out_features=640, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=640, out_features=640, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
                (ff): FeedForward(
                  (net): Sequential(
                    (0): GEGLU(
                      (proj): Linear(in_features=640, out_features=5120, bias=True)
                    )
                    (1): Dropout(p=0.0, inplace=False)
                    (2): Linear(in_features=2560, out_features=640, bias=True)
                  )
                )
                (attn2): CrossAttention(
                  (to_q): Linear(in_features=640, out_features=640, bias=False)
                  (to_k): Linear(in_features=1024, out_features=640, bias=False)
                  (to_v): Linear(in_features=1024, out_features=640, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=640, out_features=640, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
                (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
                (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
              )
            )
            (proj_out): Linear(in_features=640, out_features=640, bias=True)
          )
          (2): TemporalTransformer(
            (norm): GroupNorm(32, 640, eps=1e-06, affine=True)
            (proj_in): Linear(in_features=640, out_features=640, bias=True)
            (transformer_blocks): ModuleList(
              (0): BasicTransformerBlock(
                (attn1): CrossAttention(
                  (to_q): Linear(in_features=640, out_features=640, bias=False)
                  (to_k): Linear(in_features=640, out_features=640, bias=False)
                  (to_v): Linear(in_features=640, out_features=640, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=640, out_features=640, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
                (ff): FeedForward(
                  (net): Sequential(
                    (0): GEGLU(
                      (proj): Linear(in_features=640, out_features=5120, bias=True)
                    )
                    (1): Dropout(p=0.0, inplace=False)
                    (2): Linear(in_features=2560, out_features=640, bias=True)
                  )
                )
                (attn2): CrossAttention(
                  (to_q): Linear(in_features=640, out_features=640, bias=False)
                  (to_k): Linear(in_features=640, out_features=640, bias=False)
                  (to_v): Linear(in_features=640, out_features=640, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=640, out_features=640, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
                (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
                (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
              )
            )
            (proj_out): Linear(in_features=640, out_features=640, bias=True)
          )
        )
        (8): TimestepEmbedSequential(
          (0): ResBlock(
            (in_layers): Sequential(
              (0): GroupNormSpecific(32, 960, eps=1e-05, affine=True)
              (1): SiLU()
              (2): Conv2d(960, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
            (h_upd): Identity()
            (x_upd): Identity()
            (emb_layers): Sequential(
              (0): SiLU()
              (1): Linear(in_features=1280, out_features=640, bias=True)
            )
            (out_layers): Sequential(
              (0): GroupNormSpecific(32, 640, eps=1e-05, affine=True)
              (1): SiLU()
              (2): Dropout(p=0.0, inplace=False)
              (3): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
            (skip_connection): Conv2d(960, 640, kernel_size=(1, 1), stride=(1, 1))
            (temopral_conv): TemporalConvBlock(
              (conv1): Sequential(
                (0): GroupNorm(32, 640, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Conv3d(640, 640, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
              )
              (conv2): Sequential(
                (0): GroupNorm(32, 640, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Dropout(p=0.1, inplace=False)
                (3): Conv3d(640, 640, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
              )
              (conv3): Sequential(
                (0): GroupNorm(32, 640, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Dropout(p=0.1, inplace=False)
                (3): Conv3d(640, 640, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
              )
              (conv4): Sequential(
                (0): GroupNorm(32, 640, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Dropout(p=0.1, inplace=False)
                (3): Conv3d(640, 640, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
              )
            )
          )
          (1): SpatialTransformer(
            (norm): GroupNorm(32, 640, eps=1e-06, affine=True)
            (proj_in): Linear(in_features=640, out_features=640, bias=True)
            (transformer_blocks): ModuleList(
              (0): BasicTransformerBlock(
                (attn1): CrossAttention(
                  (to_q): Linear(in_features=640, out_features=640, bias=False)
                  (to_k): Linear(in_features=640, out_features=640, bias=False)
                  (to_v): Linear(in_features=640, out_features=640, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=640, out_features=640, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
                (ff): FeedForward(
                  (net): Sequential(
                    (0): GEGLU(
                      (proj): Linear(in_features=640, out_features=5120, bias=True)
                    )
                    (1): Dropout(p=0.0, inplace=False)
                    (2): Linear(in_features=2560, out_features=640, bias=True)
                  )
                )
                (attn2): CrossAttention(
                  (to_q): Linear(in_features=640, out_features=640, bias=False)
                  (to_k): Linear(in_features=1024, out_features=640, bias=False)
                  (to_v): Linear(in_features=1024, out_features=640, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=640, out_features=640, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
                (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
                (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
              )
            )
            (proj_out): Linear(in_features=640, out_features=640, bias=True)
          )
          (2): TemporalTransformer(
            (norm): GroupNorm(32, 640, eps=1e-06, affine=True)
            (proj_in): Linear(in_features=640, out_features=640, bias=True)
            (transformer_blocks): ModuleList(
              (0): BasicTransformerBlock(
                (attn1): CrossAttention(
                  (to_q): Linear(in_features=640, out_features=640, bias=False)
                  (to_k): Linear(in_features=640, out_features=640, bias=False)
                  (to_v): Linear(in_features=640, out_features=640, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=640, out_features=640, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
                (ff): FeedForward(
                  (net): Sequential(
                    (0): GEGLU(
                      (proj): Linear(in_features=640, out_features=5120, bias=True)
                    )
                    (1): Dropout(p=0.0, inplace=False)
                    (2): Linear(in_features=2560, out_features=640, bias=True)
                  )
                )
                (attn2): CrossAttention(
                  (to_q): Linear(in_features=640, out_features=640, bias=False)
                  (to_k): Linear(in_features=640, out_features=640, bias=False)
                  (to_v): Linear(in_features=640, out_features=640, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=640, out_features=640, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
                (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
                (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
              )
            )
            (proj_out): Linear(in_features=640, out_features=640, bias=True)
          )
          (3): Upsample(
            (conv): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
        )
        (9): TimestepEmbedSequential(
          (0): ResBlock(
            (in_layers): Sequential(
              (0): GroupNormSpecific(32, 960, eps=1e-05, affine=True)
              (1): SiLU()
              (2): Conv2d(960, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
            (h_upd): Identity()
            (x_upd): Identity()
            (emb_layers): Sequential(
              (0): SiLU()
              (1): Linear(in_features=1280, out_features=320, bias=True)
            )
            (out_layers): Sequential(
              (0): GroupNormSpecific(32, 320, eps=1e-05, affine=True)
              (1): SiLU()
              (2): Dropout(p=0.0, inplace=False)
              (3): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
            (skip_connection): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1))
            (temopral_conv): TemporalConvBlock(
              (conv1): Sequential(
                (0): GroupNorm(32, 320, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Conv3d(320, 320, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
              )
              (conv2): Sequential(
                (0): GroupNorm(32, 320, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Dropout(p=0.1, inplace=False)
                (3): Conv3d(320, 320, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
              )
              (conv3): Sequential(
                (0): GroupNorm(32, 320, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Dropout(p=0.1, inplace=False)
                (3): Conv3d(320, 320, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
              )
              (conv4): Sequential(
                (0): GroupNorm(32, 320, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Dropout(p=0.1, inplace=False)
                (3): Conv3d(320, 320, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
              )
            )
          )
          (1): SpatialTransformer(
            (norm): GroupNorm(32, 320, eps=1e-06, affine=True)
            (proj_in): Linear(in_features=320, out_features=320, bias=True)
            (transformer_blocks): ModuleList(
              (0): BasicTransformerBlock(
                (attn1): CrossAttention(
                  (to_q): Linear(in_features=320, out_features=320, bias=False)
                  (to_k): Linear(in_features=320, out_features=320, bias=False)
                  (to_v): Linear(in_features=320, out_features=320, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=320, out_features=320, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
                (ff): FeedForward(
                  (net): Sequential(
                    (0): GEGLU(
                      (proj): Linear(in_features=320, out_features=2560, bias=True)
                    )
                    (1): Dropout(p=0.0, inplace=False)
                    (2): Linear(in_features=1280, out_features=320, bias=True)
                  )
                )
                (attn2): CrossAttention(
                  (to_q): Linear(in_features=320, out_features=320, bias=False)
                  (to_k): Linear(in_features=1024, out_features=320, bias=False)
                  (to_v): Linear(in_features=1024, out_features=320, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=320, out_features=320, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
                (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
                (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
              )
            )
            (proj_out): Linear(in_features=320, out_features=320, bias=True)
          )
          (2): TemporalTransformer(
            (norm): GroupNorm(32, 320, eps=1e-06, affine=True)
            (proj_in): Linear(in_features=320, out_features=320, bias=True)
            (transformer_blocks): ModuleList(
              (0): BasicTransformerBlock(
                (attn1): CrossAttention(
                  (to_q): Linear(in_features=320, out_features=320, bias=False)
                  (to_k): Linear(in_features=320, out_features=320, bias=False)
                  (to_v): Linear(in_features=320, out_features=320, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=320, out_features=320, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
                (ff): FeedForward(
                  (net): Sequential(
                    (0): GEGLU(
                      (proj): Linear(in_features=320, out_features=2560, bias=True)
                    )
                    (1): Dropout(p=0.0, inplace=False)
                    (2): Linear(in_features=1280, out_features=320, bias=True)
                  )
                )
                (attn2): CrossAttention(
                  (to_q): Linear(in_features=320, out_features=320, bias=False)
                  (to_k): Linear(in_features=320, out_features=320, bias=False)
                  (to_v): Linear(in_features=320, out_features=320, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=320, out_features=320, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
                (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
                (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
              )
            )
            (proj_out): Linear(in_features=320, out_features=320, bias=True)
          )
        )
        (10-11): 2 x TimestepEmbedSequential(
          (0): ResBlock(
            (in_layers): Sequential(
              (0): GroupNormSpecific(32, 640, eps=1e-05, affine=True)
              (1): SiLU()
              (2): Conv2d(640, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
            (h_upd): Identity()
            (x_upd): Identity()
            (emb_layers): Sequential(
              (0): SiLU()
              (1): Linear(in_features=1280, out_features=320, bias=True)
            )
            (out_layers): Sequential(
              (0): GroupNormSpecific(32, 320, eps=1e-05, affine=True)
              (1): SiLU()
              (2): Dropout(p=0.0, inplace=False)
              (3): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
            (skip_connection): Conv2d(640, 320, kernel_size=(1, 1), stride=(1, 1))
            (temopral_conv): TemporalConvBlock(
              (conv1): Sequential(
                (0): GroupNorm(32, 320, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Conv3d(320, 320, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
              )
              (conv2): Sequential(
                (0): GroupNorm(32, 320, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Dropout(p=0.1, inplace=False)
                (3): Conv3d(320, 320, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
              )
              (conv3): Sequential(
                (0): GroupNorm(32, 320, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Dropout(p=0.1, inplace=False)
                (3): Conv3d(320, 320, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
              )
              (conv4): Sequential(
                (0): GroupNorm(32, 320, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Dropout(p=0.1, inplace=False)
                (3): Conv3d(320, 320, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
              )
            )
          )
          (1): SpatialTransformer(
            (norm): GroupNorm(32, 320, eps=1e-06, affine=True)
            (proj_in): Linear(in_features=320, out_features=320, bias=True)
            (transformer_blocks): ModuleList(
              (0): BasicTransformerBlock(
                (attn1): CrossAttention(
                  (to_q): Linear(in_features=320, out_features=320, bias=False)
                  (to_k): Linear(in_features=320, out_features=320, bias=False)
                  (to_v): Linear(in_features=320, out_features=320, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=320, out_features=320, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
                (ff): FeedForward(
                  (net): Sequential(
                    (0): GEGLU(
                      (proj): Linear(in_features=320, out_features=2560, bias=True)
                    )
                    (1): Dropout(p=0.0, inplace=False)
                    (2): Linear(in_features=1280, out_features=320, bias=True)
                  )
                )
                (attn2): CrossAttention(
                  (to_q): Linear(in_features=320, out_features=320, bias=False)
                  (to_k): Linear(in_features=1024, out_features=320, bias=False)
                  (to_v): Linear(in_features=1024, out_features=320, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=320, out_features=320, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
                (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
                (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
              )
            )
            (proj_out): Linear(in_features=320, out_features=320, bias=True)
          )
          (2): TemporalTransformer(
            (norm): GroupNorm(32, 320, eps=1e-06, affine=True)
            (proj_in): Linear(in_features=320, out_features=320, bias=True)
            (transformer_blocks): ModuleList(
              (0): BasicTransformerBlock(
                (attn1): CrossAttention(
                  (to_q): Linear(in_features=320, out_features=320, bias=False)
                  (to_k): Linear(in_features=320, out_features=320, bias=False)
                  (to_v): Linear(in_features=320, out_features=320, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=320, out_features=320, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
                (ff): FeedForward(
                  (net): Sequential(
                    (0): GEGLU(
                      (proj): Linear(in_features=320, out_features=2560, bias=True)
                    )
                    (1): Dropout(p=0.0, inplace=False)
                    (2): Linear(in_features=1280, out_features=320, bias=True)
                  )
                )
                (attn2): CrossAttention(
                  (to_q): Linear(in_features=320, out_features=320, bias=False)
                  (to_k): Linear(in_features=320, out_features=320, bias=False)
                  (to_v): Linear(in_features=320, out_features=320, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=320, out_features=320, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
                (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
                (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
              )
            )
            (proj_out): Linear(in_features=320, out_features=320, bias=True)
          )
        )
      )
      (out): Sequential(
        (0): GroupNormSpecific(32, 320, eps=1e-05, affine=True)
        (1): SiLU()
        (2): Conv2d(320, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
    )
  )
  (first_stage_model): AutoencoderKL(
    (encoder): Encoder(
      (conv_in): Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (down): ModuleList(
        (0): Module(
          (block): ModuleList(
            (0-1): 2 x ResnetBlock(
              (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
          )
          (attn): ModuleList()
          (downsample): Downsample(
            (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2))
          )
        )
        (1): Module(
          (block): ModuleList(
            (0): ResnetBlock(
              (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)
              (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (nin_shortcut): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))
            )
            (1): ResnetBlock(
              (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
          )
          (attn): ModuleList()
          (downsample): Downsample(
            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
          )
        )
        (2): Module(
          (block): ModuleList(
            (0): ResnetBlock(
              (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)
              (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (nin_shortcut): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))
            )
            (1): ResnetBlock(
              (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)
              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
          )
          (attn): ModuleList()
          (downsample): Downsample(
            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2))
          )
        )
        (3): Module(
          (block): ModuleList(
            (0-1): 2 x ResnetBlock(
              (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)
              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
          )
          (attn): ModuleList()
        )
      )
      (mid): Module(
        (block_1): ResnetBlock(
          (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)
          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (attn_1): AttnBlock(
          (norm): GroupNorm(32, 512, eps=1e-06, affine=True)
          (q): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
          (k): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
          (v): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
          (proj_out): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
        )
        (block_2): ResnetBlock(
          (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)
          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
      )
      (norm_out): GroupNorm(32, 512, eps=1e-06, affine=True)
      (conv_out): Conv2d(512, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (decoder): Decoder(
      (conv_in): Conv2d(4, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (mid): Module(
        (block_1): ResnetBlock(
          (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)
          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (attn_1): AttnBlock(
          (norm): GroupNorm(32, 512, eps=1e-06, affine=True)
          (q): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
          (k): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
          (v): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
          (proj_out): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
        )
        (block_2): ResnetBlock(
          (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)
          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
      )
      (up): ModuleList(
        (0): Module(
          (block): ModuleList(
            (0): ResnetBlock(
              (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)
              (conv1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (nin_shortcut): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
            )
            (1-2): 2 x ResnetBlock(
              (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
          )
          (attn): ModuleList()
        )
        (1): Module(
          (block): ModuleList(
            (0): ResnetBlock(
              (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)
              (conv1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (nin_shortcut): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
            )
            (1-2): 2 x ResnetBlock(
              (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
          )
          (attn): ModuleList()
          (upsample): Upsample(
            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
        )
        (2-3): 2 x Module(
          (block): ModuleList(
            (0-2): 3 x ResnetBlock(
              (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)
              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
          )
          (attn): ModuleList()
          (upsample): Upsample(
            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
        )
      )
      (norm_out): GroupNorm(32, 128, eps=1e-06, affine=True)
      (conv_out): Conv2d(128, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (loss): Identity()
    (quant_conv): Conv2d(8, 8, kernel_size=(1, 1), stride=(1, 1))
    (post_quant_conv): Conv2d(4, 4, kernel_size=(1, 1), stride=(1, 1))
  )
  (cond_stage_model): FrozenOpenCLIPEmbedder(
    (model): CLIP(
      (transformer): Transformer(
        (resblocks): ModuleList(
          (0-23): 24 x ResidualAttentionBlock(
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
            )
            (ls_1): Identity()
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): GELU(approximate='none')
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ls_2): Identity()
          )
        )
      )
      (token_embedding): Embedding(49408, 1024)
      (ln_final): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    )
  )
), 'teacher_model': LatentDiffusion(
  (model): DiffusionWrapper(
    (diffusion_model): UNetModel(
      (time_embed): Sequential(
        (0): Linear(in_features=320, out_features=1280, bias=True)
        (1): SiLU()
        (2): Linear(in_features=1280, out_features=1280, bias=True)
      )
      (fps_embedding): Sequential(
        (0): Linear(in_features=320, out_features=1280, bias=True)
        (1): SiLU()
        (2): Linear(in_features=1280, out_features=1280, bias=True)
      )
      (input_blocks): ModuleList(
        (0): TimestepEmbedSequential(
          (0): Conv2d(4, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (1-2): 2 x TimestepEmbedSequential(
          (0): ResBlock(
            (in_layers): Sequential(
              (0): GroupNormSpecific(32, 320, eps=1e-05, affine=True)
              (1): SiLU()
              (2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
            (h_upd): Identity()
            (x_upd): Identity()
            (emb_layers): Sequential(
              (0): SiLU()
              (1): Linear(in_features=1280, out_features=320, bias=True)
            )
            (out_layers): Sequential(
              (0): GroupNormSpecific(32, 320, eps=1e-05, affine=True)
              (1): SiLU()
              (2): Dropout(p=0.0, inplace=False)
              (3): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
            (skip_connection): Identity()
            (temopral_conv): TemporalConvBlock(
              (conv1): Sequential(
                (0): GroupNorm(32, 320, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Conv3d(320, 320, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
              )
              (conv2): Sequential(
                (0): GroupNorm(32, 320, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Dropout(p=0.1, inplace=False)
                (3): Conv3d(320, 320, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
              )
              (conv3): Sequential(
                (0): GroupNorm(32, 320, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Dropout(p=0.1, inplace=False)
                (3): Conv3d(320, 320, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
              )
              (conv4): Sequential(
                (0): GroupNorm(32, 320, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Dropout(p=0.1, inplace=False)
                (3): Conv3d(320, 320, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
              )
            )
          )
          (1): SpatialTransformer(
            (norm): GroupNorm(32, 320, eps=1e-06, affine=True)
            (proj_in): Linear(in_features=320, out_features=320, bias=True)
            (transformer_blocks): ModuleList(
              (0): BasicTransformerBlock(
                (attn1): CrossAttention(
                  (to_q): Linear(in_features=320, out_features=320, bias=False)
                  (to_k): Linear(in_features=320, out_features=320, bias=False)
                  (to_v): Linear(in_features=320, out_features=320, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=320, out_features=320, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
                (ff): FeedForward(
                  (net): Sequential(
                    (0): GEGLU(
                      (proj): Linear(in_features=320, out_features=2560, bias=True)
                    )
                    (1): Dropout(p=0.0, inplace=False)
                    (2): Linear(in_features=1280, out_features=320, bias=True)
                  )
                )
                (attn2): CrossAttention(
                  (to_q): Linear(in_features=320, out_features=320, bias=False)
                  (to_k): Linear(in_features=1024, out_features=320, bias=False)
                  (to_v): Linear(in_features=1024, out_features=320, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=320, out_features=320, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
                (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
                (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
              )
            )
            (proj_out): Linear(in_features=320, out_features=320, bias=True)
          )
          (2): TemporalTransformer(
            (norm): GroupNorm(32, 320, eps=1e-06, affine=True)
            (proj_in): Linear(in_features=320, out_features=320, bias=True)
            (transformer_blocks): ModuleList(
              (0): BasicTransformerBlock(
                (attn1): CrossAttention(
                  (to_q): Linear(in_features=320, out_features=320, bias=False)
                  (to_k): Linear(in_features=320, out_features=320, bias=False)
                  (to_v): Linear(in_features=320, out_features=320, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=320, out_features=320, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
                (ff): FeedForward(
                  (net): Sequential(
                    (0): GEGLU(
                      (proj): Linear(in_features=320, out_features=2560, bias=True)
                    )
                    (1): Dropout(p=0.0, inplace=False)
                    (2): Linear(in_features=1280, out_features=320, bias=True)
                  )
                )
                (attn2): CrossAttention(
                  (to_q): Linear(in_features=320, out_features=320, bias=False)
                  (to_k): Linear(in_features=320, out_features=320, bias=False)
                  (to_v): Linear(in_features=320, out_features=320, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=320, out_features=320, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
                (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
                (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
              )
            )
            (proj_out): Linear(in_features=320, out_features=320, bias=True)
          )
        )
        (3): TimestepEmbedSequential(
          (0): Downsample(
            (op): Conv2d(320, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
          )
        )
        (4): TimestepEmbedSequential(
          (0): ResBlock(
            (in_layers): Sequential(
              (0): GroupNormSpecific(32, 320, eps=1e-05, affine=True)
              (1): SiLU()
              (2): Conv2d(320, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
            (h_upd): Identity()
            (x_upd): Identity()
            (emb_layers): Sequential(
              (0): SiLU()
              (1): Linear(in_features=1280, out_features=640, bias=True)
            )
            (out_layers): Sequential(
              (0): GroupNormSpecific(32, 640, eps=1e-05, affine=True)
              (1): SiLU()
              (2): Dropout(p=0.0, inplace=False)
              (3): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
            (skip_connection): Conv2d(320, 640, kernel_size=(1, 1), stride=(1, 1))
            (temopral_conv): TemporalConvBlock(
              (conv1): Sequential(
                (0): GroupNorm(32, 640, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Conv3d(640, 640, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
              )
              (conv2): Sequential(
                (0): GroupNorm(32, 640, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Dropout(p=0.1, inplace=False)
                (3): Conv3d(640, 640, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
              )
              (conv3): Sequential(
                (0): GroupNorm(32, 640, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Dropout(p=0.1, inplace=False)
                (3): Conv3d(640, 640, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
              )
              (conv4): Sequential(
                (0): GroupNorm(32, 640, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Dropout(p=0.1, inplace=False)
                (3): Conv3d(640, 640, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
              )
            )
          )
          (1): SpatialTransformer(
            (norm): GroupNorm(32, 640, eps=1e-06, affine=True)
            (proj_in): Linear(in_features=640, out_features=640, bias=True)
            (transformer_blocks): ModuleList(
              (0): BasicTransformerBlock(
                (attn1): CrossAttention(
                  (to_q): Linear(in_features=640, out_features=640, bias=False)
                  (to_k): Linear(in_features=640, out_features=640, bias=False)
                  (to_v): Linear(in_features=640, out_features=640, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=640, out_features=640, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
                (ff): FeedForward(
                  (net): Sequential(
                    (0): GEGLU(
                      (proj): Linear(in_features=640, out_features=5120, bias=True)
                    )
                    (1): Dropout(p=0.0, inplace=False)
                    (2): Linear(in_features=2560, out_features=640, bias=True)
                  )
                )
                (attn2): CrossAttention(
                  (to_q): Linear(in_features=640, out_features=640, bias=False)
                  (to_k): Linear(in_features=1024, out_features=640, bias=False)
                  (to_v): Linear(in_features=1024, out_features=640, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=640, out_features=640, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
                (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
                (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
              )
            )
            (proj_out): Linear(in_features=640, out_features=640, bias=True)
          )
          (2): TemporalTransformer(
            (norm): GroupNorm(32, 640, eps=1e-06, affine=True)
            (proj_in): Linear(in_features=640, out_features=640, bias=True)
            (transformer_blocks): ModuleList(
              (0): BasicTransformerBlock(
                (attn1): CrossAttention(
                  (to_q): Linear(in_features=640, out_features=640, bias=False)
                  (to_k): Linear(in_features=640, out_features=640, bias=False)
                  (to_v): Linear(in_features=640, out_features=640, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=640, out_features=640, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
                (ff): FeedForward(
                  (net): Sequential(
                    (0): GEGLU(
                      (proj): Linear(in_features=640, out_features=5120, bias=True)
                    )
                    (1): Dropout(p=0.0, inplace=False)
                    (2): Linear(in_features=2560, out_features=640, bias=True)
                  )
                )
                (attn2): CrossAttention(
                  (to_q): Linear(in_features=640, out_features=640, bias=False)
                  (to_k): Linear(in_features=640, out_features=640, bias=False)
                  (to_v): Linear(in_features=640, out_features=640, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=640, out_features=640, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
                (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
                (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
              )
            )
            (proj_out): Linear(in_features=640, out_features=640, bias=True)
          )
        )
        (5): TimestepEmbedSequential(
          (0): ResBlock(
            (in_layers): Sequential(
              (0): GroupNormSpecific(32, 640, eps=1e-05, affine=True)
              (1): SiLU()
              (2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
            (h_upd): Identity()
            (x_upd): Identity()
            (emb_layers): Sequential(
              (0): SiLU()
              (1): Linear(in_features=1280, out_features=640, bias=True)
            )
            (out_layers): Sequential(
              (0): GroupNormSpecific(32, 640, eps=1e-05, affine=True)
              (1): SiLU()
              (2): Dropout(p=0.0, inplace=False)
              (3): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
            (skip_connection): Identity()
            (temopral_conv): TemporalConvBlock(
              (conv1): Sequential(
                (0): GroupNorm(32, 640, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Conv3d(640, 640, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
              )
              (conv2): Sequential(
                (0): GroupNorm(32, 640, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Dropout(p=0.1, inplace=False)
                (3): Conv3d(640, 640, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
              )
              (conv3): Sequential(
                (0): GroupNorm(32, 640, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Dropout(p=0.1, inplace=False)
                (3): Conv3d(640, 640, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
              )
              (conv4): Sequential(
                (0): GroupNorm(32, 640, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Dropout(p=0.1, inplace=False)
                (3): Conv3d(640, 640, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
              )
            )
          )
          (1): SpatialTransformer(
            (norm): GroupNorm(32, 640, eps=1e-06, affine=True)
            (proj_in): Linear(in_features=640, out_features=640, bias=True)
            (transformer_blocks): ModuleList(
              (0): BasicTransformerBlock(
                (attn1): CrossAttention(
                  (to_q): Linear(in_features=640, out_features=640, bias=False)
                  (to_k): Linear(in_features=640, out_features=640, bias=False)
                  (to_v): Linear(in_features=640, out_features=640, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=640, out_features=640, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
                (ff): FeedForward(
                  (net): Sequential(
                    (0): GEGLU(
                      (proj): Linear(in_features=640, out_features=5120, bias=True)
                    )
                    (1): Dropout(p=0.0, inplace=False)
                    (2): Linear(in_features=2560, out_features=640, bias=True)
                  )
                )
                (attn2): CrossAttention(
                  (to_q): Linear(in_features=640, out_features=640, bias=False)
                  (to_k): Linear(in_features=1024, out_features=640, bias=False)
                  (to_v): Linear(in_features=1024, out_features=640, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=640, out_features=640, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
                (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
                (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
              )
            )
            (proj_out): Linear(in_features=640, out_features=640, bias=True)
          )
          (2): TemporalTransformer(
            (norm): GroupNorm(32, 640, eps=1e-06, affine=True)
            (proj_in): Linear(in_features=640, out_features=640, bias=True)
            (transformer_blocks): ModuleList(
              (0): BasicTransformerBlock(
                (attn1): CrossAttention(
                  (to_q): Linear(in_features=640, out_features=640, bias=False)
                  (to_k): Linear(in_features=640, out_features=640, bias=False)
                  (to_v): Linear(in_features=640, out_features=640, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=640, out_features=640, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
                (ff): FeedForward(
                  (net): Sequential(
                    (0): GEGLU(
                      (proj): Linear(in_features=640, out_features=5120, bias=True)
                    )
                    (1): Dropout(p=0.0, inplace=False)
                    (2): Linear(in_features=2560, out_features=640, bias=True)
                  )
                )
                (attn2): CrossAttention(
                  (to_q): Linear(in_features=640, out_features=640, bias=False)
                  (to_k): Linear(in_features=640, out_features=640, bias=False)
                  (to_v): Linear(in_features=640, out_features=640, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=640, out_features=640, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
                (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
                (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
              )
            )
            (proj_out): Linear(in_features=640, out_features=640, bias=True)
          )
        )
        (6): TimestepEmbedSequential(
          (0): Downsample(
            (op): Conv2d(640, 640, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
          )
        )
        (7): TimestepEmbedSequential(
          (0): ResBlock(
            (in_layers): Sequential(
              (0): GroupNormSpecific(32, 640, eps=1e-05, affine=True)
              (1): SiLU()
              (2): Conv2d(640, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
            (h_upd): Identity()
            (x_upd): Identity()
            (emb_layers): Sequential(
              (0): SiLU()
              (1): Linear(in_features=1280, out_features=1280, bias=True)
            )
            (out_layers): Sequential(
              (0): GroupNormSpecific(32, 1280, eps=1e-05, affine=True)
              (1): SiLU()
              (2): Dropout(p=0.0, inplace=False)
              (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
            (skip_connection): Conv2d(640, 1280, kernel_size=(1, 1), stride=(1, 1))
            (temopral_conv): TemporalConvBlock(
              (conv1): Sequential(
                (0): GroupNorm(32, 1280, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
              )
              (conv2): Sequential(
                (0): GroupNorm(32, 1280, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Dropout(p=0.1, inplace=False)
                (3): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
              )
              (conv3): Sequential(
                (0): GroupNorm(32, 1280, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Dropout(p=0.1, inplace=False)
                (3): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
              )
              (conv4): Sequential(
                (0): GroupNorm(32, 1280, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Dropout(p=0.1, inplace=False)
                (3): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
              )
            )
          )
          (1): SpatialTransformer(
            (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)
            (proj_in): Linear(in_features=1280, out_features=1280, bias=True)
            (transformer_blocks): ModuleList(
              (0): BasicTransformerBlock(
                (attn1): CrossAttention(
                  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                  (to_k): Linear(in_features=1280, out_features=1280, bias=False)
                  (to_v): Linear(in_features=1280, out_features=1280, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=1280, out_features=1280, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
                (ff): FeedForward(
                  (net): Sequential(
                    (0): GEGLU(
                      (proj): Linear(in_features=1280, out_features=10240, bias=True)
                    )
                    (1): Dropout(p=0.0, inplace=False)
                    (2): Linear(in_features=5120, out_features=1280, bias=True)
                  )
                )
                (attn2): CrossAttention(
                  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                  (to_k): Linear(in_features=1024, out_features=1280, bias=False)
                  (to_v): Linear(in_features=1024, out_features=1280, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=1280, out_features=1280, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
                (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
                (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              )
            )
            (proj_out): Linear(in_features=1280, out_features=1280, bias=True)
          )
          (2): TemporalTransformer(
            (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)
            (proj_in): Linear(in_features=1280, out_features=1280, bias=True)
            (transformer_blocks): ModuleList(
              (0): BasicTransformerBlock(
                (attn1): CrossAttention(
                  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                  (to_k): Linear(in_features=1280, out_features=1280, bias=False)
                  (to_v): Linear(in_features=1280, out_features=1280, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=1280, out_features=1280, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
                (ff): FeedForward(
                  (net): Sequential(
                    (0): GEGLU(
                      (proj): Linear(in_features=1280, out_features=10240, bias=True)
                    )
                    (1): Dropout(p=0.0, inplace=False)
                    (2): Linear(in_features=5120, out_features=1280, bias=True)
                  )
                )
                (attn2): CrossAttention(
                  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                  (to_k): Linear(in_features=1280, out_features=1280, bias=False)
                  (to_v): Linear(in_features=1280, out_features=1280, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=1280, out_features=1280, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
                (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
                (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              )
            )
            (proj_out): Linear(in_features=1280, out_features=1280, bias=True)
          )
        )
        (8): TimestepEmbedSequential(
          (0): ResBlock(
            (in_layers): Sequential(
              (0): GroupNormSpecific(32, 1280, eps=1e-05, affine=True)
              (1): SiLU()
              (2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
            (h_upd): Identity()
            (x_upd): Identity()
            (emb_layers): Sequential(
              (0): SiLU()
              (1): Linear(in_features=1280, out_features=1280, bias=True)
            )
            (out_layers): Sequential(
              (0): GroupNormSpecific(32, 1280, eps=1e-05, affine=True)
              (1): SiLU()
              (2): Dropout(p=0.0, inplace=False)
              (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
            (skip_connection): Identity()
            (temopral_conv): TemporalConvBlock(
              (conv1): Sequential(
                (0): GroupNorm(32, 1280, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
              )
              (conv2): Sequential(
                (0): GroupNorm(32, 1280, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Dropout(p=0.1, inplace=False)
                (3): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
              )
              (conv3): Sequential(
                (0): GroupNorm(32, 1280, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Dropout(p=0.1, inplace=False)
                (3): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
              )
              (conv4): Sequential(
                (0): GroupNorm(32, 1280, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Dropout(p=0.1, inplace=False)
                (3): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
              )
            )
          )
          (1): SpatialTransformer(
            (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)
            (proj_in): Linear(in_features=1280, out_features=1280, bias=True)
            (transformer_blocks): ModuleList(
              (0): BasicTransformerBlock(
                (attn1): CrossAttention(
                  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                  (to_k): Linear(in_features=1280, out_features=1280, bias=False)
                  (to_v): Linear(in_features=1280, out_features=1280, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=1280, out_features=1280, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
                (ff): FeedForward(
                  (net): Sequential(
                    (0): GEGLU(
                      (proj): Linear(in_features=1280, out_features=10240, bias=True)
                    )
                    (1): Dropout(p=0.0, inplace=False)
                    (2): Linear(in_features=5120, out_features=1280, bias=True)
                  )
                )
                (attn2): CrossAttention(
                  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                  (to_k): Linear(in_features=1024, out_features=1280, bias=False)
                  (to_v): Linear(in_features=1024, out_features=1280, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=1280, out_features=1280, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
                (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
                (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              )
            )
            (proj_out): Linear(in_features=1280, out_features=1280, bias=True)
          )
          (2): TemporalTransformer(
            (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)
            (proj_in): Linear(in_features=1280, out_features=1280, bias=True)
            (transformer_blocks): ModuleList(
              (0): BasicTransformerBlock(
                (attn1): CrossAttention(
                  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                  (to_k): Linear(in_features=1280, out_features=1280, bias=False)
                  (to_v): Linear(in_features=1280, out_features=1280, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=1280, out_features=1280, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
                (ff): FeedForward(
                  (net): Sequential(
                    (0): GEGLU(
                      (proj): Linear(in_features=1280, out_features=10240, bias=True)
                    )
                    (1): Dropout(p=0.0, inplace=False)
                    (2): Linear(in_features=5120, out_features=1280, bias=True)
                  )
                )
                (attn2): CrossAttention(
                  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                  (to_k): Linear(in_features=1280, out_features=1280, bias=False)
                  (to_v): Linear(in_features=1280, out_features=1280, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=1280, out_features=1280, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
                (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
                (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              )
            )
            (proj_out): Linear(in_features=1280, out_features=1280, bias=True)
          )
        )
        (9): TimestepEmbedSequential(
          (0): Downsample(
            (op): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
          )
        )
        (10-11): 2 x TimestepEmbedSequential(
          (0): ResBlock(
            (in_layers): Sequential(
              (0): GroupNormSpecific(32, 1280, eps=1e-05, affine=True)
              (1): SiLU()
              (2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
            (h_upd): Identity()
            (x_upd): Identity()
            (emb_layers): Sequential(
              (0): SiLU()
              (1): Linear(in_features=1280, out_features=1280, bias=True)
            )
            (out_layers): Sequential(
              (0): GroupNormSpecific(32, 1280, eps=1e-05, affine=True)
              (1): SiLU()
              (2): Dropout(p=0.0, inplace=False)
              (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
            (skip_connection): Identity()
            (temopral_conv): TemporalConvBlock(
              (conv1): Sequential(
                (0): GroupNorm(32, 1280, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
              )
              (conv2): Sequential(
                (0): GroupNorm(32, 1280, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Dropout(p=0.1, inplace=False)
                (3): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
              )
              (conv3): Sequential(
                (0): GroupNorm(32, 1280, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Dropout(p=0.1, inplace=False)
                (3): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
              )
              (conv4): Sequential(
                (0): GroupNorm(32, 1280, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Dropout(p=0.1, inplace=False)
                (3): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
              )
            )
          )
        )
      )
      (init_attn): TimestepEmbedSequential(
        (0): TemporalTransformer(
          (norm): GroupNorm(32, 320, eps=1e-06, affine=True)
          (proj_in): Conv1d(320, 512, kernel_size=(1,), stride=(1,))
          (transformer_blocks): ModuleList(
            (0): BasicTransformerBlock(
              (attn1): CrossAttention(
                (to_q): Linear(in_features=512, out_features=512, bias=False)
                (to_k): Linear(in_features=512, out_features=512, bias=False)
                (to_v): Linear(in_features=512, out_features=512, bias=False)
                (to_out): Sequential(
                  (0): Linear(in_features=512, out_features=512, bias=True)
                  (1): Dropout(p=0.0, inplace=False)
                )
              )
              (ff): FeedForward(
                (net): Sequential(
                  (0): GEGLU(
                    (proj): Linear(in_features=512, out_features=4096, bias=True)
                  )
                  (1): Dropout(p=0.0, inplace=False)
                  (2): Linear(in_features=2048, out_features=512, bias=True)
                )
              )
              (attn2): CrossAttention(
                (to_q): Linear(in_features=512, out_features=512, bias=False)
                (to_k): Linear(in_features=512, out_features=512, bias=False)
                (to_v): Linear(in_features=512, out_features=512, bias=False)
                (to_out): Sequential(
                  (0): Linear(in_features=512, out_features=512, bias=True)
                  (1): Dropout(p=0.0, inplace=False)
                )
              )
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            )
          )
          (proj_out): Conv1d(512, 320, kernel_size=(1,), stride=(1,))
        )
      )
      (middle_block): TimestepEmbedSequential(
        (0): ResBlock(
          (in_layers): Sequential(
            (0): GroupNormSpecific(32, 1280, eps=1e-05, affine=True)
            (1): SiLU()
            (2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
          (h_upd): Identity()
          (x_upd): Identity()
          (emb_layers): Sequential(
            (0): SiLU()
            (1): Linear(in_features=1280, out_features=1280, bias=True)
          )
          (out_layers): Sequential(
            (0): GroupNormSpecific(32, 1280, eps=1e-05, affine=True)
            (1): SiLU()
            (2): Dropout(p=0.0, inplace=False)
            (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
          (skip_connection): Identity()
          (temopral_conv): TemporalConvBlock(
            (conv1): Sequential(
              (0): GroupNorm(32, 1280, eps=1e-05, affine=True)
              (1): SiLU()
              (2): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
            )
            (conv2): Sequential(
              (0): GroupNorm(32, 1280, eps=1e-05, affine=True)
              (1): SiLU()
              (2): Dropout(p=0.1, inplace=False)
              (3): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
            )
            (conv3): Sequential(
              (0): GroupNorm(32, 1280, eps=1e-05, affine=True)
              (1): SiLU()
              (2): Dropout(p=0.1, inplace=False)
              (3): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
            )
            (conv4): Sequential(
              (0): GroupNorm(32, 1280, eps=1e-05, affine=True)
              (1): SiLU()
              (2): Dropout(p=0.1, inplace=False)
              (3): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
            )
          )
        )
        (1): SpatialTransformer(
          (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)
          (proj_in): Linear(in_features=1280, out_features=1280, bias=True)
          (transformer_blocks): ModuleList(
            (0): BasicTransformerBlock(
              (attn1): CrossAttention(
                (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                (to_k): Linear(in_features=1280, out_features=1280, bias=False)
                (to_v): Linear(in_features=1280, out_features=1280, bias=False)
                (to_out): Sequential(
                  (0): Linear(in_features=1280, out_features=1280, bias=True)
                  (1): Dropout(p=0.0, inplace=False)
                )
              )
              (ff): FeedForward(
                (net): Sequential(
                  (0): GEGLU(
                    (proj): Linear(in_features=1280, out_features=10240, bias=True)
                  )
                  (1): Dropout(p=0.0, inplace=False)
                  (2): Linear(in_features=5120, out_features=1280, bias=True)
                )
              )
              (attn2): CrossAttention(
                (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                (to_k): Linear(in_features=1024, out_features=1280, bias=False)
                (to_v): Linear(in_features=1024, out_features=1280, bias=False)
                (to_out): Sequential(
                  (0): Linear(in_features=1280, out_features=1280, bias=True)
                  (1): Dropout(p=0.0, inplace=False)
                )
              )
              (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            )
          )
          (proj_out): Linear(in_features=1280, out_features=1280, bias=True)
        )
        (2): TemporalTransformer(
          (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)
          (proj_in): Linear(in_features=1280, out_features=1280, bias=True)
          (transformer_blocks): ModuleList(
            (0): BasicTransformerBlock(
              (attn1): CrossAttention(
                (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                (to_k): Linear(in_features=1280, out_features=1280, bias=False)
                (to_v): Linear(in_features=1280, out_features=1280, bias=False)
                (to_out): Sequential(
                  (0): Linear(in_features=1280, out_features=1280, bias=True)
                  (1): Dropout(p=0.0, inplace=False)
                )
              )
              (ff): FeedForward(
                (net): Sequential(
                  (0): GEGLU(
                    (proj): Linear(in_features=1280, out_features=10240, bias=True)
                  )
                  (1): Dropout(p=0.0, inplace=False)
                  (2): Linear(in_features=5120, out_features=1280, bias=True)
                )
              )
              (attn2): CrossAttention(
                (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                (to_k): Linear(in_features=1280, out_features=1280, bias=False)
                (to_v): Linear(in_features=1280, out_features=1280, bias=False)
                (to_out): Sequential(
                  (0): Linear(in_features=1280, out_features=1280, bias=True)
                  (1): Dropout(p=0.0, inplace=False)
                )
              )
              (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            )
          )
          (proj_out): Linear(in_features=1280, out_features=1280, bias=True)
        )
        (3): ResBlock(
          (in_layers): Sequential(
            (0): GroupNormSpecific(32, 1280, eps=1e-05, affine=True)
            (1): SiLU()
            (2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
          (h_upd): Identity()
          (x_upd): Identity()
          (emb_layers): Sequential(
            (0): SiLU()
            (1): Linear(in_features=1280, out_features=1280, bias=True)
          )
          (out_layers): Sequential(
            (0): GroupNormSpecific(32, 1280, eps=1e-05, affine=True)
            (1): SiLU()
            (2): Dropout(p=0.0, inplace=False)
            (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
          (skip_connection): Identity()
          (temopral_conv): TemporalConvBlock(
            (conv1): Sequential(
              (0): GroupNorm(32, 1280, eps=1e-05, affine=True)
              (1): SiLU()
              (2): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
            )
            (conv2): Sequential(
              (0): GroupNorm(32, 1280, eps=1e-05, affine=True)
              (1): SiLU()
              (2): Dropout(p=0.1, inplace=False)
              (3): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
            )
            (conv3): Sequential(
              (0): GroupNorm(32, 1280, eps=1e-05, affine=True)
              (1): SiLU()
              (2): Dropout(p=0.1, inplace=False)
              (3): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
            )
            (conv4): Sequential(
              (0): GroupNorm(32, 1280, eps=1e-05, affine=True)
              (1): SiLU()
              (2): Dropout(p=0.1, inplace=False)
              (3): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
            )
          )
        )
      )
      (output_blocks): ModuleList(
        (0-1): 2 x TimestepEmbedSequential(
          (0): ResBlock(
            (in_layers): Sequential(
              (0): GroupNormSpecific(32, 2560, eps=1e-05, affine=True)
              (1): SiLU()
              (2): Conv2d(2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
            (h_upd): Identity()
            (x_upd): Identity()
            (emb_layers): Sequential(
              (0): SiLU()
              (1): Linear(in_features=1280, out_features=1280, bias=True)
            )
            (out_layers): Sequential(
              (0): GroupNormSpecific(32, 1280, eps=1e-05, affine=True)
              (1): SiLU()
              (2): Dropout(p=0.0, inplace=False)
              (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
            (skip_connection): Conv2d(2560, 1280, kernel_size=(1, 1), stride=(1, 1))
            (temopral_conv): TemporalConvBlock(
              (conv1): Sequential(
                (0): GroupNorm(32, 1280, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
              )
              (conv2): Sequential(
                (0): GroupNorm(32, 1280, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Dropout(p=0.1, inplace=False)
                (3): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
              )
              (conv3): Sequential(
                (0): GroupNorm(32, 1280, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Dropout(p=0.1, inplace=False)
                (3): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
              )
              (conv4): Sequential(
                (0): GroupNorm(32, 1280, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Dropout(p=0.1, inplace=False)
                (3): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
              )
            )
          )
        )
        (2): TimestepEmbedSequential(
          (0): ResBlock(
            (in_layers): Sequential(
              (0): GroupNormSpecific(32, 2560, eps=1e-05, affine=True)
              (1): SiLU()
              (2): Conv2d(2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
            (h_upd): Identity()
            (x_upd): Identity()
            (emb_layers): Sequential(
              (0): SiLU()
              (1): Linear(in_features=1280, out_features=1280, bias=True)
            )
            (out_layers): Sequential(
              (0): GroupNormSpecific(32, 1280, eps=1e-05, affine=True)
              (1): SiLU()
              (2): Dropout(p=0.0, inplace=False)
              (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
            (skip_connection): Conv2d(2560, 1280, kernel_size=(1, 1), stride=(1, 1))
            (temopral_conv): TemporalConvBlock(
              (conv1): Sequential(
                (0): GroupNorm(32, 1280, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
              )
              (conv2): Sequential(
                (0): GroupNorm(32, 1280, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Dropout(p=0.1, inplace=False)
                (3): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
              )
              (conv3): Sequential(
                (0): GroupNorm(32, 1280, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Dropout(p=0.1, inplace=False)
                (3): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
              )
              (conv4): Sequential(
                (0): GroupNorm(32, 1280, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Dropout(p=0.1, inplace=False)
                (3): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
              )
            )
          )
          (1): Upsample(
            (conv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
        )
        (3-4): 2 x TimestepEmbedSequential(
          (0): ResBlock(
            (in_layers): Sequential(
              (0): GroupNormSpecific(32, 2560, eps=1e-05, affine=True)
              (1): SiLU()
              (2): Conv2d(2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
            (h_upd): Identity()
            (x_upd): Identity()
            (emb_layers): Sequential(
              (0): SiLU()
              (1): Linear(in_features=1280, out_features=1280, bias=True)
            )
            (out_layers): Sequential(
              (0): GroupNormSpecific(32, 1280, eps=1e-05, affine=True)
              (1): SiLU()
              (2): Dropout(p=0.0, inplace=False)
              (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
            (skip_connection): Conv2d(2560, 1280, kernel_size=(1, 1), stride=(1, 1))
            (temopral_conv): TemporalConvBlock(
              (conv1): Sequential(
                (0): GroupNorm(32, 1280, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
              )
              (conv2): Sequential(
                (0): GroupNorm(32, 1280, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Dropout(p=0.1, inplace=False)
                (3): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
              )
              (conv3): Sequential(
                (0): GroupNorm(32, 1280, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Dropout(p=0.1, inplace=False)
                (3): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
              )
              (conv4): Sequential(
                (0): GroupNorm(32, 1280, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Dropout(p=0.1, inplace=False)
                (3): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
              )
            )
          )
          (1): SpatialTransformer(
            (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)
            (proj_in): Linear(in_features=1280, out_features=1280, bias=True)
            (transformer_blocks): ModuleList(
              (0): BasicTransformerBlock(
                (attn1): CrossAttention(
                  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                  (to_k): Linear(in_features=1280, out_features=1280, bias=False)
                  (to_v): Linear(in_features=1280, out_features=1280, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=1280, out_features=1280, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
                (ff): FeedForward(
                  (net): Sequential(
                    (0): GEGLU(
                      (proj): Linear(in_features=1280, out_features=10240, bias=True)
                    )
                    (1): Dropout(p=0.0, inplace=False)
                    (2): Linear(in_features=5120, out_features=1280, bias=True)
                  )
                )
                (attn2): CrossAttention(
                  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                  (to_k): Linear(in_features=1024, out_features=1280, bias=False)
                  (to_v): Linear(in_features=1024, out_features=1280, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=1280, out_features=1280, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
                (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
                (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              )
            )
            (proj_out): Linear(in_features=1280, out_features=1280, bias=True)
          )
          (2): TemporalTransformer(
            (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)
            (proj_in): Linear(in_features=1280, out_features=1280, bias=True)
            (transformer_blocks): ModuleList(
              (0): BasicTransformerBlock(
                (attn1): CrossAttention(
                  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                  (to_k): Linear(in_features=1280, out_features=1280, bias=False)
                  (to_v): Linear(in_features=1280, out_features=1280, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=1280, out_features=1280, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
                (ff): FeedForward(
                  (net): Sequential(
                    (0): GEGLU(
                      (proj): Linear(in_features=1280, out_features=10240, bias=True)
                    )
                    (1): Dropout(p=0.0, inplace=False)
                    (2): Linear(in_features=5120, out_features=1280, bias=True)
                  )
                )
                (attn2): CrossAttention(
                  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                  (to_k): Linear(in_features=1280, out_features=1280, bias=False)
                  (to_v): Linear(in_features=1280, out_features=1280, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=1280, out_features=1280, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
                (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
                (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              )
            )
            (proj_out): Linear(in_features=1280, out_features=1280, bias=True)
          )
        )
        (5): TimestepEmbedSequential(
          (0): ResBlock(
            (in_layers): Sequential(
              (0): GroupNormSpecific(32, 1920, eps=1e-05, affine=True)
              (1): SiLU()
              (2): Conv2d(1920, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
            (h_upd): Identity()
            (x_upd): Identity()
            (emb_layers): Sequential(
              (0): SiLU()
              (1): Linear(in_features=1280, out_features=1280, bias=True)
            )
            (out_layers): Sequential(
              (0): GroupNormSpecific(32, 1280, eps=1e-05, affine=True)
              (1): SiLU()
              (2): Dropout(p=0.0, inplace=False)
              (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
            (skip_connection): Conv2d(1920, 1280, kernel_size=(1, 1), stride=(1, 1))
            (temopral_conv): TemporalConvBlock(
              (conv1): Sequential(
                (0): GroupNorm(32, 1280, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
              )
              (conv2): Sequential(
                (0): GroupNorm(32, 1280, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Dropout(p=0.1, inplace=False)
                (3): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
              )
              (conv3): Sequential(
                (0): GroupNorm(32, 1280, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Dropout(p=0.1, inplace=False)
                (3): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
              )
              (conv4): Sequential(
                (0): GroupNorm(32, 1280, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Dropout(p=0.1, inplace=False)
                (3): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
              )
            )
          )
          (1): SpatialTransformer(
            (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)
            (proj_in): Linear(in_features=1280, out_features=1280, bias=True)
            (transformer_blocks): ModuleList(
              (0): BasicTransformerBlock(
                (attn1): CrossAttention(
                  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                  (to_k): Linear(in_features=1280, out_features=1280, bias=False)
                  (to_v): Linear(in_features=1280, out_features=1280, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=1280, out_features=1280, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
                (ff): FeedForward(
                  (net): Sequential(
                    (0): GEGLU(
                      (proj): Linear(in_features=1280, out_features=10240, bias=True)
                    )
                    (1): Dropout(p=0.0, inplace=False)
                    (2): Linear(in_features=5120, out_features=1280, bias=True)
                  )
                )
                (attn2): CrossAttention(
                  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                  (to_k): Linear(in_features=1024, out_features=1280, bias=False)
                  (to_v): Linear(in_features=1024, out_features=1280, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=1280, out_features=1280, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
                (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
                (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              )
            )
            (proj_out): Linear(in_features=1280, out_features=1280, bias=True)
          )
          (2): TemporalTransformer(
            (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)
            (proj_in): Linear(in_features=1280, out_features=1280, bias=True)
            (transformer_blocks): ModuleList(
              (0): BasicTransformerBlock(
                (attn1): CrossAttention(
                  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                  (to_k): Linear(in_features=1280, out_features=1280, bias=False)
                  (to_v): Linear(in_features=1280, out_features=1280, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=1280, out_features=1280, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
                (ff): FeedForward(
                  (net): Sequential(
                    (0): GEGLU(
                      (proj): Linear(in_features=1280, out_features=10240, bias=True)
                    )
                    (1): Dropout(p=0.0, inplace=False)
                    (2): Linear(in_features=5120, out_features=1280, bias=True)
                  )
                )
                (attn2): CrossAttention(
                  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                  (to_k): Linear(in_features=1280, out_features=1280, bias=False)
                  (to_v): Linear(in_features=1280, out_features=1280, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=1280, out_features=1280, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
                (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
                (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              )
            )
            (proj_out): Linear(in_features=1280, out_features=1280, bias=True)
          )
          (3): Upsample(
            (conv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
        )
        (6): TimestepEmbedSequential(
          (0): ResBlock(
            (in_layers): Sequential(
              (0): GroupNormSpecific(32, 1920, eps=1e-05, affine=True)
              (1): SiLU()
              (2): Conv2d(1920, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
            (h_upd): Identity()
            (x_upd): Identity()
            (emb_layers): Sequential(
              (0): SiLU()
              (1): Linear(in_features=1280, out_features=640, bias=True)
            )
            (out_layers): Sequential(
              (0): GroupNormSpecific(32, 640, eps=1e-05, affine=True)
              (1): SiLU()
              (2): Dropout(p=0.0, inplace=False)
              (3): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
            (skip_connection): Conv2d(1920, 640, kernel_size=(1, 1), stride=(1, 1))
            (temopral_conv): TemporalConvBlock(
              (conv1): Sequential(
                (0): GroupNorm(32, 640, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Conv3d(640, 640, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
              )
              (conv2): Sequential(
                (0): GroupNorm(32, 640, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Dropout(p=0.1, inplace=False)
                (3): Conv3d(640, 640, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
              )
              (conv3): Sequential(
                (0): GroupNorm(32, 640, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Dropout(p=0.1, inplace=False)
                (3): Conv3d(640, 640, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
              )
              (conv4): Sequential(
                (0): GroupNorm(32, 640, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Dropout(p=0.1, inplace=False)
                (3): Conv3d(640, 640, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
              )
            )
          )
          (1): SpatialTransformer(
            (norm): GroupNorm(32, 640, eps=1e-06, affine=True)
            (proj_in): Linear(in_features=640, out_features=640, bias=True)
            (transformer_blocks): ModuleList(
              (0): BasicTransformerBlock(
                (attn1): CrossAttention(
                  (to_q): Linear(in_features=640, out_features=640, bias=False)
                  (to_k): Linear(in_features=640, out_features=640, bias=False)
                  (to_v): Linear(in_features=640, out_features=640, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=640, out_features=640, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
                (ff): FeedForward(
                  (net): Sequential(
                    (0): GEGLU(
                      (proj): Linear(in_features=640, out_features=5120, bias=True)
                    )
                    (1): Dropout(p=0.0, inplace=False)
                    (2): Linear(in_features=2560, out_features=640, bias=True)
                  )
                )
                (attn2): CrossAttention(
                  (to_q): Linear(in_features=640, out_features=640, bias=False)
                  (to_k): Linear(in_features=1024, out_features=640, bias=False)
                  (to_v): Linear(in_features=1024, out_features=640, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=640, out_features=640, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
                (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
                (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
              )
            )
            (proj_out): Linear(in_features=640, out_features=640, bias=True)
          )
          (2): TemporalTransformer(
            (norm): GroupNorm(32, 640, eps=1e-06, affine=True)
            (proj_in): Linear(in_features=640, out_features=640, bias=True)
            (transformer_blocks): ModuleList(
              (0): BasicTransformerBlock(
                (attn1): CrossAttention(
                  (to_q): Linear(in_features=640, out_features=640, bias=False)
                  (to_k): Linear(in_features=640, out_features=640, bias=False)
                  (to_v): Linear(in_features=640, out_features=640, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=640, out_features=640, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
                (ff): FeedForward(
                  (net): Sequential(
                    (0): GEGLU(
                      (proj): Linear(in_features=640, out_features=5120, bias=True)
                    )
                    (1): Dropout(p=0.0, inplace=False)
                    (2): Linear(in_features=2560, out_features=640, bias=True)
                  )
                )
                (attn2): CrossAttention(
                  (to_q): Linear(in_features=640, out_features=640, bias=False)
                  (to_k): Linear(in_features=640, out_features=640, bias=False)
                  (to_v): Linear(in_features=640, out_features=640, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=640, out_features=640, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
                (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
                (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
              )
            )
            (proj_out): Linear(in_features=640, out_features=640, bias=True)
          )
        )
        (7): TimestepEmbedSequential(
          (0): ResBlock(
            (in_layers): Sequential(
              (0): GroupNormSpecific(32, 1280, eps=1e-05, affine=True)
              (1): SiLU()
              (2): Conv2d(1280, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
            (h_upd): Identity()
            (x_upd): Identity()
            (emb_layers): Sequential(
              (0): SiLU()
              (1): Linear(in_features=1280, out_features=640, bias=True)
            )
            (out_layers): Sequential(
              (0): GroupNormSpecific(32, 640, eps=1e-05, affine=True)
              (1): SiLU()
              (2): Dropout(p=0.0, inplace=False)
              (3): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
            (skip_connection): Conv2d(1280, 640, kernel_size=(1, 1), stride=(1, 1))
            (temopral_conv): TemporalConvBlock(
              (conv1): Sequential(
                (0): GroupNorm(32, 640, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Conv3d(640, 640, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
              )
              (conv2): Sequential(
                (0): GroupNorm(32, 640, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Dropout(p=0.1, inplace=False)
                (3): Conv3d(640, 640, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
              )
              (conv3): Sequential(
                (0): GroupNorm(32, 640, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Dropout(p=0.1, inplace=False)
                (3): Conv3d(640, 640, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
              )
              (conv4): Sequential(
                (0): GroupNorm(32, 640, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Dropout(p=0.1, inplace=False)
                (3): Conv3d(640, 640, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
              )
            )
          )
          (1): SpatialTransformer(
            (norm): GroupNorm(32, 640, eps=1e-06, affine=True)
            (proj_in): Linear(in_features=640, out_features=640, bias=True)
            (transformer_blocks): ModuleList(
              (0): BasicTransformerBlock(
                (attn1): CrossAttention(
                  (to_q): Linear(in_features=640, out_features=640, bias=False)
                  (to_k): Linear(in_features=640, out_features=640, bias=False)
                  (to_v): Linear(in_features=640, out_features=640, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=640, out_features=640, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
                (ff): FeedForward(
                  (net): Sequential(
                    (0): GEGLU(
                      (proj): Linear(in_features=640, out_features=5120, bias=True)
                    )
                    (1): Dropout(p=0.0, inplace=False)
                    (2): Linear(in_features=2560, out_features=640, bias=True)
                  )
                )
                (attn2): CrossAttention(
                  (to_q): Linear(in_features=640, out_features=640, bias=False)
                  (to_k): Linear(in_features=1024, out_features=640, bias=False)
                  (to_v): Linear(in_features=1024, out_features=640, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=640, out_features=640, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
                (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
                (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
              )
            )
            (proj_out): Linear(in_features=640, out_features=640, bias=True)
          )
          (2): TemporalTransformer(
            (norm): GroupNorm(32, 640, eps=1e-06, affine=True)
            (proj_in): Linear(in_features=640, out_features=640, bias=True)
            (transformer_blocks): ModuleList(
              (0): BasicTransformerBlock(
                (attn1): CrossAttention(
                  (to_q): Linear(in_features=640, out_features=640, bias=False)
                  (to_k): Linear(in_features=640, out_features=640, bias=False)
                  (to_v): Linear(in_features=640, out_features=640, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=640, out_features=640, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
                (ff): FeedForward(
                  (net): Sequential(
                    (0): GEGLU(
                      (proj): Linear(in_features=640, out_features=5120, bias=True)
                    )
                    (1): Dropout(p=0.0, inplace=False)
                    (2): Linear(in_features=2560, out_features=640, bias=True)
                  )
                )
                (attn2): CrossAttention(
                  (to_q): Linear(in_features=640, out_features=640, bias=False)
                  (to_k): Linear(in_features=640, out_features=640, bias=False)
                  (to_v): Linear(in_features=640, out_features=640, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=640, out_features=640, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
                (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
                (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
              )
            )
            (proj_out): Linear(in_features=640, out_features=640, bias=True)
          )
        )
        (8): TimestepEmbedSequential(
          (0): ResBlock(
            (in_layers): Sequential(
              (0): GroupNormSpecific(32, 960, eps=1e-05, affine=True)
              (1): SiLU()
              (2): Conv2d(960, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
            (h_upd): Identity()
            (x_upd): Identity()
            (emb_layers): Sequential(
              (0): SiLU()
              (1): Linear(in_features=1280, out_features=640, bias=True)
            )
            (out_layers): Sequential(
              (0): GroupNormSpecific(32, 640, eps=1e-05, affine=True)
              (1): SiLU()
              (2): Dropout(p=0.0, inplace=False)
              (3): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
            (skip_connection): Conv2d(960, 640, kernel_size=(1, 1), stride=(1, 1))
            (temopral_conv): TemporalConvBlock(
              (conv1): Sequential(
                (0): GroupNorm(32, 640, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Conv3d(640, 640, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
              )
              (conv2): Sequential(
                (0): GroupNorm(32, 640, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Dropout(p=0.1, inplace=False)
                (3): Conv3d(640, 640, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
              )
              (conv3): Sequential(
                (0): GroupNorm(32, 640, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Dropout(p=0.1, inplace=False)
                (3): Conv3d(640, 640, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
              )
              (conv4): Sequential(
                (0): GroupNorm(32, 640, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Dropout(p=0.1, inplace=False)
                (3): Conv3d(640, 640, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
              )
            )
          )
          (1): SpatialTransformer(
            (norm): GroupNorm(32, 640, eps=1e-06, affine=True)
            (proj_in): Linear(in_features=640, out_features=640, bias=True)
            (transformer_blocks): ModuleList(
              (0): BasicTransformerBlock(
                (attn1): CrossAttention(
                  (to_q): Linear(in_features=640, out_features=640, bias=False)
                  (to_k): Linear(in_features=640, out_features=640, bias=False)
                  (to_v): Linear(in_features=640, out_features=640, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=640, out_features=640, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
                (ff): FeedForward(
                  (net): Sequential(
                    (0): GEGLU(
                      (proj): Linear(in_features=640, out_features=5120, bias=True)
                    )
                    (1): Dropout(p=0.0, inplace=False)
                    (2): Linear(in_features=2560, out_features=640, bias=True)
                  )
                )
                (attn2): CrossAttention(
                  (to_q): Linear(in_features=640, out_features=640, bias=False)
                  (to_k): Linear(in_features=1024, out_features=640, bias=False)
                  (to_v): Linear(in_features=1024, out_features=640, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=640, out_features=640, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
                (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
                (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
              )
            )
            (proj_out): Linear(in_features=640, out_features=640, bias=True)
          )
          (2): TemporalTransformer(
            (norm): GroupNorm(32, 640, eps=1e-06, affine=True)
            (proj_in): Linear(in_features=640, out_features=640, bias=True)
            (transformer_blocks): ModuleList(
              (0): BasicTransformerBlock(
                (attn1): CrossAttention(
                  (to_q): Linear(in_features=640, out_features=640, bias=False)
                  (to_k): Linear(in_features=640, out_features=640, bias=False)
                  (to_v): Linear(in_features=640, out_features=640, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=640, out_features=640, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
                (ff): FeedForward(
                  (net): Sequential(
                    (0): GEGLU(
                      (proj): Linear(in_features=640, out_features=5120, bias=True)
                    )
                    (1): Dropout(p=0.0, inplace=False)
                    (2): Linear(in_features=2560, out_features=640, bias=True)
                  )
                )
                (attn2): CrossAttention(
                  (to_q): Linear(in_features=640, out_features=640, bias=False)
                  (to_k): Linear(in_features=640, out_features=640, bias=False)
                  (to_v): Linear(in_features=640, out_features=640, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=640, out_features=640, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
                (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
                (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
              )
            )
            (proj_out): Linear(in_features=640, out_features=640, bias=True)
          )
          (3): Upsample(
            (conv): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
        )
        (9): TimestepEmbedSequential(
          (0): ResBlock(
            (in_layers): Sequential(
              (0): GroupNormSpecific(32, 960, eps=1e-05, affine=True)
              (1): SiLU()
              (2): Conv2d(960, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
            (h_upd): Identity()
            (x_upd): Identity()
            (emb_layers): Sequential(
              (0): SiLU()
              (1): Linear(in_features=1280, out_features=320, bias=True)
            )
            (out_layers): Sequential(
              (0): GroupNormSpecific(32, 320, eps=1e-05, affine=True)
              (1): SiLU()
              (2): Dropout(p=0.0, inplace=False)
              (3): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
            (skip_connection): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1))
            (temopral_conv): TemporalConvBlock(
              (conv1): Sequential(
                (0): GroupNorm(32, 320, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Conv3d(320, 320, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
              )
              (conv2): Sequential(
                (0): GroupNorm(32, 320, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Dropout(p=0.1, inplace=False)
                (3): Conv3d(320, 320, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
              )
              (conv3): Sequential(
                (0): GroupNorm(32, 320, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Dropout(p=0.1, inplace=False)
                (3): Conv3d(320, 320, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
              )
              (conv4): Sequential(
                (0): GroupNorm(32, 320, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Dropout(p=0.1, inplace=False)
                (3): Conv3d(320, 320, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
              )
            )
          )
          (1): SpatialTransformer(
            (norm): GroupNorm(32, 320, eps=1e-06, affine=True)
            (proj_in): Linear(in_features=320, out_features=320, bias=True)
            (transformer_blocks): ModuleList(
              (0): BasicTransformerBlock(
                (attn1): CrossAttention(
                  (to_q): Linear(in_features=320, out_features=320, bias=False)
                  (to_k): Linear(in_features=320, out_features=320, bias=False)
                  (to_v): Linear(in_features=320, out_features=320, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=320, out_features=320, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
                (ff): FeedForward(
                  (net): Sequential(
                    (0): GEGLU(
                      (proj): Linear(in_features=320, out_features=2560, bias=True)
                    )
                    (1): Dropout(p=0.0, inplace=False)
                    (2): Linear(in_features=1280, out_features=320, bias=True)
                  )
                )
                (attn2): CrossAttention(
                  (to_q): Linear(in_features=320, out_features=320, bias=False)
                  (to_k): Linear(in_features=1024, out_features=320, bias=False)
                  (to_v): Linear(in_features=1024, out_features=320, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=320, out_features=320, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
                (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
                (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
              )
            )
            (proj_out): Linear(in_features=320, out_features=320, bias=True)
          )
          (2): TemporalTransformer(
            (norm): GroupNorm(32, 320, eps=1e-06, affine=True)
            (proj_in): Linear(in_features=320, out_features=320, bias=True)
            (transformer_blocks): ModuleList(
              (0): BasicTransformerBlock(
                (attn1): CrossAttention(
                  (to_q): Linear(in_features=320, out_features=320, bias=False)
                  (to_k): Linear(in_features=320, out_features=320, bias=False)
                  (to_v): Linear(in_features=320, out_features=320, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=320, out_features=320, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
                (ff): FeedForward(
                  (net): Sequential(
                    (0): GEGLU(
                      (proj): Linear(in_features=320, out_features=2560, bias=True)
                    )
                    (1): Dropout(p=0.0, inplace=False)
                    (2): Linear(in_features=1280, out_features=320, bias=True)
                  )
                )
                (attn2): CrossAttention(
                  (to_q): Linear(in_features=320, out_features=320, bias=False)
                  (to_k): Linear(in_features=320, out_features=320, bias=False)
                  (to_v): Linear(in_features=320, out_features=320, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=320, out_features=320, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
                (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
                (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
              )
            )
            (proj_out): Linear(in_features=320, out_features=320, bias=True)
          )
        )
        (10-11): 2 x TimestepEmbedSequential(
          (0): ResBlock(
            (in_layers): Sequential(
              (0): GroupNormSpecific(32, 640, eps=1e-05, affine=True)
              (1): SiLU()
              (2): Conv2d(640, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
            (h_upd): Identity()
            (x_upd): Identity()
            (emb_layers): Sequential(
              (0): SiLU()
              (1): Linear(in_features=1280, out_features=320, bias=True)
            )
            (out_layers): Sequential(
              (0): GroupNormSpecific(32, 320, eps=1e-05, affine=True)
              (1): SiLU()
              (2): Dropout(p=0.0, inplace=False)
              (3): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
            (skip_connection): Conv2d(640, 320, kernel_size=(1, 1), stride=(1, 1))
            (temopral_conv): TemporalConvBlock(
              (conv1): Sequential(
                (0): GroupNorm(32, 320, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Conv3d(320, 320, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
              )
              (conv2): Sequential(
                (0): GroupNorm(32, 320, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Dropout(p=0.1, inplace=False)
                (3): Conv3d(320, 320, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
              )
              (conv3): Sequential(
                (0): GroupNorm(32, 320, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Dropout(p=0.1, inplace=False)
                (3): Conv3d(320, 320, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
              )
              (conv4): Sequential(
                (0): GroupNorm(32, 320, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Dropout(p=0.1, inplace=False)
                (3): Conv3d(320, 320, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
              )
            )
          )
          (1): SpatialTransformer(
            (norm): GroupNorm(32, 320, eps=1e-06, affine=True)
            (proj_in): Linear(in_features=320, out_features=320, bias=True)
            (transformer_blocks): ModuleList(
              (0): BasicTransformerBlock(
                (attn1): CrossAttention(
                  (to_q): Linear(in_features=320, out_features=320, bias=False)
                  (to_k): Linear(in_features=320, out_features=320, bias=False)
                  (to_v): Linear(in_features=320, out_features=320, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=320, out_features=320, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
                (ff): FeedForward(
                  (net): Sequential(
                    (0): GEGLU(
                      (proj): Linear(in_features=320, out_features=2560, bias=True)
                    )
                    (1): Dropout(p=0.0, inplace=False)
                    (2): Linear(in_features=1280, out_features=320, bias=True)
                  )
                )
                (attn2): CrossAttention(
                  (to_q): Linear(in_features=320, out_features=320, bias=False)
                  (to_k): Linear(in_features=1024, out_features=320, bias=False)
                  (to_v): Linear(in_features=1024, out_features=320, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=320, out_features=320, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
                (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
                (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
              )
            )
            (proj_out): Linear(in_features=320, out_features=320, bias=True)
          )
          (2): TemporalTransformer(
            (norm): GroupNorm(32, 320, eps=1e-06, affine=True)
            (proj_in): Linear(in_features=320, out_features=320, bias=True)
            (transformer_blocks): ModuleList(
              (0): BasicTransformerBlock(
                (attn1): CrossAttention(
                  (to_q): Linear(in_features=320, out_features=320, bias=False)
                  (to_k): Linear(in_features=320, out_features=320, bias=False)
                  (to_v): Linear(in_features=320, out_features=320, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=320, out_features=320, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
                (ff): FeedForward(
                  (net): Sequential(
                    (0): GEGLU(
                      (proj): Linear(in_features=320, out_features=2560, bias=True)
                    )
                    (1): Dropout(p=0.0, inplace=False)
                    (2): Linear(in_features=1280, out_features=320, bias=True)
                  )
                )
                (attn2): CrossAttention(
                  (to_q): Linear(in_features=320, out_features=320, bias=False)
                  (to_k): Linear(in_features=320, out_features=320, bias=False)
                  (to_v): Linear(in_features=320, out_features=320, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=320, out_features=320, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
                (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
                (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
              )
            )
            (proj_out): Linear(in_features=320, out_features=320, bias=True)
          )
        )
      )
      (out): Sequential(
        (0): GroupNormSpecific(32, 320, eps=1e-05, affine=True)
        (1): SiLU()
        (2): Conv2d(320, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
    )
  )
  (first_stage_model): AutoencoderKL(
    (encoder): Encoder(
      (conv_in): Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (down): ModuleList(
        (0): Module(
          (block): ModuleList(
            (0-1): 2 x ResnetBlock(
              (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
          )
          (attn): ModuleList()
          (downsample): Downsample(
            (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2))
          )
        )
        (1): Module(
          (block): ModuleList(
            (0): ResnetBlock(
              (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)
              (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (nin_shortcut): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))
            )
            (1): ResnetBlock(
              (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
          )
          (attn): ModuleList()
          (downsample): Downsample(
            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
          )
        )
        (2): Module(
          (block): ModuleList(
            (0): ResnetBlock(
              (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)
              (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (nin_shortcut): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))
            )
            (1): ResnetBlock(
              (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)
              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
          )
          (attn): ModuleList()
          (downsample): Downsample(
            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2))
          )
        )
        (3): Module(
          (block): ModuleList(
            (0-1): 2 x ResnetBlock(
              (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)
              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
          )
          (attn): ModuleList()
        )
      )
      (mid): Module(
        (block_1): ResnetBlock(
          (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)
          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (attn_1): AttnBlock(
          (norm): GroupNorm(32, 512, eps=1e-06, affine=True)
          (q): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
          (k): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
          (v): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
          (proj_out): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
        )
        (block_2): ResnetBlock(
          (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)
          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
      )
      (norm_out): GroupNorm(32, 512, eps=1e-06, affine=True)
      (conv_out): Conv2d(512, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (decoder): Decoder(
      (conv_in): Conv2d(4, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (mid): Module(
        (block_1): ResnetBlock(
          (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)
          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (attn_1): AttnBlock(
          (norm): GroupNorm(32, 512, eps=1e-06, affine=True)
          (q): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
          (k): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
          (v): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
          (proj_out): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
        )
        (block_2): ResnetBlock(
          (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)
          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
      )
      (up): ModuleList(
        (0): Module(
          (block): ModuleList(
            (0): ResnetBlock(
              (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)
              (conv1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (nin_shortcut): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
            )
            (1-2): 2 x ResnetBlock(
              (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
          )
          (attn): ModuleList()
        )
        (1): Module(
          (block): ModuleList(
            (0): ResnetBlock(
              (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)
              (conv1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (nin_shortcut): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
            )
            (1-2): 2 x ResnetBlock(
              (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
          )
          (attn): ModuleList()
          (upsample): Upsample(
            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
        )
        (2-3): 2 x Module(
          (block): ModuleList(
            (0-2): 3 x ResnetBlock(
              (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)
              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
          )
          (attn): ModuleList()
          (upsample): Upsample(
            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
        )
      )
      (norm_out): GroupNorm(32, 128, eps=1e-06, affine=True)
      (conv_out): Conv2d(128, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (loss): Identity()
    (quant_conv): Conv2d(8, 8, kernel_size=(1, 1), stride=(1, 1))
    (post_quant_conv): Conv2d(4, 4, kernel_size=(1, 1), stride=(1, 1))
  )
  (cond_stage_model): FrozenOpenCLIPEmbedder(
    (model): CLIP(
      (transformer): Transformer(
        (resblocks): ModuleList(
          (0-23): 24 x ResidualAttentionBlock(
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
            )
            (ls_1): Identity()
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): GELU(approximate='none')
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ls_2): Identity()
          )
        )
      )
      (token_embedding): Embedding(49408, 1024)
      (ln_final): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    )
  )
), 'name': 'inference_t2v_512_v2.0', 'folder_name': 'inference_t2v_512_v2.0-2024-08-06T00-09-31', 'output_dir': 'outputs/inference_t2v_512_v2.0-2024-08-06T00-09-31'}
2024-08-06 00:09:33,058 INFO    MainThread:38608 [wandb_init.py:init():619] starting backend
2024-08-06 00:09:33,058 INFO    MainThread:38608 [wandb_init.py:init():623] setting up manager
2024-08-06 00:09:33,059 INFO    MainThread:38608 [backend.py:_multiprocessing_setup():105] multiprocessing start_methods=fork,spawn,forkserver, using: spawn
2024-08-06 00:09:33,060 INFO    MainThread:38608 [wandb_init.py:init():631] backend started and connected
2024-08-06 00:09:33,072 INFO    MainThread:38608 [wandb_init.py:init():720] updated telemetry
2024-08-06 00:09:33,167 INFO    MainThread:38608 [wandb_init.py:init():753] communicating run to backend with 90.0 second timeout
2024-08-06 00:09:34,699 INFO    MainThread:38608 [wandb_run.py:_on_init():2435] communicating current version
2024-08-06 00:09:34,739 INFO    MainThread:38608 [wandb_run.py:_on_init():2444] got version response 
2024-08-06 00:09:34,740 INFO    MainThread:38608 [wandb_init.py:init():804] starting run threads in backend
2024-08-06 00:09:36,042 INFO    MainThread:38608 [wandb_run.py:_console_start():2413] atexit reg
2024-08-06 00:09:36,042 INFO    MainThread:38608 [wandb_run.py:_redirect():2255] redirect: wrap_raw
2024-08-06 00:09:36,042 INFO    MainThread:38608 [wandb_run.py:_redirect():2320] Wrapping output streams.
2024-08-06 00:09:36,042 INFO    MainThread:38608 [wandb_run.py:_redirect():2345] Redirects installed.
2024-08-06 00:09:36,043 INFO    MainThread:38608 [wandb_init.py:init():847] run started, returning control to user process
2024-08-06 00:09:44,273 WARNING MsgRouterThr:38608 [router.py:message_loop():77] message_loop has been closed
